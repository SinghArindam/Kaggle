{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":89659,"databundleVersionId":11522106,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":224423433,"sourceType":"kernelVersion"},{"sourceId":230023480,"sourceType":"kernelVersion"},{"sourceId":4527,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":3319,"modelId":971},{"sourceId":263093,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":225001,"modelId":164716},{"sourceId":289299,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":247861,"modelId":269379},{"sourceId":289308,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":247869,"modelId":269387}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#| default_exp core","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\nimport kagglehub\n\nimport os\nimport io\nimport re\nimport random\nimport base64\nfrom io import BytesIO\n\nimport time\nfrom datetime import timedelta\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.functional as F\n\nfrom IPython.display import SVG\n\nfrom PIL import Image\nimport cv2\n\nfrom diffusers import StableDiffusionPipeline\nfrom transformers import AutoProcessor, AutoModel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metric = kagglehub.package_import('jiazhuang/svg-image-fidelity')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport statistics\nimport pandas as pd\n\ndef image_resize(image, size=(384, 384)):\n    return image.convert('RGB').resize(size)\n\ndef bitmap_score_instance_impl(multiple_choice_qa, image, random_seed=42):\n    rng = np.random.RandomState(random_seed)\n    group_seed = rng.randint(0, np.iinfo(np.int32).max)\n    image_processor = metric.ImageProcessor(image=image_resize(image), seed=group_seed).apply()\n    image = image_processor.image\n    questions = multiple_choice_qa['question']\n    choices = multiple_choice_qa['choices']\n    answers = multiple_choice_qa['answer']\n    vqa_score = metric.vqa_evaluator.score(questions, choices, answers, image)\n    ocr_score = metric.vqa_evaluator.ocr(image_processor.original_image)\n    aesthetic_score = metric.aesthetic_evaluator.score(image)\n    instance_score = metric.harmonic_mean(vqa_score, aesthetic_score, beta=0.5) * ocr_score\n    return instance_score, vqa_score, ocr_score, aesthetic_score\n\ndef bitmap_score_instance(multiple_choice_qa, image, random_seed=42):\n    is_single = not isinstance(image, list)\n    if is_single:\n        multiple_choice_qa = [multiple_choice_qa]\n        image = [image]\n    \n    assert len(multiple_choice_qa) == len(image)\n\n    results = []\n    score_df = []\n    for one_image, one_multiple_choice_qa in zip(image, multiple_choice_qa, strict=True):\n        instance_score, vqa_score, ocr_score, aesthetic_score = bitmap_score_instance_impl(one_multiple_choice_qa, one_image, random_seed=42)\n        results.append(instance_score)\n        score_df.append([instance_score, vqa_score, ocr_score, aesthetic_score])\n\n    fidelity = statistics.mean(results)\n    score_df = pd.DataFrame(score_df, columns=['competition_score', 'vqa_score', 'ocr_score', 'aesthetic_score'])\n    if is_single:\n        return score_df.iloc[0].to_dict()\n    else:\n        return float(fidelity), score_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\n# Ensure GPU is being used and optimize for speed\ndevice = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nimport torch\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\n\n# Load with optimized scheduler and half precision\nstable_diffusion_path = kagglehub.model_download(\"stabilityai/stable-diffusion-v2/pytorch/1/1\")\n\nscheduler = DDIMScheduler.from_pretrained(stable_diffusion_path, subfolder=\"scheduler\")\n\npipe = StableDiffusionPipeline.from_pretrained(\n    stable_diffusion_path,\n    scheduler=scheduler,\n    torch_dtype=torch.float16,  # Use half precision\n    safety_checker=None         # Disable safety checker for speed\n)\n\n# Move to GPU and apply optimizations\npipe.to(device) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drawing_with_llms_path = kagglehub.competition_download('drawing-with-llms')\ntrain_df = pd.read_csv(f'{drawing_with_llms_path}/train.csv')\ntrain_question_df = pd.read_parquet(f'{drawing_with_llms_path}/questions.parquet')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_question_df = train_question_df.groupby('id').apply(lambda df: df.to_dict(orient='list'))\ntrain_question_df = train_question_df.reset_index(name='qa')\n\ntrain_question_df['question'] = train_question_df.qa.apply(lambda qa: json.dumps(qa['question'], ensure_ascii=False))\n\ntrain_question_df['choices'] = train_question_df.qa.apply(\n    lambda qa: json.dumps(\n        [x.tolist() for x in qa['choices']], ensure_ascii=False\n    )\n)\n\ntrain_question_df['answer'] = train_question_df.qa.apply(lambda qa: json.dumps(qa['answer'], ensure_ascii=False))\n\ntrain_df = pd.merge(train_df, train_question_df, how='left', on='id')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\ndef generate_bitmap(prompt, negative_prompt=\"\", num_inference_steps=20, guidance_scale=15):\n        \n    image = pipe(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=num_inference_steps, \n        guidance_scale=guidance_scale,\n    ).images[0]\n    \n    return image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_prefix = \"Simple, classic image of\"\nprompt_suffix = \"with flat color blocks, beautiful, minimal details, solid colors only\"\nnegative_prompt = \"lines, framing, hatching, background, textures, patterns, details, outlines\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"r = train_df.iloc[2]\ndescription = r.description\nprint(description)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = f'{prompt_prefix} {description} {prompt_suffix}'\nprint(prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = generate_bitmap(prompt, negative_prompt=negative_prompt)\nimage","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multiple_choice_qa = {\n    'question': json.loads(r.question),\n    'choices': json.loads(r.choices),\n    'answer': json.loads(r.answer)\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bitmap_score_instance(multiple_choice_qa, image, random_seed=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\ndef compress_hex_color(hex_color):\n    \"\"\"Convert hex color to shortest possible representation\"\"\"\n    r, g, b = int(hex_color[1:3], 16), int(hex_color[3:5], 16), int(hex_color[5:7], 16)\n    if r % 17 == 0 and g % 17 == 0 and b % 17 == 0:\n        return f'#{r//17:x}{g//17:x}{b//17:x}'\n    return hex_color\n\ndef extract_features_by_scale(img_np, num_colors=16):\n    \"\"\"\n    Extract image features hierarchically by scale\n    \n    Args:\n        img_np (np.ndarray): Input image\n        num_colors (int): Number of colors to quantize\n    \n    Returns:\n        list: Hierarchical features sorted by importance\n    \"\"\"\n    # Convert to RGB if needed\n    if len(img_np.shape) == 3 and img_np.shape[2] > 1:\n        img_rgb = img_np\n    else:\n        img_rgb = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)\n    \n    # Convert to grayscale for processing\n    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n    height, width = gray.shape\n    \n    # Perform color quantization\n    pixels = img_rgb.reshape(-1, 3).astype(np.float32)\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n    _, labels, centers = cv2.kmeans(pixels, num_colors, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n    \n    # Quantized image\n    palette = centers.astype(np.uint8)\n    quantized = palette[labels.flatten()].reshape(img_rgb.shape)\n    \n    # Hierarchical feature extraction\n    hierarchical_features = []\n    \n    # Sort colors by frequency\n    unique_labels, counts = np.unique(labels, return_counts=True)\n    sorted_indices = np.argsort(-counts)\n    sorted_colors = [palette[i] for i in sorted_indices]\n    \n    # Center point for importance calculations\n    center_x, center_y = width/2, height/2\n    \n    for color in sorted_colors:\n        # Create color mask\n        color_mask = cv2.inRange(quantized, color, color)\n        \n        # Find contours\n        contours, _ = cv2.findContours(color_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        # Sort contours by area (largest first)\n        contours = sorted(contours, key=cv2.contourArea, reverse=True)\n        \n        # Convert RGB to compressed hex\n        hex_color = compress_hex_color(f'#{color[0]:02x}{color[1]:02x}{color[2]:02x}')\n        \n        color_features = []\n        for contour in contours:\n            # Skip tiny contours\n            area = cv2.contourArea(contour)\n            if area < 20:\n                continue\n            \n            # Calculate contour center\n            m = cv2.moments(contour)\n            if m[\"m00\"] == 0:\n                continue\n            \n            cx = int(m[\"m10\"] / m[\"m00\"])\n            cy = int(m[\"m01\"] / m[\"m00\"])\n            \n            # Distance from image center (normalized)\n            dist_from_center = np.sqrt(((cx - center_x) / width)**2 + ((cy - center_y) / height)**2)\n            \n            # Simplify contour\n            epsilon = 0.02 * cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, epsilon, True)\n            \n            # Generate points string\n            points = \" \".join([f\"{pt[0][0]:.1f},{pt[0][1]:.1f}\" for pt in approx])\n            \n            # Calculate importance (area, proximity to center, complexity)\n            importance = (\n                area * \n                (1 - dist_from_center) * \n                (1 / (len(approx) + 1))\n            )\n            \n            color_features.append({\n                'points': points,\n                'color': hex_color,\n                'area': area,\n                'importance': importance,\n                'point_count': len(approx),\n                'original_contour': approx  # Store original contour for adaptive simplification\n            })\n        \n        # Sort features by importance within this color\n        color_features.sort(key=lambda x: x['importance'], reverse=True)\n        hierarchical_features.extend(color_features)\n    \n    # Final sorting by overall importance\n    hierarchical_features.sort(key=lambda x: x['importance'], reverse=True)\n    \n    return hierarchical_features\n\ndef simplify_polygon(points_str, simplification_level):\n    \"\"\"\n    Simplify a polygon by reducing coordinate precision or number of points\n    \n    Args:\n        points_str (str): Space-separated \"x,y\" coordinates\n        simplification_level (int): Level of simplification (0-3)\n    \n    Returns:\n        str: Simplified points string\n    \"\"\"\n    if simplification_level == 0:\n        return points_str\n    \n    points = points_str.split()\n    \n    # Level 1: Round to 1 decimal place\n    if simplification_level == 1:\n        return \" \".join([f\"{float(p.split(',')[0]):.1f},{float(p.split(',')[1]):.1f}\" for p in points])\n    \n    # Level 2: Round to integer\n    if simplification_level == 2:\n        return \" \".join([f\"{float(p.split(',')[0]):.0f},{float(p.split(',')[1]):.0f}\" for p in points])\n    \n    # Level 3: Reduce number of points (keep every other point, but ensure at least 3 points)\n    if simplification_level == 3:\n        if len(points) <= 4:\n            # If 4 or fewer points, just round to integer\n            return \" \".join([f\"{float(p.split(',')[0]):.0f},{float(p.split(',')[1]):.0f}\" for p in points])\n        else:\n            # Keep approximately half the points, but maintain at least 3\n            step = min(2, len(points) // 3)\n            reduced_points = [points[i] for i in range(0, len(points), step)]\n            # Ensure we keep at least 3 points and the last point\n            if len(reduced_points) < 3:\n                reduced_points = points[:3]\n            if points[-1] not in reduced_points:\n                reduced_points.append(points[-1])\n            return \" \".join([f\"{float(p.split(',')[0]):.0f},{float(p.split(',')[1]):.0f}\" for p in reduced_points])\n    \n    return points_str\n\ndef bitmap_to_svg_layered(image, max_size_bytes=10000, resize=True, target_size=(384, 384), \n                         adaptive_fill=True, num_colors=None):\n    \"\"\"\n    Convert bitmap to SVG using layered feature extraction with optimized space usage\n    \n    Args:\n        image: Input image (PIL.Image)\n        max_size_bytes (int): Maximum SVG size\n        resize (bool): Whether to resize the image before processing\n        target_size (tuple): Target size for resizing (width, height)\n        adaptive_fill (bool): Whether to adaptively fill available space\n        num_colors (int): Number of colors to quantize, if None uses adaptive selection\n    \n    Returns:\n        str: SVG representation\n    \"\"\"\n    # Adaptive color selection based on image complexity\n    if num_colors is None:\n        # Simple heuristic: more colors for complex images\n        if resize:\n            pixel_count = target_size[0] * target_size[1]\n        else:\n            pixel_count = image.size[0] * image.size[1]\n        \n        if pixel_count < 65536:  # 256x256\n            num_colors = 8\n        elif pixel_count < 262144:  # 512x512\n            num_colors = 12\n        else:\n            num_colors = 16\n    \n    # Resize the image if requested\n    if resize:\n        original_size = image.size\n        image = image.resize(target_size, Image.LANCZOS)\n    else:\n        original_size = image.size\n    \n    # Convert to numpy array\n    img_np = np.array(image)\n    \n    # Get image dimensions\n    height, width = img_np.shape[:2]\n    \n    # Calculate average background color\n    if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n        avg_bg_color = np.mean(img_np, axis=(0,1)).astype(int)\n        bg_hex_color = compress_hex_color(f'#{avg_bg_color[0]:02x}{avg_bg_color[1]:02x}{avg_bg_color[2]:02x}')\n    else:\n        bg_hex_color = '#fff'\n    \n    # Start building SVG\n    # Use original dimensions in viewBox for proper scaling when displayed\n    orig_width, orig_height = original_size\n    svg_header = f'<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"{orig_width}\" height=\"{orig_height}\" viewBox=\"0 0 {width} {height}\">\\n'\n    svg_bg = f'<rect width=\"{width}\" height=\"{height}\" fill=\"{bg_hex_color}\"/>\\n'\n    svg_base = svg_header + svg_bg\n    svg_footer = '</svg>'\n    \n    # Calculate base size\n    base_size = len((svg_base + svg_footer).encode('utf-8'))\n    available_bytes = max_size_bytes - base_size\n    \n    # Extract hierarchical features\n    features = extract_features_by_scale(img_np, num_colors=num_colors)\n    \n    # If not using adaptive fill, just add features until we hit the limit\n    if not adaptive_fill:\n        svg = svg_base\n        for feature in features:\n            # Try adding the feature\n            feature_svg = f'<polygon points=\"{feature[\"points\"]}\" fill=\"{feature[\"color\"]}\" />\\n'\n            \n            # Check if adding this feature exceeds size limit\n            if len((svg + feature_svg + svg_footer).encode('utf-8')) > max_size_bytes:\n                break\n            \n            # Add the feature\n            svg += feature_svg\n        \n        # Close SVG\n        svg += svg_footer\n        return svg\n    \n    # For adaptive fill, use binary search to find optimal simplification level\n    \n    # First attempt: calculate size of all features at different simplification levels\n    feature_sizes = []\n    for feature in features:\n        feature_sizes.append({\n            'original': len(f'<polygon points=\"{feature[\"points\"]}\" fill=\"{feature[\"color\"]}\" />\\n'.encode('utf-8')),\n            'level1': len(f'<polygon points=\"{simplify_polygon(feature[\"points\"], 1)}\" fill=\"{feature[\"color\"]}\" />\\n'.encode('utf-8')),\n            'level2': len(f'<polygon points=\"{simplify_polygon(feature[\"points\"], 2)}\" fill=\"{feature[\"color\"]}\" />\\n'.encode('utf-8')),\n            'level3': len(f'<polygon points=\"{simplify_polygon(feature[\"points\"], 3)}\" fill=\"{feature[\"color\"]}\" />\\n'.encode('utf-8'))\n        })\n    \n    # Two-pass approach: first add most important features, then fill remaining space\n    svg = svg_base\n    bytes_used = base_size\n    added_features = set()\n    \n    # Pass 1: Add most important features at original quality\n    for i, feature in enumerate(features):\n        feature_svg = f'<polygon points=\"{feature[\"points\"]}\" fill=\"{feature[\"color\"]}\" />\\n'\n        feature_size = feature_sizes[i]['original']\n        \n        if bytes_used + feature_size <= max_size_bytes:\n            svg += feature_svg\n            bytes_used += feature_size\n            added_features.add(i)\n    \n    # Pass 2: Try to add remaining features with progressive simplification\n    for level in range(1, 4):  # Try simplification levels 1-3\n        for i, feature in enumerate(features):\n            if i in added_features:\n                continue\n                \n            feature_size = feature_sizes[i][f'level{level}']\n            if bytes_used + feature_size <= max_size_bytes:\n                feature_svg = f'<polygon points=\"{simplify_polygon(feature[\"points\"], level)}\" fill=\"{feature[\"color\"]}\" />\\n'\n                svg += feature_svg\n                bytes_used += feature_size\n                added_features.add(i)\n    \n    # Finalize SVG\n    svg += svg_footer\n    \n    # Double check we didn't exceed limit\n    final_size = len(svg.encode('utf-8'))\n    if final_size > max_size_bytes:\n        # If we somehow went over, return basic SVG\n        return f'<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 {width} {height}\"><rect width=\"{width}\" height=\"{height}\" fill=\"{bg_hex_color}\"/></svg>'\n    \n    # Calculate space utilization\n    utilization = (final_size / max_size_bytes) * 100\n    \n    # Return the SVG with efficient space utilization\n    return svg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import SVG","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"svg_content = bitmap_to_svg_layered(image)\nSVG(svg_content)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multiple_choice_qa = {\n    'question': json.loads(r.question),\n    'choices': json.loads(r.choices),\n    'answer': json.loads(r.answer)\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metric.score_instance(multiple_choice_qa, svg_content, random_seed=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\nclass Model:\n    def __init__(self):\n        self.prompt_prefix = \"Simple, classic image of\"\n        self.prompt_suffix = \"with flat color blocks, beautiful, minimal details, solid colors only\"\n        self.negative_prompt = \"lines, framing, hatching, background, textures, patterns, details, outlines\"\n\n        self.num_inference_steps = 25\n        self.guidance_scale = 20\n\n    def gen_bitmap(self, description):\n        prompt = f'{self.prompt_prefix} {description} {self.prompt_suffix}'\n        bitmap = generate_bitmap(prompt, self.negative_prompt, self.num_inference_steps, self.guidance_scale)\n        return bitmap\n\n    def predict(self, prompt: str) -> str:\n        bitmap = self.gen_bitmap(prompt)\n        svg = bitmap_to_svg_layered(bitmap)\n        return svg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"svg = model.predict(description)\nSVG(svg)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metric.score_instance(multiple_choice_qa, svg, random_seed=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm.auto import tqdm\ntqdm.pandas()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['bitmap'] = train_df.description.progress_apply(model.gen_bitmap)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['svg'] = train_df.bitmap.progress_apply(bitmap_to_svg_layered)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['multiple_choice_qa'] = train_df.apply(\n    lambda r: {\n    'question': json.loads(r.question),\n    'choices': json.loads(r.choices),\n    'answer': json.loads(r.answer)\n    },\n    axis=1,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ntrain_df['bitmap_score'] = train_df.progress_apply(\n    lambda r: bitmap_score_instance(r.multiple_choice_qa, r.bitmap, random_seed=42),\n    axis=1,\n)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['svg_score'] = train_df.progress_apply(\n    lambda r: metric.score_instance(r.multiple_choice_qa, r.svg, random_seed=42),\n    axis=1,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('\\n'.join(train_df.description))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for r in train_df.itertuples():\n    b_score = r.bitmap_score['competition_score']\n    b_vqa = r.bitmap_score['vqa_score']\n    b_ocr = r.bitmap_score['ocr_score']\n    b_aesthetic = r.bitmap_score['aesthetic_score']\n\n    s_score = r.svg_score['competition_score']\n    s_vqa = r.svg_score['vqa_score']\n    s_ocr = r.svg_score['ocr_score']\n    s_aesthetic = r.svg_score['aesthetic_score']\n    \n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(np.array(r.bitmap))\n    plt.axis('off')\n    plt.title(f'bitmap: score={b_score:.2f}, vqa={b_vqa:.2f}, ocr={b_ocr:.2f}, aes={b_aesthetic:.2f}')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(metric.svg_to_png(r.svg))\n    plt.axis('off')\n    plt.title(f'svg: score={s_score:.2f}, vqa={s_vqa:.2f}, ocr={s_ocr:.2f}, aes={s_aesthetic:.2f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_bitmap_score = pd.DataFrame(train_df['bitmap_score'].tolist()).mean(axis=0)\nmean_bitmap_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_svg_score = pd.DataFrame(train_df['svg_score'].tolist()).mean(axis=0)\nmean_svg_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'Original bitmap score: {mean_bitmap_score.competition_score}')\nprint(f'Final svg score: {mean_svg_score.competition_score}')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1648809,"sourceType":"datasetVersion","datasetId":975000},{"sourceId":2816656,"sourceType":"datasetVersion","datasetId":1722209},{"sourceId":5539168,"sourceType":"datasetVersion","datasetId":3192482}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Directories","metadata":{}},{"cell_type":"code","source":"# Directories\nVOXCELEB_DIR = \"/kaggle/input/voxceleb1train/wav\"\nMUSAN_DIR = \"/kaggle/input/musan-dataset/musan\"\nRIRS_DIR = \"/kaggle/input/rirs-noises/RIRS_NOISES\"\nOUTPUT_DIR = \"/kaggle/working/augmented_voxceleb\"\nOUTPUT_FILE = \"/kaggle/working/pretrain_voxceleb.pt\"  # Binary dataset file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T17:38:15.843991Z","iopub.execute_input":"2025-01-30T17:38:15.844332Z","iopub.status.idle":"2025-01-30T17:38:15.849268Z","shell.execute_reply.started":"2025-01-30T17:38:15.844306Z","shell.execute_reply":"2025-01-30T17:38:15.848087Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Hereâ€™s a PyTorch-based script that will:  \n\n1. Load the **VoxCeleb** dataset.  \n2. Apply **random augmentations** using the **MUSAN (music, speech, noise)** and **RIRS** datasets.  \n3. Save the **augmented dataset** as a new set of preprocessed audio files for direct training.  \n\n---\n\n### **Setup Requirements**\nEnsure you have the necessary Python libraries installed:  \n\n```bash\npip install torchaudio torch numpy librosa soundfile\n```\n\nAlso, ensure you have:  \n- **VoxCeleb dataset** (`.wav` files).  \n- **MUSAN dataset** (music, speech, noise).  \n- **RIRS_NOISES dataset** (impulse responses).  \n\n---\n\n### **Code Implementation**\n```python\nimport os\nimport torch\nimport torchaudio\nimport random\nimport numpy as np\nimport soundfile as sf\nimport librosa\n\n# Directories\nVOXCELEB_DIR = \"/path/to/voxceleb\"\nMUSAN_DIR = \"/path/to/musan\"\nRIRS_DIR = \"/path/to/rirs_noises\"\nOUTPUT_DIR = \"/path/to/augmented_voxceleb\"\n\n# Ensure output directory exists\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Load list of available augmentation files\ndef get_files(directory, ext=\".wav\"):\n    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(ext)]\n\n# Load augmentation files\nmusan_music_files = get_files(os.path.join(MUSAN_DIR, \"music\"))\nmusan_speech_files = get_files(os.path.join(MUSAN_DIR, \"speech\"))\nmusan_noise_files = get_files(os.path.join(MUSAN_DIR, \"noise\"))\nrirs_files = get_files(os.path.join(RIRS_DIR, \"simulated_rirs\"))\n\n# Load an audio file\ndef load_audio(file, target_sr=16000):\n    waveform, sr = torchaudio.load(file)\n    if sr != target_sr:\n        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(waveform)\n    return waveform\n\n# Apply augmentation\ndef apply_augmentation(wav, aug_file):\n    aug_wav = load_audio(aug_file)\n    min_len = min(wav.shape[1], aug_wav.shape[1])\n    aug_wav = aug_wav[:, :min_len]  # Trim augmentation to match main audio length\n    wav = wav[:, :min_len]\n    \n    # Apply mixing with random scaling\n    mix_ratio = random.uniform(0.1, 0.5)\n    return (1 - mix_ratio) * wav + mix_ratio * aug_wav\n\n# Process all VoxCeleb files\nvoxceleb_files = get_files(VOXCELEB_DIR)\nfor idx, file in enumerate(voxceleb_files):\n    try:\n        wav = load_audio(file)\n\n        # Randomly choose 2 augmentations\n        aug_choices = random.sample([\n            random.choice(musan_music_files),\n            random.choice(musan_speech_files),\n            random.choice(musan_noise_files),\n            random.choice(rirs_files)\n        ], k=2)\n\n        # Apply augmentations\n        for aug_file in aug_choices:\n            wav = apply_augmentation(wav, aug_file)\n\n        # Save the augmented audio\n        output_file = os.path.join(OUTPUT_DIR, os.path.basename(file))\n        sf.write(output_file, wav.numpy().squeeze(), 16000)\n\n        print(f\"[{idx+1}/{len(voxceleb_files)}] Processed: {file} -> {output_file}\")\n\n    except Exception as e:\n        print(f\"Error processing {file}: {e}\")\n\nprint(\"Augmentation Complete. Augmented files saved in:\", OUTPUT_DIR)\n```\n\n---\n\n### **How It Works**\n1. **Loads** VoxCeleb audio files.  \n2. **Randomly selects** 2 augmentation sources from **MUSAN (music, speech, noise)** and **RIRS_NOISES (reverberations)**.  \n3. **Applies augmentation** by mixing the original waveform with the selected augmentations.  \n4. **Saves** the augmented dataset for future training.  \n\n---\n\n### **Next Steps**\n- Use the saved files directly for training without needing MUSAN and RIRS datasets.  \n- Modify `mix_ratio` to control augmentation strength.  \n- Add more augmentation strategies like **time-stretching, pitch shifting, and background noise variations** for robustness.  \n\nLet me know if you need modifications!","metadata":{}},{"cell_type":"markdown","source":"## Code Implementation","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nimport random\nimport numpy as np\nimport soundfile as sf\nimport librosa\n\n# Directories\nVOXCELEB_DIR = \"/kaggle/input/voxceleb1train/wav\"\nMUSAN_DIR = \"/kaggle/input/musan-dataset/musan\"\nRIRS_DIR = \"/kaggle/input/rirs-noises/RIRS_NOISES\"\nOUTPUT_DIR = \"/kaggle/working/augmented_voxceleb\"\n\n# Ensure output directory exists\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Load list of available augmentation files\ndef get_files(directory, ext=\".wav\"):\n    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(ext)]\n\n# Load augmentation files\nmusan_music_files = get_files(os.path.join(MUSAN_DIR, \"music\"))\nmusan_speech_files = get_files(os.path.join(MUSAN_DIR, \"speech\"))\nmusan_noise_files = get_files(os.path.join(MUSAN_DIR, \"noise\"))\nrirs_files = get_files(os.path.join(RIRS_DIR, \"simulated_rirs\"))\n\n# Load an audio file\ndef load_audio(file, target_sr=16000):\n    waveform, sr = torchaudio.load(file)\n    if sr != target_sr:\n        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(waveform)\n    return waveform\n\n# Apply augmentation\ndef apply_augmentation(wav, aug_file):\n    aug_wav = load_audio(aug_file)\n    min_len = min(wav.shape[1], aug_wav.shape[1])\n    aug_wav = aug_wav[:, :min_len]  # Trim augmentation to match main audio length\n    wav = wav[:, :min_len]\n    \n    # Apply mixing with random scaling\n    mix_ratio = random.uniform(0.1, 0.5)\n    return (1 - mix_ratio) * wav + mix_ratio * aug_wav\n\n# Process all VoxCeleb files\nvoxceleb_files = get_files(VOXCELEB_DIR)\nfor idx, file in enumerate(voxceleb_files):\n    try:\n        wav = load_audio(file)\n\n        # Randomly choose 2 augmentations\n        aug_choices = random.sample([\n            random.choice(musan_music_files),\n            random.choice(musan_speech_files),\n            random.choice(musan_noise_files),\n            random.choice(rirs_files)\n        ], k=2)\n\n        # Apply augmentations\n        for aug_file in aug_choices:\n            wav = apply_augmentation(wav, aug_file)\n\n        # Save the augmented audio\n        output_file = os.path.join(OUTPUT_DIR, os.path.basename(file))\n        sf.write(output_file, wav.numpy().squeeze(), 16000)\n\n        print(f\"[{idx+1}/{len(voxceleb_files)}] Processed: {file} -> {output_file}\")\n\n    except Exception as e:\n        print(f\"Error processing {file}: {e}\")\n\nprint(\"Augmentation Complete. Augmented files saved in:\", OUTPUT_DIR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Got it! Instead of saving preprocessed audio files, I'll store the augmented dataset as a **binary file** (PyTorch `.pt` format), which you can directly load for training.  \n\nThis will:  \n- Load **VoxCeleb** files.  \n- Apply **random augmentations** using **MUSAN (music, speech, noise)** and **RIRS**.  \n- Store the dataset as a **binary tensor file** (`.pt`) for direct use in training.  \n\n---\n\n### **Code Implementation**\n```python\nimport os\nimport torch\nimport torchaudio\nimport random\nimport numpy as np\n\n# Directories\nVOXCELEB_DIR = \"/path/to/voxceleb\"\nMUSAN_DIR = \"/path/to/musan\"\nRIRS_DIR = \"/path/to/rirs_noises\"\nOUTPUT_FILE = \"/path/to/pretrain_voxceleb.pt\"  # Binary dataset file\n\n# Load augmentation files\ndef get_files(directory, ext=\".wav\"):\n    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(ext)]\n\nmusan_music_files = get_files(os.path.join(MUSAN_DIR, \"music\"))\nmusan_speech_files = get_files(os.path.join(MUSAN_DIR, \"speech\"))\nmusan_noise_files = get_files(os.path.join(MUSAN_DIR, \"noise\"))\nrirs_files = get_files(os.path.join(RIRS_DIR, \"simulated_rirs\"))\n\n# Load an audio file\ndef load_audio(file, target_sr=16000):\n    waveform, sr = torchaudio.load(file)\n    if sr != target_sr:\n        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(waveform)\n    return waveform\n\n# Apply augmentation\ndef apply_augmentation(wav, aug_file):\n    aug_wav = load_audio(aug_file)\n    min_len = min(wav.shape[1], aug_wav.shape[1])\n    aug_wav = aug_wav[:, :min_len]  # Trim augmentation to match main audio length\n    wav = wav[:, :min_len]\n    \n    # Apply mixing with random scaling\n    mix_ratio = random.uniform(0.1, 0.5)\n    return (1 - mix_ratio) * wav + mix_ratio * aug_wav\n\n# Process all VoxCeleb files and store in memory\ndataset = []\nvoxceleb_files = get_files(VOXCELEB_DIR)\n\nfor idx, file in enumerate(voxceleb_files):\n    try:\n        wav = load_audio(file)\n\n        # Randomly choose 2 augmentations\n        aug_choices = random.sample([\n            random.choice(musan_music_files),\n            random.choice(musan_speech_files),\n            random.choice(musan_noise_files),\n            random.choice(rirs_files)\n        ], k=2)\n\n        # Apply augmentations\n        for aug_file in aug_choices:\n            wav = apply_augmentation(wav, aug_file)\n\n        # Store as tensor with filename metadata\n        dataset.append((wav, file))\n\n        print(f\"[{idx+1}/{len(voxceleb_files)}] Processed: {file}\")\n\n    except Exception as e:\n        print(f\"Error processing {file}: {e}\")\n\n# Save as binary file\ntorch.save(dataset, OUTPUT_FILE)\nprint(\"Dataset saved at:\", OUTPUT_FILE)\n```\n\n---\n\n### **How to Use the Saved Dataset**\n```python\n# Load preprocessed dataset\ndataset = torch.load(\"/path/to/pretrain_voxceleb.pt\")\n\n# Access first audio sample\naudio_tensor, filename = dataset[0]\nprint(\"Loaded Audio Shape:\", audio_tensor.shape)\nprint(\"Original Filename:\", filename)\n```\n\n---\n\n### **Advantages**\nâœ… **Fast Training** â€“ No need to reprocess audio each time.  \nâœ… **Efficient Storage** â€“ Saves all data in a compact binary format.  \nâœ… **Metadata Retention** â€“ Keeps original filenames for tracking.  \n\nLet me know if you need further refinements!","metadata":{}},{"cell_type":"markdown","source":"## Code Implementation","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nimport random\nimport numpy as np\n\n# Directories\n# VOXCELEB_DIR = \"/path/to/voxceleb\"\n# MUSAN_DIR = \"/path/to/musan\"\n# RIRS_DIR = \"/path/to/rirs_noises\"\n# OUTPUT_FILE = \"/kaggle/working/pretrain_voxceleb.pt\"  # Binary dataset file\n\n# Load augmentation files\ndef get_files(directory, ext=\".wav\"):\n    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(ext)]\n\nmusan_music_files = get_files(os.path.join(MUSAN_DIR, \"music\"))\nmusan_speech_files = get_files(os.path.join(MUSAN_DIR, \"speech\"))\nmusan_noise_files = get_files(os.path.join(MUSAN_DIR, \"noise\"))\nrirs_files = get_files(os.path.join(RIRS_DIR, \"simulated_rirs\"))\n\n# Load an audio file\ndef load_audio(file, target_sr=16000):\n    waveform, sr = torchaudio.load(file)\n    if sr != target_sr:\n        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)(waveform)\n    return waveform\n\n# Apply augmentation\ndef apply_augmentation(wav, aug_file):\n    aug_wav = load_audio(aug_file)\n    min_len = min(wav.shape[1], aug_wav.shape[1])\n    aug_wav = aug_wav[:, :min_len]  # Trim augmentation to match main audio length\n    wav = wav[:, :min_len]\n    \n    # Apply mixing with random scaling\n    mix_ratio = random.uniform(0.1, 0.5)\n    return (1 - mix_ratio) * wav + mix_ratio * aug_wav\n\n# Process all VoxCeleb files and store in memory\ndataset = []\nvoxceleb_files = get_files(VOXCELEB_DIR)\n\nfor idx, file in enumerate(voxceleb_files):\n    try:\n        wav = load_audio(file)\n\n        # Randomly choose 2 augmentations\n        aug_choices = random.sample([\n            random.choice(musan_music_files),\n            random.choice(musan_speech_files),\n            random.choice(musan_noise_files),\n            random.choice(rirs_files)\n        ], k=2)\n\n        # Apply augmentations\n        for aug_file in aug_choices:\n            wav = apply_augmentation(wav, aug_file)\n\n        # Store as tensor with filename metadata\n        dataset.append((wav, file))\n\n        print(f\"[{idx+1}/{len(voxceleb_files)}] Processed: {file}\")\n\n    except Exception as e:\n        print(f\"Error processing {file}: {e}\")\n\n# Save as binary file\ntorch.save(dataset, OUTPUT_FILE)\nprint(\"Dataset saved at:\", OUTPUT_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T17:39:58.467331Z","iopub.execute_input":"2025-01-30T17:39:58.467884Z","iopub.status.idle":"2025-01-30T17:39:58.500044Z","shell.execute_reply.started":"2025-01-30T17:39:58.467836Z","shell.execute_reply":"2025-01-30T17:39:58.498814Z"}},"outputs":[{"name":"stdout","text":"Dataset saved at: /kaggle/working/pretrain_voxceleb.pt\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## How to Use the Saved Dataset","metadata":{}},{"cell_type":"code","source":"# Load preprocessed dataset\ndataset = torch.load(\"/kaggle/working/pretrain_voxceleb.pt\")\n\n# Access first audio sample\naudio_tensor, filename = dataset[0]\nprint(\"Loaded Audio Shape:\", audio_tensor.shape)\nprint(\"Original Filename:\", filename)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now, I'll provide a **two-step process**:  \n\n1. **Preprocessing & Feature Extraction**  \n   - Load the augmented dataset from the binary `.pt` file.  \n   - Extract necessary features (**MFCC, Mel spectrogram, Spectral Contrast, etc.**).  \n   - Save extracted features as a **binary tensor file (`.pt`)** for training.  \n\n2. **Loading & Using Preprocessed Features for Training**  \n   - Load the preprocessed features.  \n   - Use them directly in training.  \n\n---\n\n## **Step 1: Preprocessing & Feature Extraction**\n```python\nimport torch\nimport torchaudio\nimport torchaudio.transforms as T\nimport os\n\n# Paths\nAUGMENTED_DATASET_FILE = \"/path/to/pretrain_voxceleb.pt\"  # Augmented dataset\nFEATURES_OUTPUT_FILE = \"/path/to/voxceleb_features.pt\"  # Extracted features\n\n# Define feature extraction\ndef extract_features(wav, sample_rate=16000):\n    \"\"\"Extracts various audio features from waveform.\"\"\"\n    # Convert to mono\n    if wav.shape[0] > 1:\n        wav = torch.mean(wav, dim=0, keepdim=True)\n\n    # MFCC (Mel-Frequency Cepstral Coefficients)\n    mfcc_transform = T.MFCC(sample_rate=sample_rate, n_mfcc=40)\n    mfcc = mfcc_transform(wav)\n\n    # Mel Spectrogram\n    mel_spec_transform = T.MelSpectrogram(sample_rate=sample_rate, n_mels=64)\n    mel_spec = mel_spec_transform(wav)\n\n    # Spectral Contrast\n    spectral_contrast = torchaudio.functional.spectral_centroid(wav, sample_rate)\n\n    # Normalize all features\n    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-6)\n    mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-6)\n    spectral_contrast = (spectral_contrast - spectral_contrast.mean()) / (spectral_contrast.std() + 1e-6)\n\n    return mfcc, mel_spec, spectral_contrast\n\n# Load augmented dataset\ndataset = torch.load(AUGMENTED_DATASET_FILE)\nprocessed_features = []\n\nfor idx, (wav, filename) in enumerate(dataset):\n    try:\n        # Extract features\n        mfcc, mel_spec, spectral_contrast = extract_features(wav)\n\n        # Store as tuple (features, label/filename)\n        processed_features.append((mfcc, mel_spec, spectral_contrast, filename))\n\n        print(f\"[{idx+1}/{len(dataset)}] Processed Features for: {filename}\")\n\n    except Exception as e:\n        print(f\"Error processing {filename}: {e}\")\n\n# Save extracted features\ntorch.save(processed_features, FEATURES_OUTPUT_FILE)\nprint(\"Feature extraction complete. Features saved at:\", FEATURES_OUTPUT_FILE)\n```\n\n---\n\n## **Step 2: Loading & Using Features for Training**\n```python\n# Load extracted features\nfeatures_dataset = torch.load(\"/path/to/voxceleb_features.pt\")\n\n# Example usage\nfor mfcc, mel_spec, spectral_contrast, filename in features_dataset:\n    print(f\"File: {filename}\")\n    print(\"MFCC Shape:\", mfcc.shape)\n    print(\"Mel Spectrogram Shape:\", mel_spec.shape)\n    print(\"Spectral Contrast Shape:\", spectral_contrast.shape)\n    break  # Print first sample\n```\n\n---\n\n### **Advantages of This Approach**\nâœ… **Faster Training** â€“ No need to re-extract features each time.  \nâœ… **Compact Storage** â€“ Saves all extracted features efficiently.  \nâœ… **Direct Training Ready** â€“ Just load and pass the features into your model.  \n\nLet me know if you need more modifications!","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Preprocessing & Feature Extraction","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchaudio\nimport torchaudio.transforms as T\nimport os\n\n# Paths\nAUGMENTED_DATASET_FILE = \"/kaggle/working/pretrain_voxceleb.pt\"  # Augmented dataset\nFEATURES_OUTPUT_FILE = \"/kaggle/working/voxceleb_features.pt\"  # Extracted features\n\n# Define feature extraction\ndef extract_features(wav, sample_rate=16000):\n    \"\"\"Extracts various audio features from waveform.\"\"\"\n    # Convert to mono\n    if wav.shape[0] > 1:\n        wav = torch.mean(wav, dim=0, keepdim=True)\n\n    # MFCC (Mel-Frequency Cepstral Coefficients)\n    mfcc_transform = T.MFCC(sample_rate=sample_rate, n_mfcc=40)\n    mfcc = mfcc_transform(wav)\n\n    # Mel Spectrogram\n    mel_spec_transform = T.MelSpectrogram(sample_rate=sample_rate, n_mels=64)\n    mel_spec = mel_spec_transform(wav)\n\n    # Spectral Contrast\n    spectral_contrast = torchaudio.functional.spectral_centroid(wav, sample_rate)\n\n    # Normalize all features\n    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-6)\n    mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-6)\n    spectral_contrast = (spectral_contrast - spectral_contrast.mean()) / (spectral_contrast.std() + 1e-6)\n\n    return mfcc, mel_spec, spectral_contrast\n\n# Load augmented dataset\ndataset = torch.load(AUGMENTED_DATASET_FILE)\nprocessed_features = []\n\nfor idx, (wav, filename) in enumerate(dataset):\n    try:\n        # Extract features\n        mfcc, mel_spec, spectral_contrast = extract_features(wav)\n\n        # Store as tuple (features, label/filename)\n        processed_features.append((mfcc, mel_spec, spectral_contrast, filename))\n\n        print(f\"[{idx+1}/{len(dataset)}] Processed Features for: {filename}\")\n\n    except Exception as e:\n        print(f\"Error processing {filename}: {e}\")\n\n# Save extracted features\ntorch.save(processed_features, FEATURES_OUTPUT_FILE)\nprint(\"Feature extraction complete. Features saved at:\", FEATURES_OUTPUT_FILE)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Loading & Using Features for Training","metadata":{}},{"cell_type":"code","source":"# Load extracted features\nfeatures_dataset = torch.load(\"/kaggle/working/voxceleb_features.pt\")\n\n# Example usage\nfor mfcc, mel_spec, spectral_contrast, filename in features_dataset:\n    print(f\"File: {filename}\")\n    print(\"MFCC Shape:\", mfcc.shape)\n    print(\"Mel Spectrogram Shape:\", mel_spec.shape)\n    print(\"Spectral Contrast Shape:\", spectral_contrast.shape)\n    break  # Print first sample\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91716,"databundleVersionId":11893428,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import lib and Check Input and read","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport time\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_log_error, make_scorer\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\n\nimport optuna\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain_path = \"/kaggle/input/playground-series-s5e5/train.csv\"\ntest_path = \"/kaggle/input/playground-series-s5e5/test.csv\"\n\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\n\ntest_ids = test_df['id']","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:24.716534Z","iopub.execute_input":"2025-05-31T08:48:24.716881Z","iopub.status.idle":"2025-05-31T08:48:38.758588Z","shell.execute_reply.started":"2025-05-31T08:48:24.716848Z","shell.execute_reply":"2025-05-31T08:48:38.757363Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Pre-Processing","metadata":{}},{"cell_type":"code","source":"train_df['Sex_Reversed'] = train_df['Sex'].map({'male': 1, 'female': 0})\ntest_df['Sex_Reversed'] = test_df['Sex'].map({'male': 1, 'female': 0})\n\ntrain_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\ntest_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})\n\ntrain_df['Heart_Rate_pct'] = train_df['Heart_Rate'] / (220 - train_df['Age'])\ntest_df['Heart_Rate_pct'] = test_df['Heart_Rate'] / (220 - test_df['Age'])\n\ntrain_df['BMI'] = train_df['Weight'] / (train_df['Height']/100)**2\ntest_df['BMI'] = test_df['Weight'] / (test_df['Height']/100)**2\n\ntrain_df['BMR'] = np.where(\n    train_df['Sex'] == 'female',\n    10 * train_df['Weight'] + 6.25 * train_df['Height'] - 5 * train_df['Age'] - 161,\n    10 * train_df['Weight'] + 6.25 * train_df['Height'] - 5 * train_df['Age'] + 5\n)\ntest_df['BMR'] = np.where(\n    test_df['Sex'] == 'female',\n    10 * test_df['Weight'] + 6.25 * test_df['Height'] - 5 * test_df['Age'] - 161,\n    10 * test_df['Weight'] + 6.25 * test_df['Height'] - 5 * test_df['Age'] + 5\n)\n\ntrain_df['TSI'] = 5 * ((train_df['Body_Temp'] - 36.5) / (41.5 - 36.5)) + 5 * ((train_df['Heart_Rate'] - 60) / ((220 - train_df['Age']) - 60))\ntrain_df['RPE'] = train_df['Heart_Rate_pct'] + 0.1 * (train_df['Body_Temp'] - 37)\ntrain_df['FI'] = (train_df['Heart_Rate_pct'] ** 2) / train_df['Duration']\ntrain_df['CLI'] = (train_df['Heart_Rate'] * train_df['Duration']) / train_df['Weight']\ntrain_df['TLI'] = ((train_df['Body_Temp'] - 36.6) ** 2) * train_df['Duration']\ntrain_df['AMI'] = (train_df['BMR'] * train_df['Heart_Rate_pct']) / train_df['Duration']\ntrain_df['AWI'] = (train_df['Duration'] * train_df['Heart_Rate_pct']) / train_df['Age']\ntrain_df['WLI'] = train_df['Heart_Rate'] * train_df['Duration'] * train_df['Weight']\ntrain_df['VO2_Proxy'] = np.where(\n    train_df['Sex'] == 'female',\n    (0.85 * train_df['Duration']) / (train_df['Heart_Rate_pct'] * train_df['Age']),\n    (1.00 * train_df['Duration']) / (train_df['Heart_Rate_pct'] * train_df['Age']),\n)\ntest_df['TSI'] = 5 * ((test_df['Body_Temp'] - 36.5) / (41.5 - 36.5)) + 5 * ((test_df['Heart_Rate'] - 60) / ((220 - test_df['Age']) - 60))\ntest_df['RPE'] = test_df['Heart_Rate_pct'] + 0.1 * (test_df['Body_Temp'] - 37)\ntest_df['FI'] = (test_df['Heart_Rate_pct'] ** 2) / test_df['Duration']\ntest_df['CLI'] = (test_df['Heart_Rate'] * test_df['Duration']) / test_df['Weight']\ntest_df['TLI'] = ((test_df['Body_Temp'] - 36.6) ** 2) * test_df['Duration']\ntest_df['AMI'] = (test_df['BMR'] * test_df['Heart_Rate_pct']) / test_df['Duration']\ntest_df['AWI'] = (test_df['Duration'] * test_df['Heart_Rate_pct']) / test_df['Age']\ntest_df['WLI'] = test_df['Heart_Rate'] * test_df['Duration'] * test_df['Weight']\ntest_df['VO2_Proxy'] = np.where(\n    test_df['Sex'] == 'female',\n    (0.85 * test_df['Duration']) / (test_df['Heart_Rate_pct'] * test_df['Age']),\n    (1.00 * test_df['Duration']) / (test_df['Heart_Rate_pct'] * test_df['Age']),\n)\n\ntrain_df['Duration_HR'] = train_df['Duration'] * train_df['Heart_Rate']\ntest_df['Duration_HR'] = test_df['Duration'] * test_df['Heart_Rate']\n\ntrain_df['Duration2_HR'] = (train_df['Duration'])**2 * train_df['Heart_Rate']\ntest_df['Duration2_HR'] = (test_df['Duration'])**2 * test_df['Heart_Rate']\n\ntrain_df['Intensity'] = train_df['Heart_Rate'] / train_df['Duration']\ntest_df['Intensity'] = test_df['Heart_Rate'] / test_df['Duration']\n\nfor f1 in ['Duration', 'Heart_Rate', 'Body_Temp']:\n        for f2 in ['Sex', 'Sex_Reversed']:\n            train_df[f'{f1}_x_{f2}'] = train_df[f1] * train_df[f2]\nfor f1 in ['Duration', 'Heart_Rate', 'Body_Temp']:\n        for f2 in ['Sex', 'Sex_Reversed']:\n            test_df[f'{f1}_x_{f2}'] = test_df[f1] * test_df[f2]\n\ntrain_df['Body_Temp'] = train_df['Body_Temp'] - 37.0\ntest_df['Body_Temp'] = test_df['Body_Temp'] - 37.0\n\n# for col in ['Height', 'Weight', 'Heart_Rate', 'Body_Temp']:\n#         for agg in ['min', 'max']:\n#             agg_val = train_df.groupby('Sex')[col].agg(agg).rename(f'Sex_{col}_{agg}')\n#             train_df = train_df.merge(agg_val, on='Sex', how='left')\n# for col in ['Height', 'Weight', 'Heart_Rate', 'Body_Temp']:\n#         for agg in ['min', 'max']:\n#             agg_val = test_df.groupby('Sex')[col].agg(agg).rename(f'Sex_{col}_{agg}')\n#             test_df = test_df.merge(agg_val, on='Sex', how='left')\n\n# Calculate 'Heart_Rate_Ratio' for the training data\ntrain_df['Heart_Rate_Ratio'] = train_df['Heart_Rate'] / train_df['Age']\n# Calculate 'Heart_Rate_Ratio' for the testing data\ntest_df['Heart_Rate_Ratio'] = test_df['Heart_Rate'] / test_df['Age']\n\n# Calculate 'Weight_x_Duration' for the training data\ntrain_df['Weight_x_Duration'] = train_df['Weight'] * train_df['Duration']\n# Calculate 'Weight_x_Duration' for the testing data\ntest_df['Weight_x_Duration'] = test_df['Weight'] * test_df['Duration']\n\n# Calculate 'Height_x_Duration' for the training data\ntrain_df['Height_x_Duration'] = train_df['Height'] * train_df['Duration']\n# Calculate 'Height_x_Duration' for the testing data\ntest_df['Height_x_Duration'] = test_df['Height'] * test_df['Duration']\n\n# Calculate 'Weight_x_Height' for the training data\ntrain_df['Weight_x_Height'] = train_df['Weight'] * train_df['Height']\n# Calculate 'Weight_x_Height' for the testing data\ntest_df['Weight_x_Height'] = test_df['Weight'] * test_df['Height']\n\n# Calculate 'Weight_x_Intensity' for the training data\ntrain_df['Weight_x_Intensity'] = train_df['Weight'] * train_df['Intensity']\n# Calculate 'Weight_x_Intensity' for the testing data\ntest_df['Weight_x_Intensity'] = test_df['Weight'] * test_df['Intensity']\n\n# Calculate 'Height_x_Intensity' for the training data\ntrain_df['Height_x_Intensity'] = train_df['Height'] * train_df['Intensity']\n# Calculate 'Height_x_Intensity' for the testing data\ntest_df['Height_x_Intensity'] = test_df['Height'] * test_df['Intensity']\n\ntrain_df.drop(columns=['Sex_Reversed'], inplace=True)\ntest_df.drop(columns=['Sex_Reversed'], inplace=True)\n\ntrain_df.drop(columns=['id'], inplace=True)\ntest_df.drop(columns=['id'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:38.760581Z","iopub.execute_input":"2025-05-31T08:48:38.760937Z","iopub.status.idle":"2025-05-31T08:48:39.708354Z","shell.execute_reply.started":"2025-05-31T08:48:38.760912Z","shell.execute_reply":"2025-05-31T08:48:39.707279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Features","metadata":{}},{"cell_type":"code","source":"features = train_df.columns.tolist()\nfeatures.remove('Calories')\nprint(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:39.709322Z","iopub.execute_input":"2025-05-31T08:48:39.710475Z","iopub.status.idle":"2025-05-31T08:48:39.716327Z","shell.execute_reply.started":"2025-05-31T08:48:39.710443Z","shell.execute_reply":"2025-05-31T08:48:39.71517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Scaling Numeric Features","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\ntrain_df[features] = scaler.fit_transform(train_df[features])\ntest_df[features] = scaler.transform(test_df[features])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:39.717535Z","iopub.execute_input":"2025-05-31T08:48:39.717879Z","iopub.status.idle":"2025-05-31T08:48:40.71278Z","shell.execute_reply.started":"2025-05-31T08:48:39.717852Z","shell.execute_reply":"2025-05-31T08:48:40.711894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df.head())\nprint(train_df.tail())\n\nprint(test_df.head())\nprint(test_df.tail())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:40.71513Z","iopub.execute_input":"2025-05-31T08:48:40.715461Z","iopub.status.idle":"2025-05-31T08:48:40.762524Z","shell.execute_reply.started":"2025-05-31T08:48:40.715438Z","shell.execute_reply":"2025-05-31T08:48:40.761527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = train_df[features]\ny_train = train_df['Calories']\n\nX_test = test_df[features]\n\nX_train = X_train.fillna(0) # Fill NaNs in training features\nX_test = X_test.fillna(0) # Fill NaNs in test features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:40.763454Z","iopub.execute_input":"2025-05-31T08:48:40.763745Z","iopub.status.idle":"2025-05-31T08:48:41.220811Z","shell.execute_reply.started":"2025-05-31T08:48:40.763721Z","shell.execute_reply":"2025-05-31T08:48:41.21967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"markdown","source":"## RMSLE Scorer","metadata":{}},{"cell_type":"code","source":"def rmsle_scorer(y_true, y_pred):\n    y_pred_positive = np.maximum(y_pred, 0.001) \n    return np.sqrt(mean_squared_log_error(y_true, y_pred_positive))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:41.222072Z","iopub.execute_input":"2025-05-31T08:48:41.22244Z","iopub.status.idle":"2025-05-31T08:48:41.227429Z","shell.execute_reply.started":"2025-05-31T08:48:41.222408Z","shell.execute_reply":"2025-05-31T08:48:41.226148Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## KFold CV","metadata":{}},{"cell_type":"code","source":"def run_kfold_cv(X, y, model, model_name, n_splits, random_state=42):\n    print(f\"\\n--- Starting {n_splits}-Fold Cross-Validation for {model_name} ---\")\n    start_cv_time = time.time() # Start timing for the entire CV process\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    \n    fold_rmsle_scores = []\n    oof_predictions = np.zeros(X.shape[0]) \n    \n    fold_times = [] \n\n    # Iterate through each fold\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n        fold_start_time = time.time() # Start timing for the current fold\n\n        # Split data for the current fold\n        X_train_fold, y_train_fold = X.iloc[train_idx], y.iloc[train_idx]\n        X_val_fold, y_val_fold = X.iloc[val_idx], y.iloc[val_idx]\n        \n        model_fold = model.__class__(**model.get_params()) \n        model_fold.fit(X_train_fold, y_train_fold) # Train the model on the training fold\n        \n        val_preds = model_fold.predict(X_val_fold) # Make predictions on the validation fold\n        \n        val_preds[val_preds < 0] = 0.001 \n        \n        # Store out-of-fold predictions\n        oof_predictions[val_idx] = val_preds\n\n        # Evaluate the model's performance on the validation set for this fold using RMSLE\n        try:\n            fold_rmsle = np.sqrt(mean_squared_log_error(y_val_fold, val_preds))\n            fold_rmsle_scores.append(fold_rmsle)\n        except ValueError as e:\n            print(f\"  Warning: Error calculating RMSLE for Fold {fold + 1} ({model_name}): {e}. Setting RMSLE to NaN.\")\n            fold_rmsle_scores.append(np.nan)\n\n        fold_end_time = time.time() # End timing for the current fold\n        fold_duration = fold_end_time - fold_start_time\n        fold_times.append(fold_duration)\n\n    end_cv_time = time.time() # End timing for the entire CV process\n    total_cv_time = end_cv_time - start_cv_time\n\n    # Summarize results\n    valid_fold_rmsle_scores = [s for s in fold_rmsle_scores if not np.isnan(s)]\n    \n    mean_cv_rmsle = np.nan\n    std_cv_rmsle = np.nan\n    overall_oof_rmsle = np.nan\n\n    if valid_fold_rmsle_scores:\n        mean_cv_rmsle = np.mean(valid_fold_rmsle_scores)\n        std_cv_rmsle = np.std(valid_fold_rmsle_scores)\n        print(f\"\\n--- {n_splits}-Fold CV Summary for {model_name} ---\")\n        print(f\"Average RMSLE: {mean_cv_rmsle:.4f} +/- {std_cv_rmsle:.4f}\")\n    else:\n        print(f\"\\n--- {n_splits}-Fold CV Summary for {model_name} ---\")\n        print(f\"RMSLE calculation failed for all folds.\")\n\n    # Calculate overall OOF RMSLE if possible\n    if y.min() >= 0 and oof_predictions.min() >= 0 and valid_fold_rmsle_scores:\n        try:\n            overall_oof_rmsle = np.sqrt(mean_squared_log_error(y, oof_predictions))\n            print(f\"Overall OOF RMSLE: {overall_oof_rmsle:.4f}\")\n        except ValueError as e:\n            print(f\"Error calculating Overall OOF RMSLE for {model_name}: {e}. Ensure target and predictions are non-negative.\")\n    \n    return {\n        'Model': model_name,\n        'N_Splits': n_splits,\n        'Average RMSLE': mean_cv_rmsle,\n        'Std RMSLE': std_cv_rmsle,\n        'Overall OOF RMSLE': overall_oof_rmsle,\n        'Total CV Time (s)': total_cv_time,\n        'Avg Fold Time (s)': np.mean(fold_times) if fold_times else np.nan\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:41.228799Z","iopub.execute_input":"2025-05-31T08:48:41.229213Z","iopub.status.idle":"2025-05-31T08:48:41.256881Z","shell.execute_reply.started":"2025-05-31T08:48:41.229181Z","shell.execute_reply":"2025-05-31T08:48:41.255764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{}},{"cell_type":"code","source":"n_splits_list = [5]#, 10, 15]#, 30, 50]\n\nRANDOM_STATE = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:41.257886Z","iopub.execute_input":"2025-05-31T08:48:41.258255Z","iopub.status.idle":"2025-05-31T08:48:41.288327Z","shell.execute_reply.started":"2025-05-31T08:48:41.258225Z","shell.execute_reply":"2025-05-31T08:48:41.287267Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optuna","metadata":{}},{"cell_type":"code","source":"# --- General Optuna Objective Function ---\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\ndef general_objective(trial, model_class, model_name_prefix):\n    \"\"\"\n    General objective function for Optuna to optimize various booster models.\n    Minimizes the average RMSLE from 5-fold cross-validation.\n    \"\"\"\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'random_state': RANDOM_STATE,\n        'n_jobs': -1,\n    }\n\n    if model_class == xgb.XGBRegressor:\n        params['objective'] = 'reg:squarederror'\n        params['eval_metric'] = 'rmse'\n        params['min_child_weight'] = trial.suggest_int('min_child_weight', 1, 10)\n        params['colsample_bytree'] = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n        params['gamma'] = trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n        params['lambda'] = trial.suggest_float('lambda', 1e-8, 1.0, log=True) # reg_lambda\n        params['alpha'] = trial.suggest_float('alpha', 1e-8, 1.0, log=True)   # reg_alpha\n        # params['tree_method'] = 'gpu_hist'\n        # params['predictor'] = 'gpu_predictor'\n\n    model = model_class(**params)\n    \n    # cv_results = run_kfold_cv(X_train, y_train, model, model_name_prefix, n_splits=5, random_state=RANDOM_STATE)\n    \n    # return cv_results['Average RMSLE']\n    \n    # Perform train-validation split\n    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n    \n    # Train the model\n    model.fit(X_tr, y_tr)\n    \n    # Predict on validation set\n    y_pred = model.predict(X_val)\n    \n    # Ensure non-negative predictions for RMSLE\n    y_pred = np.clip(y_pred, 0, None)\n    \n    # Calculate RMSLE\n    rmsle = np.sqrt(mean_squared_log_error(y_val, y_pred))\n    \n    return rmsle #cv_results['Average RMSLE']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:41.289428Z","iopub.execute_input":"2025-05-31T08:48:41.289927Z","iopub.status.idle":"2025-05-31T08:48:41.318452Z","shell.execute_reply.started":"2025-05-31T08:48:41.289897Z","shell.execute_reply":"2025-05-31T08:48:41.317267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- Optuna Optimization for XGBoost ---\n# print(\"\\n\" + \"=\"*80)\n# print(\"Starting Optuna Hyperparameter Optimization for XGBoost\")\n# print(\"=\"*80)\n\n# study_xgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE + 1))\n# study_xgb.optimize(lambda trial: general_objective(trial, xgb.XGBRegressor, \"XGBoost_Optuna\"), n_trials=50, show_progress_bar=True)\n\n# print(\"\\nOptuna XGBoost optimization finished.\")\n# print(\"Number of finished trials: \", len(study_xgb.trials))\n# print(\"Best trial (XGBoost):\")\n# trial_xgb = study_xgb.best_trial\n\n# print(\"  Value (Avg RMSLE): \", trial_xgb.value)\n# print(\"  Params: \")\n# for key, value in trial_xgb.params.items():\n#     print(f\"    {key}: {value}\")\n\n# best_xgb_params = trial_xgb.params\n# best_xgb_params['objective'] = 'reg:squarederror'\n# best_xgb_params['eval_metric'] = 'rmse'\n# best_xgb_params['random_state'] = RANDOM_STATE\n# # best_xgb_params['tree_method'] = 'gpu_hist' # Ensure GPU is set for the final model if available\n# # best_xgb_params['predictor'] = 'gpu_predictor' # Ensure GPU is set for the final model if available\n# best_xgb_params['n_jobs'] = -1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:41.319543Z","iopub.execute_input":"2025-05-31T08:48:41.319826Z","iopub.status.idle":"2025-05-31T08:48:41.346975Z","shell.execute_reply.started":"2025-05-31T08:48:41.319805Z","shell.execute_reply":"2025-05-31T08:48:41.345711Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"models = {\n    # \"XGBoost Regressor\": xgb.XGBRegressor(**study_xgb.best_params),\n    \"XGBoost Regressor\": xgb.XGBRegressor(\n    n_estimators=633,  # Start with a large number and use early stopping during training\n    learning_rate=0.014465037740657534,  # Start with a value in the suggested range (0.01 to 0.05)\n    max_depth=10,         # Start in the suggested range (4 to 10)\n    colsample_bytree=0.7126992343210137, # Start in the suggested range (0.6 to 1.0). Lower if many features.\n    subsample=0.834441652086676,       # Start in the suggested range (0.6 to 1.0)\n    min_child_weight=2,\n    reg_alpha=1.9027210521909599e-07,         # You might want to tune this (e.g., 0 to 5)\n    # reg_lambda=1,        # You might want to tune this (e.g., 0 to 5)\n    # reg_gamma=6.358507422599293e-06,\n    reg_lambda=0.12835242746436062,\n    random_state=42,\n    n_jobs=-1\n),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:41.347842Z","iopub.execute_input":"2025-05-31T08:48:41.348079Z","iopub.status.idle":"2025-05-31T08:48:41.371701Z","shell.execute_reply.started":"2025-05-31T08:48:41.348062Z","shell.execute_reply":"2025-05-31T08:48:41.370718Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# KFold and Train and Save CSV","metadata":{}},{"cell_type":"code","source":"# List to store all CV results for comparison table\nall_cv_results = []\n\n# --- Run K-Fold CV for Each Model ---\nfor model_name, model_instance in models.items():\n    print(f\"\\n{'='*80}\\nRunning K-Fold Cross-Validation for: {model_name}\\n{'='*80}\")\n    for n_splits_val in n_splits_list:\n        results = run_kfold_cv(X_train, y_train, model_instance, model_name, n_splits_val, RANDOM_STATE)\n        all_cv_results.append(results)\n    \n    # --- Train on Full X_train and Predict on X_test (after CV for this model) ---\n    print(f\"\\n--- Training {model_name} on full X_train and predicting on X_test ---\")\n    final_model = model_instance.__class__(**model_instance.get_params()) # Create a fresh instance for final training\n    \n    start_time_full_train = time.time()\n    final_model.fit(X_train, y_train)\n    end_time_full_train = time.time()\n    print(f\"Full training complete in {(end_time_full_train - start_time_full_train):.4f} seconds.\")\n\n    start_time_predict = time.time()\n    predictions_test = final_model.predict(X_test)\n    end_time_predict = time.time()\n    print(f\"Predictions made in {(end_time_predict - start_time_predict):.4f} seconds.\")\n\n    # Handle negative predictions for submission file\n    predictions_test[predictions_test < 0] = 0.001 #np.abs(predictions_test)\n    print(predictions_test)\n\n    # Save predictions to CSV\n    submission_df = pd.DataFrame({'id': test_ids, 'Predictions': predictions_test})\n    csv_filename = f'{model_name.replace(\" \", \"_\")}_predictions.csv'\n    submission_df.to_csv(csv_filename, index=False)\n    print(f\"Submission file '{csv_filename}' created successfully.\")\n\n    # Print feature importances if available (for tree-based models)\n    if hasattr(final_model, 'feature_importances_'):\n        print(f\"Feature Importances for {model_name}:\")\n        # Map feature importances to original feature names\n        feature_importances_df = pd.DataFrame({\n            'Feature': X_train.columns,\n            'Importance': final_model.feature_importances_\n        }).sort_values(by='Importance', ascending=False)\n        print(feature_importances_df.to_string(index=False))\n    print(f\"{'-'*80}\") # Separator after each model's full training/prediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:48:41.373182Z","iopub.execute_input":"2025-05-31T08:48:41.37349Z","iopub.status.idle":"2025-05-31T08:55:54.942921Z","shell.execute_reply.started":"2025-05-31T08:48:41.373465Z","shell.execute_reply":"2025-05-31T08:55:54.941691Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Comparison Table","metadata":{}},{"cell_type":"code","source":"# Final Comparison Table\nprint(\"\\n\" + \"=\"*120) # Adjusted width for new columns and more models\nprint(\"                                Cross-Validation Summary Across Different Models and Folds                                \")\nprint(\"=\"*120)\n\n# Create a DataFrame from the results for a nice tabular output\nresults_df = pd.DataFrame(all_cv_results)\n\n# Sort for better comparison: by Model, then by N_Splits\nresults_df = results_df.sort_values(by=['Model', 'N_Splits']).reset_index(drop=True)\n\n# Format the numerical columns for better readability\nresults_df['Average RMSLE'] = results_df['Average RMSLE'].map('{:.4f}'.format)\nresults_df['Std RMSLE'] = results_df['Std RMSLE'].map('{:.4f}'.format)\nresults_df['Overall OOF RMSLE'] = results_df['Overall OOF RMSLE'].map('{:.4f}'.format)\nresults_df['Total CV Time (s)'] = results_df['Total CV Time (s)'].map('{:.4f}'.format)\nresults_df['Avg Fold Time (s)'] = results_df['Avg Fold Time (s)'].map('{:.4f}'.format)\n\n# Print the DataFrame\nprint(results_df.to_string(index=False))\nprint(\"=\"*120)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:55:54.946736Z","iopub.execute_input":"2025-05-31T08:55:54.947063Z","iopub.status.idle":"2025-05-31T08:55:54.966568Z","shell.execute_reply.started":"2025-05-31T08:55:54.947038Z","shell.execute_reply":"2025-05-31T08:55:54.965334Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NN","metadata":{}},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:05:40.436177Z","iopub.execute_input":"2025-05-31T09:05:40.436627Z","iopub.status.idle":"2025-05-31T09:05:40.463821Z","shell.execute_reply.started":"2025-05-31T09:05:40.436602Z","shell.execute_reply":"2025-05-31T09:05:40.462617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q pytorch-tabnet2\n# Import necessary libraries\nimport pandas as pd\nfrom pytorch_tabnet import TabNetRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch import nn\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:05:40.683622Z","iopub.execute_input":"2025-05-31T09:05:40.683945Z","iopub.status.idle":"2025-05-31T09:05:45.407584Z","shell.execute_reply.started":"2025-05-31T09:05:40.683926Z","shell.execute_reply":"2025-05-31T09:05:45.406071Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Load data\ntrain_tabnn = train_df.copy() #pd.read_csv(\"/kaggle/input/playground-series-s5e5/train.csv\")\ntest_tabnn = test_df.copy() #pd.read_csv(\"/kaggle/input/playground-series-s5e5/test.csv\")\nsample_submission_tabnn = pd.read_csv(\"/kaggle/input/playground-series-s5e5/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:05:47.7487Z","iopub.execute_input":"2025-05-31T09:05:47.749092Z","iopub.status.idle":"2025-05-31T09:05:48.61879Z","shell.execute_reply.started":"2025-05-31T09:05:47.749058Z","shell.execute_reply":"2025-05-31T09:05:48.617779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tst_ids = test_tabnn[\"id\"]\ntst_ids = pd.read_csv(\"/kaggle/input/playground-series-s5e5/test.csv\")[\"id\"]\n# trn_ids = train_tabnn[\"id\"]\ntrn_ids = pd.read_csv(\"/kaggle/input/playground-series-s5e5/train.csv\")[\"id\"]\n\ncals = pd.read_csv(\"/kaggle/input/playground-series-s5e5/train.csv\")[\"Calories\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:12:16.462507Z","iopub.execute_input":"2025-05-31T09:12:16.462921Z","iopub.status.idle":"2025-05-31T09:12:16.468565Z","shell.execute_reply.started":"2025-05-31T09:12:16.462899Z","shell.execute_reply":"2025-05-31T09:12:16.467219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nstart_time_predict = time.time()\npredictions_train = final_model.predict(X_train)\nend_time_predict = time.time()\nprint(f\"Train Predictions made in {(end_time_predict - start_time_predict):.4f} seconds.\")\n\n# Handle negative predictions for submission file\npredictions_train[predictions_train < 0] = 0.001 #np.abs(predictions_test)\nprint(predictions_train)\n\n# Save predictions to CSV\ntrn_df = pd.DataFrame({'id': trn_ids, 'Predictions': predictions_train})\ncsv_filename = f'{model_name.replace(\" \", \"_\")}_train_predictions.csv'\ntrn_df.to_csv(csv_filename, index=False)\nprint(f\"Submission file '{csv_filename}' created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:08:28.825729Z","iopub.execute_input":"2025-05-31T09:08:28.826186Z","iopub.status.idle":"2025-05-31T09:09:00.735565Z","shell.execute_reply.started":"2025-05-31T09:08:28.826159Z","shell.execute_reply":"2025-05-31T09:09:00.734543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trn_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:10:03.297877Z","iopub.execute_input":"2025-05-31T09:10:03.298321Z","iopub.status.idle":"2025-05-31T09:10:03.309943Z","shell.execute_reply.started":"2025-05-31T09:10:03.298272Z","shell.execute_reply":"2025-05-31T09:10:03.308954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_tabnn[\"Predictions\"] = predictions_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:11:00.434623Z","iopub.execute_input":"2025-05-31T09:11:00.434967Z","iopub.status.idle":"2025-05-31T09:11:00.441997Z","shell.execute_reply.started":"2025-05-31T09:11:00.434944Z","shell.execute_reply":"2025-05-31T09:11:00.440614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_tabnn.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:11:16.574829Z","iopub.execute_input":"2025-05-31T09:11:16.575199Z","iopub.status.idle":"2025-05-31T09:11:16.590142Z","shell.execute_reply.started":"2025-05-31T09:11:16.575173Z","shell.execute_reply":"2025-05-31T09:11:16.589146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_tabnn = pd.read_csv(\"/kaggle/working/XGBoost_Regressor_train_predictions.csv\")\n\n# train_tabnn['Sex'] = train_tabnn['Sex'].map({'male': 0, 'female': 1})\n# test_tabnn['Sex'] = test_tabnn['Sex'].map({'male': 0, 'female': 1})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:05:47.7487Z","iopub.execute_input":"2025-05-31T09:05:47.749092Z","iopub.status.idle":"2025-05-31T09:05:48.61879Z","shell.execute_reply.started":"2025-05-31T09:05:47.749058Z","shell.execute_reply":"2025-05-31T09:05:48.617779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_tabnn.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:12:35.572022Z","iopub.execute_input":"2025-05-31T09:12:35.573044Z","iopub.status.idle":"2025-05-31T09:12:35.588215Z","shell.execute_reply.started":"2025-05-31T09:12:35.573008Z","shell.execute_reply":"2025-05-31T09:12:35.587181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_tabnn[\"Predictions\"] = predictions_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:13:34.180408Z","iopub.execute_input":"2025-05-31T09:13:34.181102Z","iopub.status.idle":"2025-05-31T09:13:34.188056Z","shell.execute_reply.started":"2025-05-31T09:13:34.181064Z","shell.execute_reply":"2025-05-31T09:13:34.186591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_tabnn.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:13:34.89551Z","iopub.execute_input":"2025-05-31T09:13:34.895813Z","iopub.status.idle":"2025-05-31T09:13:34.911006Z","shell.execute_reply.started":"2025-05-31T09:13:34.895795Z","shell.execute_reply":"2025-05-31T09:13:34.909838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_tabnn.fillna(train_tabnn.median(), inplace=True)\n# test_tabnn.fillna(test_tabnn.median(), inplace=True)\n# sample_submission_tabnn.fillna(sample_submission_tabnn.median(), inplace=True)\n# xgb_tabnn.fillna(xgb_tabnn.median(), inplace=True)\n\n# print(train_tabnn.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T08:59:00.710728Z","iopub.execute_input":"2025-05-31T08:59:00.711063Z","iopub.status.idle":"2025-05-31T08:59:00.715802Z","shell.execute_reply.started":"2025-05-31T08:59:00.711041Z","shell.execute_reply":"2025-05-31T08:59:00.71482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode the categorical variable 'sex'\n# cat_features_tabnn = ['Sex']\n# for col in cat_features_tabnn:\n#     encoder_tabnn = LabelEncoder()\n#     train_tabnn[col] = encoder_tabnn.fit_transform(train_tabnn[col])\n#     test_tabnn[col] = encoder_tabnn.transform(test_tabnn[col])\n\n# train_tabnn['preds']= xgb_tabnn[\"Predictions\"]\n\n# Drop the 'id' column from the features\nX_train_tabnn, X_val_tabnn, y_train_tabnn, y_val_tabnn = train_test_split(\n    train_tabnn, cals, test_size=0.1, random_state=42\n)\n\nprint(X_train_tabnn.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:14:07.425549Z","iopub.execute_input":"2025-05-31T09:14:07.425963Z","iopub.status.idle":"2025-05-31T09:14:07.579038Z","shell.execute_reply.started":"2025-05-31T09:14:07.425937Z","shell.execute_reply":"2025-05-31T09:14:07.577983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MSLELoss(nn.Module):\n    \"\"\"\n    Calculates the Mean Squared Logarithmic Error (MSLE) between\n    predictions and targets.\n\n    MSLE = mean( (log(prediction + 1) - log(target + 1))^2 )\n\n    Args:\n        epsilon (float): A small value added to prediction and target\n                         before taking the logarithm and clamping to\n                         prevent log(0) or log(<negative>).\n                         Ensures the input to log is >= epsilon.\n                         Default: 1e-8\n    \"\"\"\n    def __init__(self, epsilon: float = 1e-8):\n        super().__init__()\n        # Ensure epsilon is positive\n        assert epsilon > 0, \"epsilon must be positive\"\n        self.epsilon = epsilon\n        # Using built-in MSELoss to calculate the mean squared error\n        # of the log-transformed values.\n        self.mse = nn.MSELoss()\n\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Calculates the forward pass for MSLE.\n\n        Args:\n            y_pred (torch.Tensor): The predicted values from the model.\n                                   Expected to be raw outputs (non-negative).\n            y_true (torch.Tensor): The ground truth target values.\n                                   Expected to be non-negative.\n\n        Returns:\n            torch.Tensor: The calculated MSLE loss (scalar).\n        \"\"\"\n        # Ensure inputs have the same shape\n        if y_pred.shape != y_true.shape:\n            raise ValueError(\n                f\"Input shapes must match. Got pred: {y_pred.shape}, true: {y_true.shape}\"\n            )\n\n        # Ensure inputs are non-negative (optional but good practice for MSLE context)\n        # If predictions can be negative, clamping is crucial.\n        # Clamping predictions ensures log input is valid even if model outputs < -1\n        y_pred_clamped = torch.clamp(y_pred, min=0.)\n        # Targets are usually assumed non-negative for MSLE\n        y_true_clamped = torch.clamp(y_true, min=0.)\n\n\n        # Add 1, clamp to ensure input >= epsilon, then take log\n        # Clamping *after* adding 1 is important\n        log_pred = torch.log(torch.clamp(y_pred_clamped + 1, min=self.epsilon))\n        log_true = torch.log(torch.clamp(y_true_clamped + 1, min=self.epsilon))\n\n        # Calculate the Mean Squared Error between the log-transformed values\n        loss = self.mse(log_pred, log_true)\n\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:14:13.200425Z","iopub.execute_input":"2025-05-31T09:14:13.200895Z","iopub.status.idle":"2025-05-31T09:14:13.209719Z","shell.execute_reply.started":"2025-05-31T09:14:13.200871Z","shell.execute_reply":"2025-05-31T09:14:13.208558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = TabNetRegressor(cat_idxs=[0],cat_dims=[2])\nmodel.fit(\n    X_train_tabnn.values,\n    y_train_tabnn.values.reshape(-1,1),\n    eval_set=[(X_val_tabnn.values, y_val_tabnn.values.reshape(-1,1))],\n    eval_metric=['rmsle'],\n    max_epochs=300,\n    patience=50,\n    batch_size=1024*64,\n    loss_fn = MSLELoss()\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:14:20.918082Z","iopub.execute_input":"2025-05-31T09:14:20.918942Z","iopub.status.idle":"2025-05-31T09:33:19.003484Z","shell.execute_reply.started":"2025-05-31T09:14:20.918909Z","shell.execute_reply":"2025-05-31T09:33:19.001666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions\ny_test_tabnn = model.predict(test_tabnn.drop(['id',], axis=1).values)[:, 0]\ny_test_tabnn = np.clip(y_test_tabnn,0,999999)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:33:19.005461Z","iopub.execute_input":"2025-05-31T09:33:19.005795Z","iopub.status.idle":"2025-05-31T09:33:19.588114Z","shell.execute_reply.started":"2025-05-31T09:33:19.005773Z","shell.execute_reply":"2025-05-31T09:33:19.587092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame({\"id\": test_tabnn[\"id\"], \"Predictions\": y_test_tabnn})\nsubmission.to_csv(\"XGB_NN_submission.csv\", index=False)\n\nprint(submission)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T09:33:20.170924Z","iopub.execute_input":"2025-05-31T09:33:20.171343Z","iopub.status.idle":"2025-05-31T09:33:20.751941Z","shell.execute_reply.started":"2025-05-31T09:33:20.171286Z","shell.execute_reply":"2025-05-31T09:33:20.750838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
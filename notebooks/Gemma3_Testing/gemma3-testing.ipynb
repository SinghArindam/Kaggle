{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":282742,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":239467,"modelId":222398},{"sourceId":282751,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":239470,"modelId":222398},{"sourceId":363139,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":301517,"modelId":322000}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Test Gemma3","metadata":{}},{"cell_type":"code","source":"%%time\n# !pip install -U transformers accelerate bitsandbytes torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:48:05.210038Z","iopub.execute_input":"2025-05-24T15:48:05.211147Z","iopub.status.idle":"2025-05-24T15:48:05.219229Z","shell.execute_reply.started":"2025-05-24T15:48:05.211107Z","shell.execute_reply":"2025-05-24T15:48:05.217977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# 2. Import libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:48:05.221387Z","iopub.execute_input":"2025-05-24T15:48:05.221872Z","iopub.status.idle":"2025-05-24T15:48:05.248522Z","shell.execute_reply.started":"2025-05-24T15:48:05.221835Z","shell.execute_reply":"2025-05-24T15:48:05.247377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# 4. Define the model path and load tokenizer and model\nmodel_path = \"/kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\"\n# model_path = \"/kaggle/input/gemma-3/transformers/gemma-3-4b-it/1\"\nprint(f\"Loading model from: {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:48:05.249762Z","iopub.execute_input":"2025-05-24T15:48:05.250205Z","iopub.status.idle":"2025-05-24T15:48:05.271102Z","shell.execute_reply.started":"2025-05-24T15:48:05.250139Z","shell.execute_reply":"2025-05-24T15:48:05.270077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32, # Use bfloat16 for GPUs, float32 for CPU\n    device_map=\"auto\" # This handles loading the model onto the correct device(s)\n)\nmodel.eval() # Set model to evaluation mode\n\nprint(\"Model and tokenizer loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:48:05.273219Z","iopub.execute_input":"2025-05-24T15:48:05.273524Z","iopub.status.idle":"2025-05-24T15:49:14.466933Z","shell.execute_reply.started":"2025-05-24T15:48:05.2735Z","shell.execute_reply":"2025-05-24T15:49:14.465833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# 5. Create a text generation pipeline (optional, but convenient)\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    # device=0 if device == \"cuda\" else -1, # Use 0 for first GPU, -1 for CPU\n    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:49:14.468097Z","iopub.execute_input":"2025-05-24T15:49:14.46847Z","iopub.status.idle":"2025-05-24T15:49:14.794017Z","shell.execute_reply.started":"2025-05-24T15:49:14.468428Z","shell.execute_reply":"2025-05-24T15:49:14.79312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# 6. Define a chat function using Gemma's prompt template\ndef chat_with_ai(user_input):\n    \"\"\"\n    Gets a single response from the Gemma 3 model for a given user input.\n\n    Args:\n        user_input (str): The current user's message.\n\n    Returns:\n        str: The model's response.\n    \"\"\"\n    # Create a simple chat history for a single turn\n    messages = [{\"role\": \"user\", \"content\": user_input}]\n\n    # Apply the chat template to format the single turn conversation\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n    # Generate the response\n    outputs = pipe(\n        prompt,\n        max_new_tokens=200,  # Adjust as needed for desired response length\n        do_sample=True,      # Enable sampling for more creative responses\n        temperature=0.7,     # Control randomness (lower = more deterministic)\n        top_k=50,            # Filter top k tokens\n        top_p=0.95,          # Nucleus sampling\n        repetition_penalty=1.2, # Penalize repeating tokens\n    )\n    generated_text = outputs[0][\"generated_text\"]\n    start_model_response = generated_text.rfind(\"<start_of_turn>model\\n\")\n    if start_model_response != -1:\n        model_response = generated_text[start_model_response + len(\"<start_of_turn>model\\n\"):].strip()\n    else:\n        model_response = generated_text.strip() # Fallback if template parsing fails\n\n    return model_response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:49:14.800937Z","iopub.execute_input":"2025-05-24T15:49:14.801318Z","iopub.status.idle":"2025-05-24T15:49:14.815027Z","shell.execute_reply.started":"2025-05-24T15:49:14.80129Z","shell.execute_reply":"2025-05-24T15:49:14.814056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# # 7. Start the chat!\n# user_input = \"Why is the sky blue?\"\n# response = chat_with_ai(user_input)\n# print(f\"Gemma 3: {response}\")\n# print()\n# print(\"_\"*10)\n# print(\"Timing Information:\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:49:14.816073Z","iopub.execute_input":"2025-05-24T15:49:14.816491Z","execution_failed":"2025-05-24T15:52:05.96Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# General chat test function","metadata":{}},{"cell_type":"code","source":"# %%time\n# print(\"Installing required libraries...\")\n# !pip install -U transformers accelerate bitsandbytes torch\n# print(\"Libraries installed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\ndef get_llm_response(user_query: str, model_path: str, **kwargs) -> str:\n    \"\"\"\n    Generates a response from a specified language model for a given user query.\n\n    This function dynamically loads the tokenizer and model from the provided path,\n    detects the optimal device (GPU/CPU), applies the model's chat template,\n    and generates a response.\n\n    Args:\n        user_query (str): The input query from the user.\n        model_path (str): The local path to the pre-trained model on Kaggle\n                          (e.g., \"/kaggle/input/gemma-3/transformers/gemma-3-4b-it/1\")\n                          or a Hugging Face Hub ID (e.g., \"Qwen/Qwen1.5-1.8B-Chat\").\n                          For Kaggle, ensure the model is added as a dataset\n                          and the path points to its location in /kaggle/input/.\n        **kwargs: Arbitrary keyword arguments to pass to the model's `generate` method.\n                  Common arguments include:\n                  - max_new_tokens (int): Maximum number of tokens to generate. (Default: 200)\n                  - temperature (float): Controls randomness. Lower values (e.g., 0.7)\n                                         make output more deterministic, higher values\n                                         (e.g., 1.0) make it more creative. (Default: 0.7)\n                  - do_sample (bool): Whether to use sampling. Set to True for creative\n                                      responses. (Default: True)\n                  - top_k (int): Filter top k tokens before sampling. (Default: 50)\n                  - top_p (float): Nucleus sampling threshold. (Default: 0.95)\n                  - repetition_penalty (float): Penalizes repeated tokens. (Default: 1.2)\n\n    Returns:\n        str: The generated response from the language model.\n    \"\"\"\n    print(f\"\\n--- Processing query for model: {model_path} ---\")\n\n    try:\n        # Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        print(\"Tokenizer loaded.\")\n\n        # Determine device\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\"Using device: {device}\")\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n            device_map=\"auto\"  # Automatically distributes model across available devices\n        )\n        model.eval()  # Set model to evaluation mode\n        print(\"Model loaded.\")\n\n        # Create the message list for the chat template\n        messages = [{\"role\": \"user\", \"content\": user_query}]\n\n        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        print(f\"Formatted prompt:\\n{prompt}\")\n\n        # Set default generation arguments if not provided in kwargs\n        generation_args = {\n            \"max_new_tokens\": kwargs.get(\"max_new_tokens\", 200),\n            \"do_sample\": kwargs.get(\"do_sample\", True),\n            \"temperature\": kwargs.get(\"temperature\", 0.7),\n            \"top_k\": kwargs.get(\"top_k\", 50),\n            \"top_p\": kwargs.get(\"top_p\", 0.95),\n            \"repetition_penalty\": kwargs.get(\"repetition_penalty\", 1.2),\n        }\n        print(f\"Generation arguments: {generation_args}\")\n\n        # Create a Hugging Face pipeline for simplified generation\n        llm_pipeline = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            # device=0 if device == \"cuda\" else -1, # 0 for first GPU, -1 for CPU\n            torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n        )\n\n        # Generate the response\n        outputs = llm_pipeline(\n            prompt,\n            **generation_args\n        )\n\n        generated_text = outputs[0][\"generated_text\"]\n        \n        try:\n            start_model_response_tag = tokenizer.apply_chat_template([{\"role\": \"model\", \"content\": \"\"}], tokenize=False).strip()\n        except Exception:\n            start_model_response_tag = \"<start_of_turn>model\\n\" \n\n        start_model_response_idx = generated_text.rfind(start_model_response_tag)\n\n        if start_model_response_idx != -1:\n            model_response = generated_text[start_model_response_idx + len(start_model_response_tag):].strip()\n        else:\n            # Fallback if the template parsing is unexpected, return the whole generated text\n            print(\"Warning: Could not parse model's response using chat template tag. Returning full generated text.\")\n            model_response = generated_text.strip()\n\n        return model_response\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        if 'model' in locals() and model is not None:\n            del model\n        if 'tokenizer' in locals() and tokenizer is not None:\n            del tokenizer\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        return f\"Error: Could not generate response. Reason: {e}\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- How to use it in your Kaggle Notebook ---\n\n# For Gemma 3 4B Instruction-tuned:\nGEMMA_3_4B_IT_KAGGLE_PATH = \"/kaggle/input/gemma-3/transformers/gemma-3-4b-it/1\"\n# For Gemma 3 2B Instruction-tuned (if available as a Kaggle dataset):\nGEMMA_3_1B_IT_KAGGLE_PATH = \"/kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\"\n\n# For Qwen 1.5 1.8B Chat (if you add it as a Kaggle dataset, example path):\nQWEN_3_8B_CHAT_KAGGLE_PATH = \"/kaggle/input/qwen-3/transformers/8b/1\"\n\n# If you prefer to download from Hugging Face Hub directly (requires Internet enabled in Kaggle):\n# GEMMA_2B_IT_HF_ID = \"google/gemma-2b-it\"\n# QWEN_1_8B_CHAT_HF_ID = \"Qwen/Qwen1.5-1.8B-Chat\"\n# QWEN_7B_CHAT_HF_ID = \"Qwen/Qwen1.5-7B-Chat\" # For larger Qwen model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n--- Starting LLM response generation examples ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Example 1: Using Gemma 3 4B Instruction-tuned (from Kaggle Input)\n# Make sure you have added 'gemma-3' as a dataset to your notebook.\nif os.path.exists(GEMMA_3_4B_IT_KAGGLE_PATH):\n    print(\"\\n--- Testing with Gemma 3 4B Instruction-tuned (Kaggle Input) ---\")\n    query_gemma = \"Explain the concept of quantum entanglement in simple terms.\"\n    response_gemma = get_llm_response(query_gemma, GEMMA_3_4B_IT_KAGGLE_PATH, max_new_tokens=300, temperature=0.5)\n    print(f\"Gemma 3 (4B-IT) Response: {response_gemma}\")\nelse:\n    print(f\"\\nSkipping Gemma 3 4B-IT test: Model path not found at {GEMMA_3_4B_IT_KAGGLE_PATH}. Please add it as a Kaggle dataset.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nprint(\"\\n--- Testing with Qwen3 8B Chat ---\")\nquery_qwen = \"Write a short, creative story about a cat who can talk and flies to the moon.\"\nresponse_qwen = get_llm_response(query_qwen, QWEN_3_8B_CHAT_KAGGLE_PATH, max_new_tokens=250, temperature=0.8, top_p=0.9)\nprint(f\"Qwen3 (8B-Chat) Response: {response_qwen}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Example 3: Using Gemma 1B Instruction-tuned\nprint(\"\\n--- Testing with Gemma 1B Instruction-tuned ---\")\nquery_gemma_1b = \"What are the key benefits of regular exercise for mental health?\"\nresponse_gemma_1b = get_llm_response(query_gemma_1b, GEMMA_3_1B_IT_KAGGLE_PATH, max_new_tokens=150)\nprint(f\"Gemma 1B (IT) Response: {response_gemma_1b}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# Example 4: A more complex query or role-play\nprint(\"\\n--- Testing with a role-play query (using Gemma 3 4B-IT if available) ---\")\nrole_play_query = \"You are a wise old wizard. Tell me a magical riddle.\"\nif os.path.exists(GEMMA_3_4B_IT_KAGGLE_PATH):\n    response_role_play = get_llm_response(role_play_query, GEMMA_3_4B_IT_KAGGLE_PATH, max_new_tokens=180, temperature=0.9)\n    print(f\"Gemma 3 (4B-IT) Wizard's Riddle: {response_role_play}\")\nelse:\n    print(f\"\\nSkipping role-play test: Gemma 3 4B-IT model path not found at {GEMMA_3_4B_IT_KAGGLE_PATH}.\")\n    print(\"Consider using a Hugging Face ID if you have internet access enabled, e.g., using QWEN_1_8B_CHAT_HF_ID instead.\")\n\nprint(\"\\n--- LLM response generation examples completed ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
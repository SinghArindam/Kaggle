{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4419969,"sourceType":"datasetVersion","datasetId":2553751}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction:\n\nThis project focuses on predicting battery voltage using deep learning techniques, particularly **Long Short-Term Memory (LSTM)** networks. \n\n**Dataset Used :** NASA Battery Dataset\n\n**Aim :** \nTo build a robust predictive model that can forecast battery voltage based on parameters like:\n- Voltage_measured\n- Current_measured\n- Temperature_measured\n- Current_load\n- Voltage_load\n- Time","metadata":{"_uuid":"5cab0bf5-1186-4cf7-84aa-da5e92410abf","_cell_guid":"f3778612-b174-4c2d-9566-ec07203e8131","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Imports","metadata":{"_uuid":"63b77e15-cf08-4221-9a1d-3626ba2fb925","_cell_guid":"b270bc1e-f3ae-4019-b976-e3da1868ebfd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport glob\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch","metadata":{"_uuid":"e9524721-c3e9-4f39-871d-0232fcc59d3e","_cell_guid":"501710d8-de6a-465c-b252-17e7b3a20901","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:08:52.063059Z","iopub.execute_input":"2025-04-28T09:08:52.063328Z","iopub.status.idle":"2025-04-28T09:09:00.972889Z","shell.execute_reply.started":"2025-04-28T09:08:52.063308Z","shell.execute_reply":"2025-04-28T09:09:00.971948Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{"_uuid":"758e3cc2-8239-4fd9-9193-40152853738c","_cell_guid":"fc272658-6bb2-4930-829a-9c843bc38df4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","metadata":{"_uuid":"8668f56a-101d-4e23-9892-6c0b151ce09e","_cell_guid":"12bb8779-29aa-4bd1-b12d-edc2f12f93fd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:09:00.973863Z","iopub.execute_input":"2025-04-28T09:09:00.974376Z","iopub.status.idle":"2025-04-28T09:09:00.979335Z","shell.execute_reply.started":"2025-04-28T09:09:00.974343Z","shell.execute_reply":"2025-04-28T09:09:00.978252Z"},"scrolled":true,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Read & Merge all valid CSVs with Battery_ID\n\n- Reads and merges NASA Battery Dataset CSVs\n- Filters valid files with key features\n- Assigns unique Battery_ID\n- Sorts data by time\n- Combines into a single dataset\n- Enables time-series analysis and ML (e.g., LSTMs) for battery health and voltage prediction","metadata":{"_uuid":"12a34c73-3bd2-4cc9-a7ed-440f0d2bcfb0","_cell_guid":"4f54aa0f-fcb2-4634-a80d-c4800edaba61","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"directory_path = '/kaggle/input/cleaned_dataset/data/'\n\nrequired_features = ['Voltage_measured', 'Current_measured', 'Temperature_measured', \n                     'Current_load', 'Voltage_load', 'Time']","metadata":{"_uuid":"785cdcd1-5917-4b53-92d6-49ab0ee723b0","_cell_guid":"3a678726-98d9-4268-9ff3-7610a48676ec","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:09:00.980832Z","iopub.execute_input":"2025-04-28T09:09:00.981161Z","iopub.status.idle":"2025-04-28T09:09:01.002706Z","shell.execute_reply.started":"2025-04-28T09:09:00.981132Z","shell.execute_reply":"2025-04-28T09:09:01.001733Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def merge_time_series_with_id(directory_path):\n    all_files = glob.glob(os.path.join(directory_path, '*.csv'))\n    print(f\"Found {len(all_files)} CSV files in {directory_path}\")\n\n    if not all_files:\n        raise ValueError(f\"No CSV files found in {directory_path}\")\n\n    selected_files = []\n    for file in all_files:\n        try:\n            df = pd.read_csv(file, nrows=1)\n            if set(required_features).issubset(df.columns):\n                selected_files.append(file)\n            else:\n                pass\n                # print(f\"File {file} skipped: Missing some required features {set(required_features) - set(df.columns)}\")\n        except Exception as e:\n            print(f\"Error reading {file}: {e}\")\n\n    if not selected_files:\n        raise ValueError(f\"No files contain all required features: {required_features}\")\n\n    data_list = []\n    for i, file in enumerate(selected_files):\n        try:\n            df = pd.read_csv(file, usecols=required_features)\n            df['Battery_ID'] = f'Battery_{i+1}'\n            df = df.sort_values(by=[\"Battery_ID\", \"Time\"]).reset_index(drop=True)\n            data_list.append(df)\n            # print(f\"Processed {file} as Battery_{i+1}\")\n        except Exception as e:\n            print(f\"Error processing {file}: {e}\")\n\n    if not data_list:\n        raise ValueError(\"No dataframes to concatenate after processing\")\n\n    merged_df = pd.concat(data_list, ignore_index=True)\n    print(f\"\\nSuccessfully merged {len(selected_files)} files!...\")\n    return merged_df","metadata":{"_uuid":"9142a86b-f162-43e7-bb0e-c62136fc4c69","_cell_guid":"4877b0c6-d13b-4789-a634-e1e5355ce5bc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:09:01.02384Z","iopub.execute_input":"2025-04-28T09:09:01.024169Z","iopub.status.idle":"2025-04-28T09:09:01.03727Z","shell.execute_reply.started":"2025-04-28T09:09:01.024139Z","shell.execute_reply":"2025-04-28T09:09:01.036274Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Example usage: Merge all valid files","metadata":{"_uuid":"33a2eb90-5f7b-4940-8e14-bf47b1a1b4c9","_cell_guid":"ffd4e880-c367-4953-ab83-9d2e1b561f4b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"final_df = merge_time_series_with_id(directory_path)","metadata":{"_uuid":"48524892-2fc1-4ed0-a94d-9841f3ba8a5f","_cell_guid":"fe9509e6-6a83-4953-903e-5558807dd667","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:10:45.354256Z","iopub.execute_input":"2025-04-28T09:10:45.354785Z","iopub.status.idle":"2025-04-28T09:11:21.182791Z","shell.execute_reply.started":"2025-04-28T09:10:45.354753Z","shell.execute_reply":"2025-04-28T09:11:21.181857Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA) and Preprocessing\n\n- **Exploratory Data Analysis (EDA)**:\n  - Understand the dataset by visualizing trends.\n  - Identify patterns in battery behavior.\n  - Detect anomalies in battery behavior.\n  - Analyze key features (voltage, current, temperature).\n  - Gain insights into feature distributions and correlations.\n\n- **Preprocessing**:\n  - Clean the data.\n  - Handle missing values.\n  - Normalize numerical features.\n  - Structure data into a time-series format for LSTM models.\n  - Ensure the dataset is well-prepared for training.\n  - Improve model performance.","metadata":{"_uuid":"9b760fe5-36fc-4b48-addf-d6147592dbb7","_cell_guid":"33cfbf59-ac46-44c8-a964-0f9218b80036","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"final_df.head()","metadata":{"_uuid":"80c8e4d1-f13f-4856-a464-e9e4397d310e","_cell_guid":"49bcfb31-1480-4a3c-aa1a-5ea22590b9be","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:21.184429Z","iopub.execute_input":"2025-04-28T09:11:21.184714Z","iopub.status.idle":"2025-04-28T09:11:21.210509Z","shell.execute_reply.started":"2025-04-28T09:11:21.184691Z","shell.execute_reply":"2025-04-28T09:11:21.209688Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.info()","metadata":{"_uuid":"42056cc2-094b-40a0-a6fa-fd84ba493b3f","_cell_guid":"19f2f8c1-0597-413c-bb71-202584b91417","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:21.2112Z","iopub.execute_input":"2025-04-28T09:11:21.211429Z","iopub.status.idle":"2025-04-28T09:11:21.296535Z","shell.execute_reply.started":"2025-04-28T09:11:21.211394Z","shell.execute_reply":"2025-04-28T09:11:21.295649Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.describe()","metadata":{"_uuid":"35689573-3408-42be-bb31-9d06d38b4346","_cell_guid":"7793cd35-0994-4ccf-81f3-0c49b83c87da","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:21.298026Z","iopub.execute_input":"2025-04-28T09:11:21.298441Z","iopub.status.idle":"2025-04-28T09:11:21.488003Z","shell.execute_reply.started":"2025-04-28T09:11:21.298394Z","shell.execute_reply":"2025-04-28T09:11:21.487207Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create Charge_Discharge Feature (1 for Charging, 0 for Discharging)","metadata":{"_uuid":"ef0583fb-8792-4c83-91f1-d53dd5a18cbd","_cell_guid":"21e8fef0-d4fa-4e9d-9cf2-8723e50454df","collapsed":false,"execution":{"iopub.status.busy":"2025-04-27T17:05:54.818286Z","iopub.execute_input":"2025-04-27T17:05:54.818604Z","iopub.status.idle":"2025-04-27T17:05:55.155409Z","shell.execute_reply.started":"2025-04-27T17:05:54.81858Z","shell.execute_reply":"2025-04-27T17:05:55.154597Z"},"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"final_df['Charge_Discharge'] = final_df['Current_measured'].apply(lambda x: 1 if x > 0 else 0)","metadata":{"_uuid":"ef0583fb-8792-4c83-91f1-d53dd5a18cbd","_cell_guid":"21e8fef0-d4fa-4e9d-9cf2-8723e50454df","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:30.948794Z","iopub.execute_input":"2025-04-28T09:11:30.949134Z","iopub.status.idle":"2025-04-28T09:11:31.264057Z","shell.execute_reply.started":"2025-04-28T09:11:30.949103Z","shell.execute_reply":"2025-04-28T09:11:31.26318Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Convert Current_measured to absolute value","metadata":{"_uuid":"ef0583fb-8792-4c83-91f1-d53dd5a18cbd","_cell_guid":"21e8fef0-d4fa-4e9d-9cf2-8723e50454df","collapsed":false,"execution":{"iopub.status.busy":"2025-04-27T17:05:54.818286Z","iopub.execute_input":"2025-04-27T17:05:54.818604Z","iopub.status.idle":"2025-04-27T17:05:55.155409Z","shell.execute_reply.started":"2025-04-27T17:05:54.81858Z","shell.execute_reply":"2025-04-27T17:05:55.154597Z"},"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"final_df['Current_measured'] = final_df['Current_measured'].abs()","metadata":{"_uuid":"ef0583fb-8792-4c83-91f1-d53dd5a18cbd","_cell_guid":"21e8fef0-d4fa-4e9d-9cf2-8723e50454df","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:32.059887Z","iopub.execute_input":"2025-04-28T09:11:32.06018Z","iopub.status.idle":"2025-04-28T09:11:32.076306Z","shell.execute_reply.started":"2025-04-28T09:11:32.06016Z","shell.execute_reply":"2025-04-28T09:11:32.074816Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.head()","metadata":{"_uuid":"6a534830-09d8-47bc-83fc-aa40dacf7990","_cell_guid":"1983ca37-a1a2-4a7c-865e-5f16817bb5a9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:34.98429Z","iopub.execute_input":"2025-04-28T09:11:34.985653Z","iopub.status.idle":"2025-04-28T09:11:35.003783Z","shell.execute_reply.started":"2025-04-28T09:11:34.985605Z","shell.execute_reply":"2025-04-28T09:11:35.002791Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot","metadata":{"_uuid":"320c8188-3972-4250-9110-93a3bdb1f6f5","_cell_guid":"924ea0cc-bd9c-4c67-9d39-948fdcce9834","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.boxplot(data=final_df[['Voltage_measured', 'Current_measured', 'Temperature_measured', 'Current_load', 'Voltage_load']])\nplt.xticks(rotation=45)\nplt.title(\"Boxplot for Outlier Detection\")\nplt.show()","metadata":{"_uuid":"79a706e3-da20-4df4-9a49-d69a19ad945a","_cell_guid":"d5b61456-b144-42f2-b046-c44aab6407a4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:38.814158Z","iopub.execute_input":"2025-04-28T09:11:38.8145Z","iopub.status.idle":"2025-04-28T09:11:39.744892Z","shell.execute_reply.started":"2025-04-28T09:11:38.814475Z","shell.execute_reply":"2025-04-28T09:11:39.743912Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in ['Voltage_measured', 'Current_measured', 'Current_load']:\n    final_df[col] = final_df[col].clip(lower=final_df[col].quantile(0.05),\n                                       upper=final_df[col].quantile(0.95))","metadata":{"_uuid":"55408b54-dc5d-4627-a946-921fbd5d1118","_cell_guid":"899527fd-bf3a-484e-bef1-908a8bf8c5dc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:39.769072Z","iopub.execute_input":"2025-04-28T09:11:39.769379Z","iopub.status.idle":"2025-04-28T09:11:39.875559Z","shell.execute_reply.started":"2025-04-28T09:11:39.769355Z","shell.execute_reply":"2025-04-28T09:11:39.874768Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"q_low, q_high = final_df['Voltage_load'].quantile([0.001, 0.999])\nfinal_df = final_df[(final_df['Voltage_load'] >= q_low) & (final_df['Voltage_load'] <= q_high)]","metadata":{"_uuid":"d36cb83c-a645-43fb-811d-1be785ab5943","_cell_guid":"b117371d-5789-4711-bf47-fc2c4be1b004","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:42.977104Z","iopub.execute_input":"2025-04-28T09:11:42.977443Z","iopub.status.idle":"2025-04-28T09:11:43.053646Z","shell.execute_reply.started":"2025-04-28T09:11:42.977394Z","shell.execute_reply":"2025-04-28T09:11:43.052405Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.describe()","metadata":{"_uuid":"f6cb4862-93e3-4a3f-9a8e-039dc31b9503","_cell_guid":"7b989bdc-9dec-478c-8d9c-8fcd0b1800d6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:43.433213Z","iopub.execute_input":"2025-04-28T09:11:43.433545Z","iopub.status.idle":"2025-04-28T09:11:43.673234Z","shell.execute_reply.started":"2025-04-28T09:11:43.433521Z","shell.execute_reply":"2025-04-28T09:11:43.672361Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.boxplot(data=final_df[['Voltage_measured', 'Current_measured', 'Temperature_measured', 'Current_load', 'Voltage_load']])\nplt.xticks(rotation=45)\nplt.title(\"Boxplot for Outlier Detection\")\nplt.show()","metadata":{"_uuid":"41ecdf9b-2874-4bda-b6d9-ff81f2e51ae5","_cell_guid":"3f14b12d-43dd-40bf-af4a-ffff08722f81","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:44.094006Z","iopub.execute_input":"2025-04-28T09:11:44.094313Z","iopub.status.idle":"2025-04-28T09:11:44.845242Z","shell.execute_reply.started":"2025-04-28T09:11:44.09429Z","shell.execute_reply":"2025-04-28T09:11:44.844349Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nsns.histplot(final_df[\"Voltage_measured\"], kde=True, bins=50, color=\"blue\", alpha=0.7)\nplt.title(\"Voltage Distribution Across Batteries\")\nplt.xlabel(\"Voltage (V)\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"_uuid":"19bd5ea2-982a-4ee5-9255-25d8c194bb69","_cell_guid":"d9a97054-dae4-4bac-a7da-4675b25b4dfb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:48.048088Z","iopub.execute_input":"2025-04-28T09:11:48.048509Z","iopub.status.idle":"2025-04-28T09:11:51.431335Z","shell.execute_reply.started":"2025-04-28T09:11:48.048482Z","shell.execute_reply":"2025-04-28T09:11:51.430226Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.scatterplot(data=final_df, x=\"Time\", y=\"Temperature_measured\", hue=\"Battery_ID\", alpha=0.5, legend=False)\nplt.title(\"Temperature vs. Time for Different Batteries\")\nplt.xlabel(\"Time (Seconds)\")\nplt.ylabel(\"Temperature (°C)\")\nplt.show()","metadata":{"_uuid":"33e19207-2781-4a60-89ab-22440f479810","_cell_guid":"eab26648-e6f1-4cb1-80d4-a3221f0a2d30","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:11:51.432878Z","iopub.execute_input":"2025-04-28T09:11:51.433615Z","iopub.status.idle":"2025-04-28T09:12:08.372321Z","shell.execute_reply.started":"2025-04-28T09:11:51.43359Z","shell.execute_reply":"2025-04-28T09:12:08.371376Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Explanation of the Feature Correlation Heatmap:\nThis heatmap visualizes the correlation between different features in the dataset, where values range from -1 to 1:\n\n### Strong Correlation (Closer to 1 or -1):\n1. Current_measured and Current_load have a very high positive correlation (0.97), indicating that as one increases, the other also increases.\n2. Voltage_measured and Current_measured show a moderate negative correlation (-0.35), suggesting that higher current measurements may lead to lower voltage.\n3. Time has a negative correlation with Current_measured (-0.45) and Current_load (-0.44), indicating that as time progresses, these values tend to decrease.\n   \n### Weak Correlation (Closer to 0):\n1. Charge_Discharge has very weak correlations with most variables, suggesting that charge/discharge state doesn't significantly influence other numerical features.\n2. Temperature_measured shows mild correlations with Current_measured (0.31) and Current_load (0.29), indicating that temperature may slightly increase as current increases.\n3. This heatmap helps identify which features strongly impact each other, guiding feature selection and model optimization.","metadata":{"_uuid":"6f070e5d-287a-4bd0-95e2-b27548a4d3da","_cell_guid":"da4b670d-a095-4ae8-95bb-3432d485df63","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"numeric_df = final_df.select_dtypes(include=['number'])","metadata":{"_uuid":"500ca4f6-3efe-4a2e-a9b1-34e7e568eb1f","_cell_guid":"8898e2a8-0524-4ed5-af38-ea13b56387c5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:12:09.023046Z","iopub.execute_input":"2025-04-28T09:12:09.023368Z","iopub.status.idle":"2025-04-28T09:12:09.070251Z","shell.execute_reply.started":"2025-04-28T09:12:09.023345Z","shell.execute_reply":"2025-04-28T09:12:09.069399Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.heatmap(numeric_df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()","metadata":{"_uuid":"3c9fb65c-61a1-45da-9903-6e84f40ab826","_cell_guid":"e7262a37-9eb4-4b68-9d75-d421ba1cf1dc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:12:09.513167Z","iopub.execute_input":"2025-04-28T09:12:09.513489Z","iopub.status.idle":"2025-04-28T09:12:10.032051Z","shell.execute_reply.started":"2025-04-28T09:12:09.513465Z","shell.execute_reply":"2025-04-28T09:12:10.03062Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RandomForestRegressor","metadata":{"_uuid":"39ab4d44-1870-4480-b6e5-2d3bbd6c6fb7","_cell_guid":"88f4611a-e6ae-45ea-8617-d672f669446b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nX = final_df.drop(columns=['Voltage_measured', 'Time', 'Battery_ID'])  # Predicting Voltage, so drop it\ny = final_df['Voltage_measured']\n\nmodel = RandomForestRegressor()\nmodel.fit(X, y)\n\n# Plot feature importance\nimportances = model.feature_importances_\nfeatures = X.columns\nsorted_idx = np.argsort(importances)\n\nplt.figure(figsize=(8,5))\nplt.barh(range(len(importances)), importances[sorted_idx], align='center')\nplt.yticks(range(len(importances)), [features[i] for i in sorted_idx])\nplt.xlabel(\"Feature Importance Score\")\nplt.title(\"Feature Importance (Random Forest)\")\nplt.show()","metadata":{"_uuid":"71ef8f35-6598-4a93-9dd2-766f483fb631","_cell_guid":"e296285f-fce5-4bc0-8e5d-befff6278e04","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:12:15.878005Z","iopub.execute_input":"2025-04-28T09:12:15.87832Z","iopub.status.idle":"2025-04-28T09:18:26.919753Z","shell.execute_reply.started":"2025-04-28T09:12:15.878297Z","shell.execute_reply":"2025-04-28T09:18:26.91869Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df = final_df.drop(columns=['Temperature_measured', 'Charge_Discharge'])","metadata":{"_uuid":"5b510424-9f32-481a-91fd-63b3f682247a","_cell_guid":"84683951-b925-4758-9d02-7870815fa633","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:26.921321Z","iopub.execute_input":"2025-04-28T09:18:26.921671Z","iopub.status.idle":"2025-04-28T09:18:26.94716Z","shell.execute_reply.started":"2025-04-28T09:18:26.921649Z","shell.execute_reply":"2025-04-28T09:18:26.946003Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df","metadata":{"_uuid":"606e48b5-efdc-4c86-8eec-3a528ce1e143","_cell_guid":"502d8365-12a0-4292-89af-1d6e95cdb27c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:26.948263Z","iopub.execute_input":"2025-04-28T09:18:26.948626Z","iopub.status.idle":"2025-04-28T09:18:26.963072Z","shell.execute_reply.started":"2025-04-28T09:18:26.948592Z","shell.execute_reply":"2025-04-28T09:18:26.962228Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr_matrix = final_df[['Current_measured', 'Current_load']].corr()\nprint(corr_matrix)","metadata":{"_uuid":"6b068c03-4a0a-4bef-9386-4a73e37fd859","_cell_guid":"1c9ab54c-bd44-4f6b-addd-dc40ae896794","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:26.965101Z","iopub.execute_input":"2025-04-28T09:18:26.965359Z","iopub.status.idle":"2025-04-28T09:18:27.010938Z","shell.execute_reply.started":"2025-04-28T09:18:26.965339Z","shell.execute_reply":"2025-04-28T09:18:27.00997Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation between Current_measured & Current_load\")\nplt.show()","metadata":{"_uuid":"ecc170e9-308c-49ff-96b3-d5902a3b2501","_cell_guid":"75b67398-1301-4e92-9e06-f9cdbb1749c6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:27.011833Z","iopub.execute_input":"2025-04-28T09:18:27.012086Z","iopub.status.idle":"2025-04-28T09:18:27.20111Z","shell.execute_reply.started":"2025-04-28T09:18:27.012067Z","shell.execute_reply":"2025-04-28T09:18:27.200259Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df = final_df.drop(columns=['Current_load'])","metadata":{"_uuid":"9728d9bc-8d83-4d96-bdde-0ed795b15b3e","_cell_guid":"4ad25458-3e85-4176-bff2-7f67712be4b1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:27.202006Z","iopub.execute_input":"2025-04-28T09:18:27.202274Z","iopub.status.idle":"2025-04-28T09:18:27.222009Z","shell.execute_reply.started":"2025-04-28T09:18:27.202247Z","shell.execute_reply":"2025-04-28T09:18:27.220836Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df","metadata":{"_uuid":"dc9509bd-ad35-4fda-a51d-6f1ebc157b0e","_cell_guid":"123b2623-39dc-4c1f-894c-d5957ddd36e3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:27.223207Z","iopub.execute_input":"2025-04-28T09:18:27.223771Z","iopub.status.idle":"2025-04-28T09:18:27.236193Z","shell.execute_reply.started":"2025-04-28T09:18:27.22374Z","shell.execute_reply":"2025-04-28T09:18:27.235244Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\nfinal_df['Battery_ID'] = encoder.fit_transform(final_df['Battery_ID'])","metadata":{"_uuid":"e11865df-96d1-40e0-9861-71690d94c570","_cell_guid":"47f80258-17c3-4c78-a806-181c473e37c9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:27.237362Z","iopub.execute_input":"2025-04-28T09:18:27.237757Z","iopub.status.idle":"2025-04-28T09:18:27.387297Z","shell.execute_reply.started":"2025-04-28T09:18:27.237727Z","shell.execute_reply":"2025-04-28T09:18:27.386393Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df","metadata":{"_uuid":"fbeacd52-d887-421d-9a0f-0d608cf7fda9","_cell_guid":"081cc395-1a12-4a89-af0f-22043730af80","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:27.388249Z","iopub.execute_input":"2025-04-28T09:18:27.388539Z","iopub.status.idle":"2025-04-28T09:18:27.401925Z","shell.execute_reply.started":"2025-04-28T09:18:27.388517Z","shell.execute_reply":"2025-04-28T09:18:27.400785Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nnum_features = ['Voltage_measured', 'Current_measured', 'Voltage_load', 'Time','Battery_ID']\n\nscaler = MinMaxScaler()\nfinal_df[num_features] = scaler.fit_transform(final_df[num_features])","metadata":{"_uuid":"be39ee08-2709-4ee0-afa0-6fb1d513ad68","_cell_guid":"dae6671c-762f-4fe0-94d9-c08c6465fbdf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:27.404734Z","iopub.execute_input":"2025-04-28T09:18:27.405067Z","iopub.status.idle":"2025-04-28T09:18:27.510581Z","shell.execute_reply.started":"2025-04-28T09:18:27.40503Z","shell.execute_reply":"2025-04-28T09:18:27.509524Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df","metadata":{"_uuid":"a8ebaa60-64aa-4641-ba68-b8b4f9379c53","_cell_guid":"775976d5-a3b7-48f2-851c-00eb43e1ec7c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:27.511677Z","iopub.execute_input":"2025-04-28T09:18:27.511999Z","iopub.status.idle":"2025-04-28T09:18:27.524808Z","shell.execute_reply.started":"2025-04-28T09:18:27.511975Z","shell.execute_reply":"2025-04-28T09:18:27.52394Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define target variable (Voltage to predict)","metadata":{"_uuid":"c68555a8-77d2-45d8-a2b8-387cdf7b1fb9","_cell_guid":"11fe4311-49ab-45a4-b35d-683a03f39269","collapsed":false,"execution":{"iopub.status.busy":"2025-04-27T17:14:00.964205Z","iopub.execute_input":"2025-04-27T17:14:00.964589Z","iopub.status.idle":"2025-04-27T17:14:01.005908Z","shell.execute_reply.started":"2025-04-27T17:14:00.964555Z","shell.execute_reply":"2025-04-27T17:14:01.004909Z"},"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"target_col = 'Voltage_measured'","metadata":{"_uuid":"c68555a8-77d2-45d8-a2b8-387cdf7b1fb9","_cell_guid":"11fe4311-49ab-45a4-b35d-683a03f39269","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:27.525912Z","iopub.execute_input":"2025-04-28T09:18:27.526225Z","iopub.status.idle":"2025-04-28T09:18:27.544186Z","shell.execute_reply.started":"2025-04-28T09:18:27.526199Z","shell.execute_reply":"2025-04-28T09:18:27.543143Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define features (excluding target)","metadata":{"_uuid":"c68555a8-77d2-45d8-a2b8-387cdf7b1fb9","_cell_guid":"11fe4311-49ab-45a4-b35d-683a03f39269","collapsed":false,"execution":{"iopub.status.busy":"2025-04-27T17:14:00.964205Z","iopub.execute_input":"2025-04-27T17:14:00.964589Z","iopub.status.idle":"2025-04-27T17:14:01.005908Z","shell.execute_reply.started":"2025-04-27T17:14:00.964555Z","shell.execute_reply":"2025-04-27T17:14:01.004909Z"},"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"feature_cols = ['Current_measured', 'Voltage_load', 'Time', 'Battery_ID']","metadata":{"_uuid":"c68555a8-77d2-45d8-a2b8-387cdf7b1fb9","_cell_guid":"11fe4311-49ab-45a4-b35d-683a03f39269","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:27.545271Z","iopub.execute_input":"2025-04-28T09:18:27.545991Z","iopub.status.idle":"2025-04-28T09:18:27.562183Z","shell.execute_reply.started":"2025-04-28T09:18:27.545956Z","shell.execute_reply":"2025-04-28T09:18:27.561299Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = final_df[feature_cols].values\ny = final_df[target_col].values","metadata":{"_uuid":"c68555a8-77d2-45d8-a2b8-387cdf7b1fb9","_cell_guid":"11fe4311-49ab-45a4-b35d-683a03f39269","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:27.563473Z","iopub.execute_input":"2025-04-28T09:18:27.563797Z","iopub.status.idle":"2025-04-28T09:18:27.607145Z","shell.execute_reply.started":"2025-04-28T09:18:27.563766Z","shell.execute_reply":"2025-04-28T09:18:27.606215Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 80% Train, 20% Test (Keeping sequential order)\ntrain_size = int(0.8 * len(final_df))\n\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]\n\nprint(f\"Train Size: {len(X_train)}, Test Size: {len(X_test)}...\")","metadata":{"_uuid":"8f25290e-9444-4a9f-9621-8828646f5bad","_cell_guid":"94affa15-a796-4485-9529-59cebb5169cf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:27.60828Z","iopub.execute_input":"2025-04-28T09:18:27.608625Z","iopub.status.idle":"2025-04-28T09:18:27.614998Z","shell.execute_reply.started":"2025-04-28T09:18:27.608595Z","shell.execute_reply":"2025-04-28T09:18:27.614066Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sequence_length = 30  # Using 30 past time steps\n\nimport numpy as np\n\ndef create_sequences(X, y, seq_length):\n    Xs, ys = [], []\n    for i in range(len(X) - seq_length):\n        Xs.append(X[i : i + seq_length])  # Select past `seq_length` values\n        ys.append(y[i + seq_length])  # Predict the next value\n    return np.array(Xs), np.array(ys)\n\n# Convert train and test sets into sequences\nX_train_seq, y_train_seq = create_sequences(X_train, y_train, sequence_length)\nX_test_seq, y_test_seq = create_sequences(X_test, y_test, sequence_length)\n\nprint(f\"X_train shape: {X_train_seq.shape}, y_train shape: {y_train_seq.shape}\")\nprint(f\"X_test shape: {X_test_seq.shape}, y_test shape: {y_test_seq.shape}\")","metadata":{"_uuid":"d0094bee-a728-4d81-a92d-050b5d7c637c","_cell_guid":"0539c2a5-3ba3-4bb9-856a-aad9ac611d09","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:18:27.616115Z","iopub.execute_input":"2025-04-28T09:18:27.616385Z","iopub.status.idle":"2025-04-28T09:18:28.92072Z","shell.execute_reply.started":"2025-04-28T09:18:27.616365Z","shell.execute_reply":"2025-04-28T09:18:28.919655Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LSTM for Battery Voltage Prediction\n\n- **Why LSTMs?**\n  - Specialized RNNs for sequential dependencies in time-series data.\n  - Overcome vanishing gradient problem.\n  - Retain information over long time steps.\n  - Ideal for battery voltage forecasting.\n\n- **Model Architecture**\n  - **LSTM Layers**:\n    - 64 units → 32 units: Captures short- and long-term dependencies.\n  - **Dropout**:\n    - 0.2 & 0.4: Reduces overfitting.\n  - **Dense Layers**:\n    - 16 units → 1 unit: Learns patterns and outputs voltage prediction.\n  - **Optimizer**:\n    - Adam with adaptive learning rate: Ensures stable and efficient training.\n\n- **Optimizations Used**\n  - **Early Stopping**: Stops training when validation loss stops improving.\n  - **ReduceLROnPlateau**: Lowers learning rate if training stagnates.","metadata":{"_uuid":"8c779d03-48c4-4253-8e4f-539b990f097e","_cell_guid":"ee71c071-3ea6-45bf-8504-03ee3def3d0e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Define the LSTM model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, sequence_length):\n        super(LSTMModel, self).__init__()\n        # self.lstm1 = nn.LSTM(input_size, 64, batch_first=True, return_sequences=True)\n        self.lstm1 = nn.LSTM(input_size, 64, batch_first=True)\n        self.dropout1 = nn.Dropout(0.2)\n        # self.lstm2 = nn.LSTM(64, 32, batch_first=True, return_sequences=False)\n        self.lstm2 = nn.LSTM(64, 32, batch_first=True)\n        self.dropout2 = nn.Dropout(0.4)\n        self.dense1 = nn.Linear(32, 16)\n        self.relu = nn.ReLU()\n        self.dense2 = nn.Linear(16, 1)\n        \n    def forward(self, x):\n        x, _ = self.lstm1(x)\n        x = self.dropout1(x)\n        x, _ = self.lstm2(x)\n        x = self.dropout2(x)\n        x = self.dense1(x)\n        x = self.relu(x)\n        x = self.dense2(x)\n        return x\n\n# Initialize model, loss, and optimizer\ninput_size = X_train_seq.shape[2]  # Assuming X_train_seq is defined\nsequence_length = X_train_seq.shape[1]\nmodel = LSTMModel(input_size, sequence_length)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Model summary\nprint(model)","metadata":{"_uuid":"f5f2e7fa-4148-47c5-b7e8-bb70a453dc29","_cell_guid":"9777b4fa-7835-4cbb-9a5b-4e7dbafc20fe","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:19:46.952095Z","iopub.execute_input":"2025-04-28T09:19:46.952383Z","iopub.status.idle":"2025-04-28T09:19:50.741578Z","shell.execute_reply.started":"2025-04-28T09:19:46.952364Z","shell.execute_reply":"2025-04-28T09:19:50.740681Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport numpy as np\n\n# Assuming model, X_train_seq, y_train_seq, X_test_seq, y_test_seq are defined\n# Convert data to PyTorch tensors\nX_train_seq = torch.tensor(X_train_seq, dtype=torch.float32)\ny_train_seq = torch.tensor(y_train_seq, dtype=torch.float32)\nX_test_seq = torch.tensor(X_test_seq, dtype=torch.float32)\ny_test_seq = torch.tensor(y_test_seq, dtype=torch.float32)\n\n# Create DataLoader for batch processing\nfrom torch.utils.data import TensorDataset, DataLoader\ntrain_dataset = TensorDataset(X_train_seq, y_train_seq)\ntest_dataset = TensorDataset(X_test_seq, y_test_seq)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Initialize optimizer\ninitial_learning_rate = 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=initial_learning_rate)\n\n# Loss function\ncriterion = nn.MSELoss()\n\n# Learning rate scheduler\nlr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6, verbose=True)","metadata":{"_uuid":"462b09c2-f9ff-442d-af02-eb401d2fdd19","_cell_guid":"b9ee31a1-edf0-4f49-b9c2-01986c3732c0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:19:57.59093Z","iopub.execute_input":"2025-04-28T09:19:57.591445Z","execution_failed":"2025-04-28T09:21:10.641Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Early stopping implementation","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=10, min_delta=0, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = float('inf')\n        self.best_model = None\n        self.counter = 0\n        \n    def __call__(self, val_loss, model):\n        if val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            if self.restore_best_weights:\n                self.best_model = model.state_dict()\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                print(f'Early stopping triggered after {self.counter} epochs')\n                if self.restore_best_weights:\n                    model.load_state_dict(self.best_model)\n                return True\n        return False\n\nearly_stopping = EarlyStopping(patience=10, restore_best_weights=True)","metadata":{"_uuid":"462b09c2-f9ff-442d-af02-eb401d2fdd19","_cell_guid":"b9ee31a1-edf0-4f49-b9c2-01986c3732c0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:19:57.59093Z","iopub.execute_input":"2025-04-28T09:19:57.591445Z","execution_failed":"2025-04-28T09:21:10.641Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Checkpointing","metadata":{}},{"cell_type":"code","source":"import os\n\ncheckpoint_dir = \"checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Initialize best validation loss for checkpointing\nbest_val_loss = float('inf')\nbest_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n\n# Check for existing checkpoint\nstart_epoch = 0\ncheckpoint_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    best_val_loss = checkpoint['best_val_loss']\n    lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    early_stopping.best_loss = checkpoint['early_stopping_best_loss']\n    early_stopping.best_model = checkpoint['early_stopping_best_model']\n    early_stopping.counter = checkpoint['early_stopping_counter']\n    print(f\"Resuming from epoch {start_epoch}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train the LSTM model","metadata":{}},{"cell_type":"code","source":"# Training loop\nnum_epochs = 50\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\nfor epoch in range(start_epoch, num_epochs):\n    model.train()\n    train_loss = 0.0\n    train_mae = 0.0\n    \n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * X_batch.size(0)\n        train_mae += torch.mean(torch.abs(outputs - y_batch)).item() * X_batch.size(0)\n    \n    train_loss /= len(train_loader.dataset)\n    train_mae /= len(train_loader.dataset)\n    \n    # Validation\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            outputs = model(X_batch)\n            val_loss += criterion(outputs, y_batch).item() * X_batch.size(0)\n    \n    val_loss /= len(test_loader.dataset)\n    \n    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Train MAE: {train_mae:.6f}, Val Loss: {val_loss:.6f}')\n    \n    # Save checkpoint every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': lr_scheduler.state_dict(),\n            'best_val_loss': best_val_loss,\n            'early_stopping_best_loss': early_stopping.best_loss,\n            'early_stopping_best_model': early_stopping.best_model,\n            'early_stopping_counter': early_stopping.counter\n        }\n        torch.save(checkpoint, os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n        print(f\"Saved checkpoint at epoch {epoch+1}\")\n    \n    # Save latest checkpoint\n    torch.save(checkpoint, checkpoint_path)\n    \n    # Save best model if validation loss improves\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"New best model saved with val_loss: {best_val_loss:.4f}\")\n    \n    # Update learning rate\n    lr_scheduler.step(val_loss)\n    \n    # Check early stopping\n    if early_stopping(val_loss, model):\n        break","metadata":{"_uuid":"462b09c2-f9ff-442d-af02-eb401d2fdd19","_cell_guid":"b9ee31a1-edf0-4f49-b9c2-01986c3732c0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-28T09:19:57.59093Z","iopub.execute_input":"2025-04-28T09:19:57.591445Z","execution_failed":"2025-04-28T09:21:10.641Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Best Model for evaluating ","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(os.path.join(checkpoint_dir, 'best_model.pth')))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate the model","metadata":{"_uuid":"bcffac8e-7187-4b92-b6ac-f2f404a5c56e","_cell_guid":"b53e22c5-fc75-4c4b-b74e-7cc56869dd68","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nmodel.eval()\ny_pred = []\nwith torch.no_grad():\n    for X_batch, _ in test_loader:\n        X_batch = X_batch.to(device)\n        outputs = model(X_batch)  # Shape: (batch_size, sequence_length, 1)\n        # Extract the prediction from the last timestep\n        outputs = outputs[:, -1, :]  # Shape: (batch_size, 1)\n        y_pred.append(outputs.cpu().numpy())\n\n# Concatenate predictions and flatten to 1D\ny_pred = np.concatenate(y_pred, axis=0).flatten()  # Shape: (n_samples,)\n\n# Ensure y_test_seq is 1D\ny_test_seq_np = y_test_seq.cpu().numpy().flatten()  # Shape: (n_samples,)\n\n# Plot actual vs predicted\nplt.figure(figsize=(10, 5))\nplt.plot(y_test_seq_np[:100], label=\"Actual Voltage\", marker=\"o\")\nplt.plot(y_pred[:100], label=\"Predicted Voltage\", marker=\"x\")\nplt.legend()\nplt.xlabel(\"Time Step\")\nplt.ylabel(\"Voltage\")\nplt.title(\"Actual vs Predicted Voltage\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.eval()\n# test_loss = 0.0\n# test_mae = 0.0\n\n# with torch.no_grad():\n#     for X_batch, y_batch in test_loader:\n#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n#         outputs = model(X_batch)\n#         test_loss += criterion(outputs, y_batch).item() * X_batch.size(0)\n#         test_mae += torch.mean(torch.abs(outputs - y_batch)).item() * X_batch.size(0)\n\n# test_loss /= len(test_loader.dataset)\n# test_mae /= len(test_loader.dataset)\n\n# print(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")","metadata":{"_uuid":"22ff13bd-24c6-4a4c-a590-3527035b33e9","_cell_guid":"9f5619d9-22f6-4205-a3c8-840bc112f134","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-27T17:24:31.09396Z","iopub.execute_input":"2025-04-27T17:24:31.09426Z","iopub.status.idle":"2025-04-27T17:24:31.111783Z","shell.execute_reply.started":"2025-04-27T17:24:31.09424Z","shell.execute_reply":"2025-04-27T17:24:31.110589Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predict on test data","metadata":{"_uuid":"e91e4e24-ce8c-43ab-b623-1e5dbd5c50e6","_cell_guid":"ee17c35d-e238-4db4-b0e8-a9569cf2751c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# model.eval()\n# y_pred = []\n# with torch.no_grad():\n#     for X_batch, _ in test_loader:\n#         X_batch = X_batch.to(device)\n#         outputs = model(X_batch)\n#         y_pred.append(outputs.cpu().numpy())\n# y_pred = np.concatenate(y_pred, axis=0)\n\n# # Plot actual vs predicted\n# plt.figure(figsize=(10, 5))\n# plt.plot(y_test_seq[:100], label=\"Actual Voltage\", marker=\"o\")\n# plt.plot(y_pred[:100], label=\"Predicted Voltage\", marker=\"x\")\n# plt.legend()\n# plt.xlabel(\"Time Step\")\n# plt.ylabel(\"Voltage\")\n# plt.title(\"Actual vs Predicted Voltage\")\n# plt.show()","metadata":{"_uuid":"90671078-e88e-4dfb-b7b1-63e08c15d4f1","_cell_guid":"bab718ac-7fe8-4d00-a712-50da069025ef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Calculate prediction errors","metadata":{"_uuid":"a5ce27af-b016-423c-821a-fecb5a7502da","_cell_guid":"30a2c538-28f5-4cb6-9804-89a8bff6fd65","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"errors = y_test_seq.cpu().numpy() - y_pred.flatten()","metadata":{"_uuid":"f2d0a535-fdfd-41ae-94a3-0feda16b0b7b","_cell_guid":"0cd85dfe-265b-46c9-876c-7cf1bc2bf178","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Plot histogram of errors","metadata":{"_uuid":"136deccb-0bc9-43b0-94f3-f7cc583b4384","_cell_guid":"2d26f4d1-a116-4aad-be54-853ecf08c5a2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"plt.hist(errors, bins=50, alpha=0.7, color='blue')\nplt.xlabel(\"Prediction Error\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Prediction Errors\")\nplt.show()","metadata":{"_uuid":"bcc4cb55-78e0-4ff4-90fa-3adb6b1fa5b7","_cell_guid":"0bc9f350-ffe9-4cee-8d6e-7edea9500309","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Convert tensors to numpy arrays and flatten\nactual = y_test_seq.cpu().numpy().flatten()\npredicted = y_pred.flatten()\n\n# Calculate metrics\nmae = mean_absolute_error(actual, predicted)\nmse = mean_squared_error(actual, predicted)\nrmse = np.sqrt(mse)\nr2 = r2_score(actual, predicted)\n\n# Print results\nprint(f\"MAE: {mae}\")\nprint(f\"MSE: {mse}\")\nprint(f\"RMSE: {rmse}\")\nprint(f\"R² Score: {r2}\")","metadata":{"_uuid":"e51f4de5-a0d6-4472-b530-bdec244a094d","_cell_guid":"2fac99a7-9d22-4b0d-9c14-c5a821345256","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select a random sample from the test set\nsample_index = np.random.randint(0, X_test_seq.shape[0])\nsample_input = X_test_seq[sample_index]\n\n# Reshape for LSTM input (batch_size=1)\nsample_input = torch.tensor(sample_input, dtype=torch.float32).unsqueeze(0).to(device)","metadata":{"_uuid":"237c0d0c-18d0-4937-bbcb-f8e666bf389b","_cell_guid":"5ecc9172-c636-4972-b3e1-aac44a62a578","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-27T17:18:54.574294Z","iopub.execute_input":"2025-04-27T17:18:54.574669Z","iopub.status.idle":"2025-04-27T17:18:54.663702Z","shell.execute_reply.started":"2025-04-27T17:18:54.574618Z","shell.execute_reply":"2025-04-27T17:18:54.661986Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict voltage for the sample\nmodel.eval()\nwith torch.no_grad():\n    predicted_voltage = model(sample_input).cpu().numpy()[0][0]\n\nactual_voltage = y_test_seq[sample_index].cpu().numpy()\n\nprint(f\"Predicted Voltage: {predicted_voltage:.4f} V\")\nprint(f\"Actual Voltage: {actual_voltage:.4f} V\")","metadata":{"_uuid":"bb6ad554-89d5-475a-b317-1b867254ccc2","_cell_guid":"acd4cd1a-b46a-461d-b776-e3316af0208a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))\nplt.bar([\"Actual Voltage\", \"Predicted Voltage\"], [actual_voltage, predicted_voltage], color=['blue', 'orange'])\nplt.ylabel(\"Voltage (V)\")\nplt.title(\"Actual vs. Predicted Voltage\")\nplt.show()","metadata":{"_uuid":"441203cf-96c2-4a49-81e7-ca52a1a7b6c0","_cell_guid":"0017e5ad-ceb0-48de-a0d6-59d5118e15ba","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-30T21:58:04.316848Z","iopub.execute_input":"2025-03-30T21:58:04.317164Z","iopub.status.idle":"2025-03-30T21:58:04.466765Z","shell.execute_reply.started":"2025-03-30T21:58:04.317138Z","shell.execute_reply":"2025-03-30T21:58:04.465778Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss (MSE)\")\nplt.title(\"Training vs Validation Loss Curve\")\nplt.legend()\nplt.show()","metadata":{"_uuid":"87cd2d1b-ecd8-4c95-9b9c-d79e651c9979","_cell_guid":"4da7a286-9f7d-4314-9815-980199e348b9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-30T21:59:10.561549Z","iopub.execute_input":"2025-03-30T21:59:10.561888Z","iopub.status.idle":"2025-03-30T21:59:10.785081Z","shell.execute_reply.started":"2025-03-30T21:59:10.561864Z","shell.execute_reply":"2025-03-30T21:59:10.784081Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get model predictions\ny_pred = model.predict(X_test_seq)\n\n# Scatter plot of actual vs predicted\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test_seq, y_pred, alpha=0.5, color='blue')\nplt.plot([min(y_test_seq), max(y_test_seq)], [min(y_test_seq), max(y_test_seq)], color='red', linestyle='--')  \nplt.xlabel(\"Actual Voltage\")\nplt.ylabel(\"Predicted Voltage\")\nplt.title(\"Actual vs. Predicted Voltage\")\nplt.show()","metadata":{"_uuid":"2d1e52fa-9b06-4b9e-848b-35f8fc65a4d6","_cell_guid":"f0eae07d-d605-4ede-9986-25425de0328d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-30T21:59:18.561963Z","iopub.execute_input":"2025-03-30T21:59:18.562306Z","iopub.status.idle":"2025-03-30T21:59:59.346165Z","shell.execute_reply.started":"2025-03-30T21:59:18.562275Z","shell.execute_reply":"2025-03-30T21:59:59.345184Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5. Evaluation ---\nmodel.eval() # Ensure model is in evaluation mode (important if dropout/batchnorm are used)\ntest_loss = 0.0\ntest_mae = 0.0\nall_preds = []\nall_targets = []\n\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        outputs = model(X_batch)\n\n        # Accumulate loss and MAE\n        test_loss += criterion(outputs, y_batch).item() * X_batch.size(0)\n        test_mae += torch.mean(torch.abs(outputs - y_batch)).item() * X_batch.size(0)\n\n        # Store predictions and targets for later analysis (e.g., R2 score)\n        all_preds.append(outputs.cpu().numpy())\n        all_targets.append(y_batch.cpu().numpy())\n\n# Calculate final metrics\nfinal_test_loss = test_loss / len(test_loader.dataset)\nfinal_test_mae = test_mae / len(test_loader.dataset)\n\n# Concatenate predictions and targets from all batches\ny_pred_np = np.concatenate(all_preds, axis=0)\ny_test_np = np.concatenate(all_targets, axis=0)\n\n# Calculate other metrics using sklearn\nfinal_test_rmse = np.sqrt(mean_squared_error(y_test_np, y_pred_np))\nfinal_r2_score = r2_score(y_test_np, y_pred_np)\n\nprint(f\"\\n--- Evaluation Results ---\")\nprint(f\"Test Loss (MSE): {final_test_loss:.6f}\")\nprint(f\"Test MAE: {final_test_mae:.6f}\")\nprint(f\"Test RMSE: {final_test_rmse:.6f}\")\nprint(f\"Test R² Score: {final_r2_score:.6f}\")\n\n\n# --- 6. Plotting and Prediction Example ---\n\n# Plot actual vs predicted for a subset of test data\nplt.figure(figsize=(12, 6))\nplt.plot(y_test_np[:200], label=\"Actual Voltage\", marker=\"o\", linestyle='-', markersize=4)\nplt.plot(y_pred_np[:200], label=\"Predicted Voltage\", marker=\"x\", linestyle='--', markersize=4)\nplt.legend()\nplt.xlabel(\"Time Step (in test set sample)\")\nplt.ylabel(\"Normalized Voltage\") # Assuming data was normalized\nplt.title(\"Actual vs Predicted Voltage (First 200 Test Samples)\")\nplt.grid(True)\nplt.show()\n\n# Plot training & validation loss curves\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss (MSE)')\nplt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss (MSE)')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss (MSE)\")\nplt.title(\"Training vs Validation Loss Curve\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot histogram of prediction errors\nerrors = y_test_np - y_pred_np\nplt.figure(figsize=(10, 5))\nplt.hist(errors, bins=50, alpha=0.7, color='blue', edgecolor='black')\nplt.xlabel(\"Prediction Error (Actual - Predicted)\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Prediction Errors\")\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n\n# Scatter plot of actual vs predicted\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test_np, y_pred_np, alpha=0.5, color='blue', label='Predictions')\n# Add identity line (y=x)\nmin_val = min(y_test_np.min(), y_pred_np.min())\nmax_val = max(y_test_np.max(), y_pred_np.max())\nplt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Ideal Fit (y=x)')\nplt.xlabel(\"Actual Voltage (Normalized)\")\nplt.ylabel(\"Predicted Voltage (Normalized)\")\nplt.title(\"Actual vs. Predicted Voltage Scatter Plot\")\nplt.legend()\nplt.grid(True)\nplt.axis('equal') # Ensure axes have the same scale for better interpretation\nplt.show()\n\n\n# --- Example: Predict on a single sample ---\n# Select a random sample from the test set\nsample_index = np.random.randint(0, X_test_tensor.shape[0])\nsample_input_tensor = X_test_tensor[sample_index] # Shape: (sequence_length, num_features)\nactual_voltage_sample = y_test_tensor[sample_index].item() # Get scalar value\n\n# Reshape for LSTM input (batch_size=1) -> (1, sequence_length, num_features)\nsample_input_tensor = sample_input_tensor.unsqueeze(0).to(device)\n\n# Predict voltage for the sample\nmodel.eval()\nwith torch.no_grad():\n    predicted_voltage_sample = model(sample_input_tensor).item() # Get scalar value\n\nprint(f\"\\n--- Single Sample Prediction ---\")\nprint(f\"Sample Index: {sample_index}\")\nprint(f\"Predicted Voltage: {predicted_voltage_sample:.4f}\")\nprint(f\"Actual Voltage:    {actual_voltage_sample:.4f}\")\n\n# Optional: Bar plot for single prediction\nplt.figure(figsize=(6, 4))\nplt.bar([\"Actual Voltage\", \"Predicted Voltage\"], [actual_voltage_sample, predicted_voltage_sample], color=['blue', 'orange'])\nplt.ylabel(\"Voltage (Normalized)\")\nplt.title(f\"Actual vs. Predicted Voltage (Sample {sample_index})\")\nplt.ylim(min_val, max_val) # Use limits from scatter plot for consistency\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torch.optim.lr_scheduler import ReduceLROnPlateau\n# import numpy as np\n# from torch.utils.data import TensorDataset, DataLoader\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# import matplotlib.pyplot as plt\n\n# # Assuming X_train_seq, y_train_seq, X_test_seq, y_test_seq are NumPy arrays\n# # defined earlier in your script (e.g., after create_sequences function)\n# # Example shapes (replace with your actual shapes):\n# # X_train_seq.shape: (num_train_samples, sequence_length, num_features)\n# # y_train_seq.shape: (num_train_samples,)\n# # X_test_seq.shape: (num_test_samples, sequence_length, num_features)\n# # y_test_seq.shape: (num_test_samples,)\n\n# # --- 1. LSTM Model Definition ---\n# class LSTMModel(nn.Module):\n#     def __init__(self, input_size, hidden_size1=64, hidden_size2=32, dense_size=16, dropout1=0.2, dropout2=0.4):\n#         super(LSTMModel, self).__init__()\n#         # First LSTM layer\n#         self.lstm1 = nn.LSTM(input_size, hidden_size1, batch_first=True) # Note: PyTorch LSTM doesn't have return_sequences, it's controlled by batch_first and output slicing\n#         self.dropout1 = nn.Dropout(dropout1)\n#         # Second LSTM layer\n#         self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, batch_first=True)\n#         self.dropout2 = nn.Dropout(dropout2)\n#         # Dense layers\n#         self.dense1 = nn.Linear(hidden_size2, dense_size)\n#         self.relu = nn.ReLU()\n#         self.dense2 = nn.Linear(dense_size, 1) # Output layer for voltage prediction\n\n#     def forward(self, x):\n#         # LSTM layers\n#         # lstm1 output shape: (batch, seq_len, hidden_size1)\n#         # hn/cn shape: (num_layers * num_directions, batch, hidden_size)\n#         lstm_out1, _ = self.lstm1(x)\n#         # Take the output of the last time step from the first LSTM layer\n#         # In PyTorch with batch_first=True, output is (batch, seq_len, feature), so we need the last sequence element\n#         # However, the second LSTM layer expects the full sequence output from the first\n#         x = self.dropout1(lstm_out1)\n\n#         lstm_out2, (hn, cn) = self.lstm2(x)\n#         # Use the hidden state of the last time step from the second LSTM layer\n#         # hn shape is (num_layers*num_directions, batch, hidden_size2), we want the last layer's hidden state\n#         # For a single-layer, non-bidirectional LSTM, hn[-1] gets the hidden state for the last time step implicitly because return_sequences=False equivalent behavior\n#         x = self.dropout2(hn[-1]) # hn[-1] takes the hidden state of the last layer for all items in the batch\n\n#         # Dense layers\n#         x = self.dense1(x)\n#         x = self.relu(x)\n#         x = self.dense2(x)\n#         # Ensure output is squeezed to match target shape (batch_size,) if y_batch is (batch_size,)\n#         # If criterion expects (batch_size, 1), remove the .squeeze()\n#         return x.squeeze(-1) # Squeeze the last dimension if target is 1D\n\n# # --- 2. Data Preparation ---\n# # Assuming sequence_length and feature dimensions are known\n# # Replace dummy data with your actual X_train_seq, y_train_seq, etc.\n# sequence_length = 30\n# num_features = 4 # Example: ['Current_measured', 'Voltage_load', 'Time', 'Battery_ID']\n# # Dummy Data (Replace with your actual loaded and preprocessed data)\n# X_train_seq = np.random.rand(1000, sequence_length, num_features)\n# y_train_seq = np.random.rand(1000)\n# X_test_seq = np.random.rand(200, sequence_length, num_features)\n# y_test_seq = np.random.rand(200)\n\n\n# # Convert data to PyTorch tensors\n# X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n# y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32)\n# X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n# y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32)\n\n# # Create DataLoader for batch processing\n# batch_size = 64\n# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n# test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# # --- 3. Model Initialization and Training Setup ---\n# input_size = X_train_tensor.shape[2] # Number of features\n# model = LSTMModel(input_size=input_size)\n\n# # Loss function\n# criterion = nn.MSELoss()\n\n# # Optimizer\n# initial_learning_rate = 0.001\n# optimizer = torch.optim.Adam(model.parameters(), lr=initial_learning_rate)\n\n# # Learning rate scheduler\n# lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6, verbose=True)\n\n# # Early stopping implementation\n# class EarlyStopping:\n#     def __init__(self, patience=10, min_delta=0, restore_best_weights=True):\n#         self.patience = patience\n#         self.min_delta = min_delta\n#         self.restore_best_weights = restore_best_weights\n#         self.best_loss = float('inf')\n#         self.best_model_state_dict = None # Store the state_dict\n#         self.counter = 0\n\n#     def __call__(self, val_loss, model):\n#         if val_loss < self.best_loss - self.min_delta:\n#             self.best_loss = val_loss\n#             self.counter = 0\n#             if self.restore_best_weights:\n#                 # Save the model's state_dict instead of the model itself\n#                 self.best_model_state_dict = model.state_dict()\n#         else:\n#             self.counter += 1\n#             print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n#             if self.counter >= self.patience:\n#                 print(f'Early stopping triggered after {self.patience} epochs of no improvement.')\n#                 if self.restore_best_weights and self.best_model_state_dict:\n#                     print(\"Restoring best model weights.\")\n#                     model.load_state_dict(self.best_model_state_dict)\n#                 return True\n#         return False\n\n# early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n\n# # --- 4. Training Loop ---\n# num_epochs = 50\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# print(f\"Using device: {device}\")\n# model = model.to(device)\n\n# train_losses = []\n# val_losses = []\n# train_maes_list = [] # To store MAE per epoch\n\n# for epoch in range(num_epochs):\n#     model.train() # Set model to training mode\n#     epoch_train_loss = 0.0\n#     epoch_train_mae = 0.0\n\n#     for i, (X_batch, y_batch) in enumerate(train_loader):\n#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n#         # Zero gradients\n#         optimizer.zero_grad()\n\n#         # Forward pass\n#         outputs = model(X_batch)\n\n#         # Calculate loss\n#         loss = criterion(outputs, y_batch)\n\n#         # Backward pass and optimization\n#         loss.backward()\n#         optimizer.step()\n\n#         # Accumulate loss and MAE for the epoch\n#         epoch_train_loss += loss.item() * X_batch.size(0)\n#         # Calculate MAE for the batch\n#         mae_batch = torch.mean(torch.abs(outputs.detach() - y_batch)).item() # Use detach() for metrics\n#         epoch_train_mae += mae_batch * X_batch.size(0)\n\n\n#     # Calculate average loss and MAE for the epoch\n#     avg_train_loss = epoch_train_loss / len(train_loader.dataset)\n#     avg_train_mae = epoch_train_mae / len(train_loader.dataset)\n#     train_losses.append(avg_train_loss)\n#     train_maes_list.append(avg_train_mae)\n\n#     # --- Validation ---\n#     model.eval() # Set model to evaluation mode\n#     epoch_val_loss = 0.0\n#     with torch.no_grad(): # Disable gradient calculation for validation\n#         for X_batch, y_batch in test_loader:\n#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n#             outputs = model(X_batch)\n#             loss = criterion(outputs, y_batch)\n#             epoch_val_loss += loss.item() * X_batch.size(0)\n\n#     avg_val_loss = epoch_val_loss / len(test_loader.dataset)\n#     val_losses.append(avg_val_loss)\n\n#     print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.6f}, Train MAE: {avg_train_mae:.6f}, Val Loss: {avg_val_loss:.6f}')\n\n#     # Update learning rate based on validation loss\n#     lr_scheduler.step(avg_val_loss)\n\n#     # Check for early stopping based on validation loss\n#     if early_stopping(avg_val_loss, model):\n#         break # Stop training\n\n# print(\"Training finished.\")\n\n# # --- 5. Evaluation ---\n# model.eval() # Ensure model is in evaluation mode (important if dropout/batchnorm are used)\n# test_loss = 0.0\n# test_mae = 0.0\n# all_preds = []\n# all_targets = []\n\n# with torch.no_grad():\n#     for X_batch, y_batch in test_loader:\n#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n#         outputs = model(X_batch)\n\n#         # Accumulate loss and MAE\n#         test_loss += criterion(outputs, y_batch).item() * X_batch.size(0)\n#         test_mae += torch.mean(torch.abs(outputs - y_batch)).item() * X_batch.size(0)\n\n#         # Store predictions and targets for later analysis (e.g., R2 score)\n#         all_preds.append(outputs.cpu().numpy())\n#         all_targets.append(y_batch.cpu().numpy())\n\n# # Calculate final metrics\n# final_test_loss = test_loss / len(test_loader.dataset)\n# final_test_mae = test_mae / len(test_loader.dataset)\n\n# # Concatenate predictions and targets from all batches\n# y_pred_np = np.concatenate(all_preds, axis=0)\n# y_test_np = np.concatenate(all_targets, axis=0)\n\n# # Calculate other metrics using sklearn\n# final_test_rmse = np.sqrt(mean_squared_error(y_test_np, y_pred_np))\n# final_r2_score = r2_score(y_test_np, y_pred_np)\n\n# print(f\"\\n--- Evaluation Results ---\")\n# print(f\"Test Loss (MSE): {final_test_loss:.6f}\")\n# print(f\"Test MAE: {final_test_mae:.6f}\")\n# print(f\"Test RMSE: {final_test_rmse:.6f}\")\n# print(f\"Test R² Score: {final_r2_score:.6f}\")\n\n\n# # --- 6. Plotting and Prediction Example ---\n\n# # Plot actual vs predicted for a subset of test data\n# plt.figure(figsize=(12, 6))\n# plt.plot(y_test_np[:200], label=\"Actual Voltage\", marker=\"o\", linestyle='-', markersize=4)\n# plt.plot(y_pred_np[:200], label=\"Predicted Voltage\", marker=\"x\", linestyle='--', markersize=4)\n# plt.legend()\n# plt.xlabel(\"Time Step (in test set sample)\")\n# plt.ylabel(\"Normalized Voltage\") # Assuming data was normalized\n# plt.title(\"Actual vs Predicted Voltage (First 200 Test Samples)\")\n# plt.grid(True)\n# plt.show()\n\n# # Plot training & validation loss curves\n# plt.figure(figsize=(10, 5))\n# plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss (MSE)')\n# plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss (MSE)')\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"Loss (MSE)\")\n# plt.title(\"Training vs Validation Loss Curve\")\n# plt.legend()\n# plt.grid(True)\n# plt.show()\n\n# # Plot histogram of prediction errors\n# errors = y_test_np - y_pred_np\n# plt.figure(figsize=(10, 5))\n# plt.hist(errors, bins=50, alpha=0.7, color='blue', edgecolor='black')\n# plt.xlabel(\"Prediction Error (Actual - Predicted)\")\n# plt.ylabel(\"Frequency\")\n# plt.title(\"Distribution of Prediction Errors\")\n# plt.grid(axis='y', alpha=0.75)\n# plt.show()\n\n# # Scatter plot of actual vs predicted\n# plt.figure(figsize=(6, 6))\n# plt.scatter(y_test_np, y_pred_np, alpha=0.5, color='blue', label='Predictions')\n# # Add identity line (y=x)\n# min_val = min(y_test_np.min(), y_pred_np.min())\n# max_val = max(y_test_np.max(), y_pred_np.max())\n# plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Ideal Fit (y=x)')\n# plt.xlabel(\"Actual Voltage (Normalized)\")\n# plt.ylabel(\"Predicted Voltage (Normalized)\")\n# plt.title(\"Actual vs. Predicted Voltage Scatter Plot\")\n# plt.legend()\n# plt.grid(True)\n# plt.axis('equal') # Ensure axes have the same scale for better interpretation\n# plt.show()\n\n\n# # --- Example: Predict on a single sample ---\n# # Select a random sample from the test set\n# sample_index = np.random.randint(0, X_test_tensor.shape[0])\n# sample_input_tensor = X_test_tensor[sample_index] # Shape: (sequence_length, num_features)\n# actual_voltage_sample = y_test_tensor[sample_index].item() # Get scalar value\n\n# # Reshape for LSTM input (batch_size=1) -> (1, sequence_length, num_features)\n# sample_input_tensor = sample_input_tensor.unsqueeze(0).to(device)\n\n# # Predict voltage for the sample\n# model.eval()\n# with torch.no_grad():\n#     predicted_voltage_sample = model(sample_input_tensor).item() # Get scalar value\n\n# print(f\"\\n--- Single Sample Prediction ---\")\n# print(f\"Sample Index: {sample_index}\")\n# print(f\"Predicted Voltage: {predicted_voltage_sample:.4f}\")\n# print(f\"Actual Voltage:    {actual_voltage_sample:.4f}\")\n\n# # Optional: Bar plot for single prediction\n# plt.figure(figsize=(6, 4))\n# plt.bar([\"Actual Voltage\", \"Predicted Voltage\"], [actual_voltage_sample, predicted_voltage_sample], color=['blue', 'orange'])\n# plt.ylabel(\"Voltage (Normalized)\")\n# plt.title(f\"Actual vs. Predicted Voltage (Sample {sample_index})\")\n# plt.ylim(min_val, max_val) # Use limits from scatter plot for consistency\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion & Scope for Improvement\n\n- **Conclusion**:\n  - Successfully implemented an LSTM-based deep learning model for battery voltage prediction using the NASA Battery Dataset.\n  - Effective data preprocessing and exploratory data analysis (EDA) conducted.\n  - Model tuned with adaptive learning rates, dropout layers for regularization, and early stopping to prevent overfitting.\n  - Evaluation metrics (MAE, MSE, RMSE, R² score) indicate effective capture of battery behavior.\n  - Promising approach for battery health monitoring and predictive maintenance.\n\n- **Scope for Improvement**:\n  - **Hyperparameter Optimization**:\n    - Experiment with Bidirectional LSTMs or GRUs for improved learning efficiency.\n    - Fine-tune hyperparameters (batch size, dropout rate, sequence length) using Bayesian Optimization or Grid Search.\n  - **Feature Engineering**:\n    - Introduce derived features (e.g., voltage change rate, cumulative charge/discharge cycles) to enhance predictions.\n  - **Handling Imbalanced Data**:\n    - Address datasets with uneven charge/discharge cycles using resampling techniques for better generalization.\n  - **External Data Integration**:\n    - Incorporate environmental factors (humidity, pressure, real-world usage scenarios) for robust predictions.\n  - **Model Deployment & Real-Time Predictions**:\n    - Convert model into a real-time monitoring system using Flask or FastAPI.\n    - Deploy on Kaggle, Hugging Face, or cloud platforms for live battery voltage inference.\n  - **Outcome**:\n    - Improvements can make the model more generalizable, efficient, and deployable for industrial battery monitoring systems.","metadata":{"_uuid":"3c7004e6-4847-45b4-8d5c-c2c2c8f0665f","_cell_guid":"e964b1c6-94da-428f-a506-17513bc16a58","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nimport uuid\n\n# Download NLTK tokenizer data\nnltk.download('punkt')\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparameters\nMAX_VOCAB_SIZE = 25000\nMAX_SEQUENCE_LENGTH = 500\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nDROPOUT = 0.5\nBATCH_SIZE = 64\nN_EPOCHS = 5\n\n# Load IMDB dataset using datasets\ndataset = load_dataset(\"imdb\")\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]\n\n# Build vocabulary\ndef build_vocab(texts, max_size):\n    counter = Counter()\n    for text in texts:\n        tokens = word_tokenize(text.lower())\n        counter.update(tokens)\n    \n    # Select most common tokens\n    most_common = counter.most_common(max_size - 2)  # Reserve 2 for <unk> and <pad>\n    vocab = {\"<unk>\": 0, \"<pad>\": 1}\n    for i, (word, _) in enumerate(most_common, 2):\n        vocab[word] = i\n    \n    return vocab\n\n# Create vocabulary from training data\nvocab = build_vocab([item[\"text\"] for item in train_data], MAX_VOCAB_SIZE)\n\n# Text and label processing\ndef text_pipeline(text, vocab):\n    tokens = word_tokenize(text.lower())\n    indices = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens][:MAX_SEQUENCE_LENGTH]\n    if len(indices) < MAX_SEQUENCE_LENGTH:\n        indices += [vocab[\"<pad>\"]] * (MAX_SEQUENCE_LENGTH - len(indices))\n    return indices\n\ndef label_pipeline(label):\n    return float(label)\n\n# Custom Dataset class\nclass IMDBDataset(Dataset):\n    def __init__(self, data, vocab):\n        self.data = data\n        self.vocab = vocab\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        text = self.data[idx][\"text\"]\n        label = self.data[idx][\"label\"]\n        return text_pipeline(text, self.vocab), label_pipeline(label)\n\n# Create data loaders\ntrain_dataset = IMDBDataset(train_data, vocab)\ntest_dataset = IMDBDataset(test_data, vocab)\n\ndef collate_batch(batch):\n    text_list, label_list = zip(*batch)\n    text_tensor = torch.tensor(text_list, dtype=torch.long).to(device)\n    label_tensor = torch.tensor(label_list, dtype=torch.float32).to(device)\n    return text_tensor, label_tensor\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n\n# Define BiLSTM model\nclass BiLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab[\"<pad>\"])\n        self.lstm = nn.LSTM(\n            embedding_dim,\n            hidden_dim,\n            num_layers=n_layers,\n            bidirectional=True,\n            dropout=dropout if n_layers > 1 else 0,\n            batch_first=True\n        )\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, text):\n        embedded = self.dropout(self.embedding(text))\n        output, (hidden, cell) = self.lstm(embedded)\n        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n        dense = self.fc(hidden)\n        return self.sigmoid(dense)\n\n# Initialize model\nmodel = BiLSTM(\n    vocab_size=len(vocab),\n    embedding_dim=EMBEDDING_DIM,\n    hidden_dim=HIDDEN_DIM,\n    output_dim=OUTPUT_DIM,\n    n_layers=N_LAYERS,\n    dropout=DROPOUT\n).to(device)\n\n# Loss and optimizer\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters())\n\n# Training function\ndef train(model, iterator, optimizer, criterion):\n    model.train()\n    epoch_loss = 0\n    epoch_acc = 0\n    total = 0\n    \n    for text, labels in iterator:\n        optimizer.zero_grad()\n        predictions = model(text).squeeze(1)\n        loss = criterion(predictions, labels)\n        acc = ((predictions > 0.5).float() == labels).float().mean()\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item() * len(labels)\n        epoch_acc += acc.item() * len(labels)\n        total += len(labels)\n    \n    return epoch_loss / total, epoch_acc / total\n\n# Evaluation function\ndef evaluate(model, iterator, criterion):\n    model.eval()\n    epoch_loss = 0\n    epoch_acc = 0\n    total = 0\n    \n    with torch.no_grad():\n        for text, labels in iterator:\n            predictions = model(text).squeeze(1)\n            loss = criterion(predictions, labels)\n            acc = ((predictions > 0.5).float() == labels).float().mean()\n            \n            epoch_loss += loss.item() * len(labels)\n            epoch_acc += acc.item() * len(labels)\n            total += len(labels)\n    \n    return epoch_loss / total, epoch_acc / total\n\n# Training loop\ntrain_losses, train_accs = [], []\nval_losses, val_accs = [], []\n\nfor epoch in range(N_EPOCHS):\n    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n    val_loss, val_acc = evaluate(model, test_loader, criterion)\n    \n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n    \n    print(f'Epoch: {epoch+1:02}')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\tVal Loss: {val_loss:.3f} | Val Acc: {val_acc*100:.2f}%')\n\n# Plotting\nplt.figure(figsize=(10, 5))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(range(1, N_EPOCHS+1), train_losses, label='Train Loss')\nplt.plot(range(1, N_EPOCHS+1), val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.savefig('loss_plot.png')\n\n# Accuracy plot\nplt.subplot(1, 2, 2)\nplt.plot(range(1, N_EPOCHS+1), train_accs, label='Train Accuracy')\nplt.plot(range(1, N_EPOCHS+1), val_accs, label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.savefig('accuracy_plot.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:49:12.828611Z","iopub.execute_input":"2025-04-20T12:49:12.828971Z","execution_failed":"2025-04-20T12:57:42.823Z"}},"outputs":[],"execution_count":null}]}
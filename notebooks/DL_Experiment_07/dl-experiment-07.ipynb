{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\nimport nltk\nfrom collections import Counter\nimport uuid\n\n# Download NLTK data\nnltk.download('punkt')\n\n# Set device to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load IMDB dataset using datasets library\ndataset = load_dataset(\"imdb\")\ntrain_data = dataset['train']\ntest_data = dataset['test']\n\n# Custom vocabulary builder\ndef build_vocab(texts, max_tokens=20000):\n    all_tokens = []\n    for item in texts:\n        tokens = word_tokenize(item['text'].lower())\n        all_tokens.extend(tokens)\n    \n    # Count token frequencies\n    token_counts = Counter(all_tokens)\n    most_common = token_counts.most_common(max_tokens - 2)  # Reserve 2 for <unk> and <pad>\n    vocab = {token: idx + 2 for idx, (token, _) in enumerate(most_common)}\n    vocab['<unk>'] = 0\n    vocab['<pad>'] = 1\n    reverse_vocab = {idx: token for token, idx in vocab.items()}\n    return vocab, reverse_vocab\n\nvocab, reverse_vocab = build_vocab(train_data)\npad_idx = vocab['<pad>']\n\n# Custom Dataset\nclass IMDBDataset(Dataset):\n    def __init__(self, data, vocab, max_length=500):\n        self.data = data\n        self.vocab = vocab\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data[idx]['text']\n        label = self.data[idx]['label']\n        tokens = word_tokenize(text.lower())[:self.max_length]\n        token_ids = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n        \n        if len(token_ids) < self.max_length:\n            token_ids += [self.vocab['<pad>']] * (self.max_length - len(token_ids))\n        else:\n            token_ids = token_ids[:self.max_length]\n        \n        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n\n# DataLoaders\ntrain_dataset = IMDBDataset(train_data, vocab)\ntest_dataset = IMDBDataset(test_data, vocab)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64)\n\n# RNN Model\nclass RNNModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5, pad_idx=0):\n        super(RNNModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n        self.fc = nn.Linear(hidden_size, 1)\n        self.dropout = nn.Dropout(dropout)\n        self.init_weights()\n\n    def init_weights(self):\n        nn.init.xavier_uniform_(self.embedding.weight)\n        nn.init.xavier_uniform_(self.fc.weight)\n        nn.init.zeros_(self.fc.bias)\n\n    def forward(self, text):\n        embedded = self.dropout(self.embedding(text))\n        output, (hidden, cell) = self.rnn(embedded)\n        hidden = self.dropout(hidden[-1])\n        return self.fc(hidden).squeeze(1)\n\n# Model parameters\nvocab_size = len(vocab)\nembed_size = 100\nhidden_size = 256\nnum_layers = 2\ndropout = 0.5\n\nmodel = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout, pad_idx).to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training and evaluation functions\ndef train(model, iterator, criterion, optimizer):\n    model.train()\n    epoch_loss = 0\n    epoch_acc = 0\n    for texts, labels in iterator:\n        texts, labels = texts.to(device), labels.to(device)\n        optimizer.zero_grad()\n        predictions = model(texts)\n        loss = criterion(predictions, labels)\n        acc = ((predictions > 0).float() == labels).float().mean()\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef evaluate(model, iterator, criterion):\n    model.eval()\n    epoch_loss = 0\n    epoch_acc = 0\n    with torch.no_grad():\n        for texts, labels in iterator:\n            texts, labels = texts.to(device), labels.to(device)\n            predictions = model(texts)\n            loss = criterion(predictions, labels)\n            acc = ((predictions > 0).float() == labels).float().mean()\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\n# Training loop with tracking for plots\nn_epochs = 25\ntrain_losses = []\ntrain_accuracies = []\ntest_losses = []\ntest_accuracies = []\n\nfor epoch in range(n_epochs):\n    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n    test_loss, test_acc = evaluate(model, test_loader, criterion)\n    \n    train_losses.append(train_loss)\n    train_accuracies.append(train_acc)\n    test_losses.append(test_loss)\n    test_accuracies.append(test_acc)\n    \n    print(f'Epoch: {epoch+1}')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n\n# Save model\ntorch.save(model.state_dict(), 'imdb_rnn_model.pth')\n\n# Plotting\nepochs = range(1, n_epochs + 1)\n\n# Plot Loss\nplt.figure(figsize=(10, 5))\nplt.plot(epochs, train_losses, label='Train Loss', marker='o')\nplt.plot(epochs, test_losses, label='Test Loss', marker='o')\nplt.title('Training and Test Loss over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.savefig('loss_plot.png')\nplt.show()\n\n# Plot Accuracy\nplt.figure(figsize=(10, 5))\nplt.plot(epochs, train_accuracies, label='Train Accuracy', marker='o')\nplt.plot(epochs, test_accuracies, label='Test Accuracy', marker='o')\nplt.title('Training and Test Accuracy over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.savefig('accuracy_plot.png')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:57:28.444729Z","iopub.execute_input":"2025-04-20T11:57:28.445353Z","execution_failed":"2025-04-20T12:05:37.633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
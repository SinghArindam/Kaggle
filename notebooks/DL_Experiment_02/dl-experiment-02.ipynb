{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# Parameters\nmax_words = 10000  # Vocabulary size\nmax_len = 200      # Max sequence length\nembedding_dim = 100\nbatch_size = 64\nepochs = 25\n\n# Load IMDB dataset\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n\n# Pad sequences to fixed length\nx_train = pad_sequences(x_train, maxlen=max_len)\nx_test = pad_sequences(x_test, maxlen=max_len)\n\n# Convert to PyTorch tensors\nx_train = torch.tensor(x_train, dtype=torch.long)\ny_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nx_test = torch.tensor(x_test, dtype=torch.long)\ny_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\n# Create DataLoader\ntrain_dataset = TensorDataset(x_train, y_train)\ntest_dataset = TensorDataset(x_test, y_test)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define 1D-CNN model\nclass CNN1D(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, max_len):\n        super(CNN1D, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=5)\n        self.pool1 = nn.MaxPool1d(kernel_size=2)\n        self.conv2 = nn.Conv1d(128, 64, kernel_size=5)\n        self.pool2 = nn.MaxPool1d(kernel_size=2)\n        # Calculate the size after convolutions and pooling\n        conv_output_size = self._get_conv_output_size(max_len)\n        self.fc1 = nn.Linear(conv_output_size, 128)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(128, 1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def _get_conv_output_size(self, input_length):\n        size = input_length\n        size = (size - 5 + 1) // 2  # Conv1 + Pool1\n        size = (size - 5 + 1) // 2  # Conv2 + Pool2\n        return size * 64  # 64 filters from conv2\n\n    def forward(self, x):\n        x = self.embedding(x).permute(0, 2, 1)  # (batch, embedding_dim, seq_len)\n        x = self.relu(self.conv1(x))\n        x = self.pool1(x)\n        x = self.relu(self.conv2(x))\n        x = self.pool2(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.sigmoid(self.fc2(x))\n        return x\n\n# Initialize model, loss, and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CNN1D(max_words, embedding_dim, max_len).to(device)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters())\n\n# Training loop\nmodel.train()\nfor epoch in range(epochs):\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        predicted = (outputs >= 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    accuracy = 100 * correct / total\n    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%\")\n\n# Evaluation\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        predicted = (outputs >= 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_accuracy = 100 * correct / total\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:14:38.846345Z","iopub.execute_input":"2025-04-20T10:14:38.846615Z","iopub.status.idle":"2025-04-20T10:15:19.876865Z","shell.execute_reply.started":"2025-04-20T10:14:38.846599Z","shell.execute_reply":"2025-04-20T10:15:19.875997Z"}},"outputs":[],"execution_count":null}]}
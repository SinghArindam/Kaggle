{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3649,"databundleVersionId":46718,"sourceType":"competition"}],"dockerImageVersionId":30461,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip3 install torch torchvision\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# Data Augumentation. \n\ntransform_train = transforms.Compose([\n    transforms.RandomResizedCrop(224),  # crop image at random and resizes it to 224.\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n\ntransform_test = transforms.Compose([\n    transforms.RandomResizedCrop(224), \n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n                                        download=True, \n                                        transform=transform_train)\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, \n                                        download=True, \n                                        transform=transform_test)\n\nnum_classes = 10\nbatch_size = 4\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\nprint(images.shape)\n\nprint(images[1].shape)\nprint(labels[1].item())\n\ndef imshow(img, title):\n    npimg = img.numpy() / 2 + 0.5 # converting the image to to numpy and un-normalise it.\n    plt.figure(figsize=(batch_size, 1))\n    plt.axis('off')\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.title(title)\n    plt.show()\n\ndef show_batch_images(dataloader):\n    images, labels = next(iter(dataloader))\n    img = torchvision.utils.make_grid(images)\n    imshow(img, title=[str(x.item()) for x in labels])\n\nfor i in range(4):\n    show_batch_images(trainloader)\n\nfrom torchvision import models\n\nvgg = models.vgg16_bn() #bn stands for batch normalization.\n\nprint(vgg)\n\nprint(vgg.features[0])\n\nprint(vgg.classifier[6])\n\n# last layer has 1000 classes, but for CIFAR10 has only 10 label outputs, changing the same with below code. \n\nfinal_in_features = vgg.classifier[6].in_features\nmod_classifier = list(vgg.classifier.children())[:-1] # keeping all the layers expect the last one\nmod_classifier.extend([nn.Linear(final_in_features, num_classes)]) # extending the last layer with the requiered number of class. \nprint(mod_classifier)\n\nvgg.classifier = nn.Sequential(*mod_classifier)\nprint(vgg)\n\nbatch_size = 16\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n\ndef evaluation(dataloader, model):\n    total, correct = 0, 0\n    for data in dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, pred = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (pred == labels).sum().item()\n    return 100 * correct / total\n\nvgg = vgg.to(device)\nloss_fn = nn.CrossEntropyLoss()\nopt = optim.SGD(vgg.parameters(), lr=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:28:27.136371Z","iopub.execute_input":"2023-05-01T15:28:27.137329Z","iopub.status.idle":"2023-05-01T15:28:27.145944Z","shell.execute_reply.started":"2023-05-01T15:28:27.137283Z","shell.execute_reply":"2023-05-01T15:28:27.144967Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time \n\nloss_epoch_arr = []\nmax_epochs = 1\nclip_value = 5\n\nn_iters = np.ceil(50000/batch_size)\n\nfor epoch in range(max_epochs):\n\n    for i, data in enumerate(trainloader, 0):\n\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        opt.zero_grad()\n\n        outputs = vgg(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(vgg.parameters(), clip_value)\n        opt.step()\n        \n        # below steps ensures that the memory usage in the GPU is optimised. \n        del inputs, labels, outputs\n        torch.cuda.empty_cache()\n        \n        if i % 100 == 0:\n            print('Iteration: %d/%d, Loss: %0.2f' % (i, n_iters, loss.item()))\n        \n    loss_epoch_arr.append(loss.item())\n        \n    print('Epoch: %d/%d, Test acc: %0.2f, Train acc: %0.2f' % (\n        epoch, max_epochs, \n        evaluation(testloader, vgg), evaluation(trainloader, vgg)))\n    \n# change max_epochs value to > 1 \n#plt.plot(loss_epoch_arr) \n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:28:40.088721Z","iopub.execute_input":"2023-05-01T15:28:40.089091Z","iopub.status.idle":"2023-05-01T15:44:07.226679Z","shell.execute_reply.started":"2023-05-01T15:28:40.089052Z","shell.execute_reply":"2023-05-01T15:44:07.225493Z"},"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 16\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:44:07.228263Z","iopub.execute_input":"2023-05-01T15:44:07.228832Z","iopub.status.idle":"2023-05-01T15:44:07.235985Z","shell.execute_reply.started":"2023-05-01T15:44:07.22879Z","shell.execute_reply":"2023-05-01T15:44:07.233848Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# downloading the pre-trained model. Please note VGGnet is trained for ImageNet dataset that had 1000 labels.\n\nvgg = models.vgg16_bn(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:44:07.237571Z","iopub.execute_input":"2023-05-01T15:44:07.238316Z","iopub.status.idle":"2023-05-01T15:44:17.105707Z","shell.execute_reply.started":"2023-05-01T15:44:07.238279Z","shell.execute_reply":"2023-05-01T15:44:17.104652Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# preventing the weights updates via back propegation.\n\nfor param in vgg.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:44:17.107515Z","iopub.execute_input":"2023-05-01T15:44:17.107947Z","iopub.status.idle":"2023-05-01T15:44:17.115303Z","shell.execute_reply.started":"2023-05-01T15:44:17.107888Z","shell.execute_reply":"2023-05-01T15:44:17.11299Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adding the final layes as a linear layer with num of classes.\n# only this layer will have requires_grad as true, only weights of this layer get updated.\n\nfinal_in_features = vgg.classifier[6].in_features\nvgg.classifier[6] = nn.Linear(final_in_features, num_classes)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:44:17.117972Z","iopub.execute_input":"2023-05-01T15:44:17.11832Z","iopub.status.idle":"2023-05-01T15:44:17.194517Z","shell.execute_reply.started":"2023-05-01T15:44:17.118284Z","shell.execute_reply":"2023-05-01T15:44:17.193429Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for param in vgg.parameters():\n    if param.requires_grad:\n        print(param.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:44:17.196017Z","iopub.execute_input":"2023-05-01T15:44:17.196472Z","iopub.status.idle":"2023-05-01T15:44:17.206582Z","shell.execute_reply.started":"2023-05-01T15:44:17.196433Z","shell.execute_reply":"2023-05-01T15:44:17.205398Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vgg = vgg.to(device)\nloss_fn = nn.CrossEntropyLoss()\nopt = optim.SGD(vgg.parameters(), lr=0.05)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:44:17.207683Z","iopub.execute_input":"2023-05-01T15:44:17.207997Z","iopub.status.idle":"2023-05-01T15:44:17.362313Z","shell.execute_reply.started":"2023-05-01T15:44:17.20797Z","shell.execute_reply":"2023-05-01T15:44:17.361123Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time \n\nloss_epoch_arr = []\nmax_epochs = 1\n\nn_iters = np.ceil(50000/batch_size)\n\nfor epoch in range(max_epochs):\n\n    for i, data in enumerate(trainloader, 0):\n\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        opt.zero_grad()\n\n        outputs = vgg(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        opt.step()\n        \n        if i % 100 == 0:\n            print('Iteration: %d/%d, Loss: %0.2f' % (i, n_iters, loss.item()))\n            \n        del inputs, labels, outputs\n        torch.cuda.empty_cache()\n        \n    loss_epoch_arr.append(loss.item())\n        \n    print('Epoch: %d/%d, Test acc: %0.2f, Train acc: %0.2f' % (\n        epoch, max_epochs, \n        evaluation(testloader, vgg), evaluation(trainloader, vgg)))\n\n# change max_epochs value to > 1 \n#plt.plot(loss_epoch_arr) \n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:44:17.369186Z","iopub.execute_input":"2023-05-01T15:44:17.369503Z","iopub.status.idle":"2023-05-01T15:53:07.258784Z","shell.execute_reply.started":"2023-05-01T15:44:17.369474Z","shell.execute_reply":"2023-05-01T15:53:07.257554Z"},"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# With model copies (Checkpoint)","metadata":{}},{"cell_type":"code","source":"import copy","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:53:07.260703Z","iopub.execute_input":"2023-05-01T15:53:07.261534Z","iopub.status.idle":"2023-05-01T15:53:07.266473Z","shell.execute_reply.started":"2023-05-01T15:53:07.26149Z","shell.execute_reply":"2023-05-01T15:53:07.265412Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_epoch_arr = []\nmax_epochs = 1\n\nmin_loss = 1000\n\nn_iters = np.ceil(50000/batch_size)\n\nfor epoch in range(max_epochs):\n\n    for i, data in enumerate(trainloader, 0):\n\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        opt.zero_grad()\n\n        outputs = vgg(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        opt.step()\n        \n        if min_loss > loss.item():\n            min_loss = loss.item()\n            best_model = copy.deepcopy(vgg.state_dict())\n            print('Min loss %0.2f' % min_loss)\n        \n        if i % 100 == 0:\n            print('Iteration: %d/%d, Loss: %0.2f' % (i, n_iters, loss.item()))\n            \n        del inputs, labels, outputs\n        torch.cuda.empty_cache()\n        \n    loss_epoch_arr.append(loss.item())\n\n# change max_epochs value to > 1 \n#plt.plot(loss_epoch_arr) \n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:53:07.268182Z","iopub.execute_input":"2023-05-01T15:53:07.268925Z","iopub.status.idle":"2023-05-01T15:57:41.447882Z","shell.execute_reply.started":"2023-05-01T15:53:07.268872Z","shell.execute_reply":"2023-05-01T15:57:41.446846Z"},"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vgg.load_state_dict(best_model)\nprint(evaluation(trainloader, vgg), evaluation(testloader, vgg))","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:57:41.449308Z","iopub.execute_input":"2023-05-01T15:57:41.450113Z","iopub.status.idle":"2023-05-01T16:01:56.56569Z","shell.execute_reply.started":"2023-05-01T15:57:41.450073Z","shell.execute_reply":"2023-05-01T16:01:56.564428Z"},"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":928025,"sourceType":"datasetVersion","datasetId":500970}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:12.318484Z","iopub.execute_input":"2025-01-31T18:05:12.318912Z","iopub.status.idle":"2025-01-31T18:05:12.658273Z","shell.execute_reply.started":"2025-01-31T18:05:12.318873Z","shell.execute_reply":"2025-01-31T18:05:12.657339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nx = torch.rand(5, 3)\nprint(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:12.659544Z","iopub.execute_input":"2025-01-31T18:05:12.660041Z","iopub.status.idle":"2025-01-31T18:05:15.872186Z","shell.execute_reply.started":"2025-01-31T18:05:12.660008Z","shell.execute_reply":"2025-01-31T18:05:15.871287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:15.87397Z","iopub.execute_input":"2025-01-31T18:05:15.87435Z","iopub.status.idle":"2025-01-31T18:05:15.930367Z","shell.execute_reply.started":"2025-01-31T18:05:15.874326Z","shell.execute_reply":"2025-01-31T18:05:15.929504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:15.931837Z","iopub.execute_input":"2025-01-31T18:05:15.932098Z","iopub.status.idle":"2025-01-31T18:05:15.950314Z","shell.execute_reply.started":"2025-01-31T18:05:15.932076Z","shell.execute_reply":"2025-01-31T18:05:15.949474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:15.951215Z","iopub.execute_input":"2025-01-31T18:05:15.951515Z","iopub.status.idle":"2025-01-31T18:05:15.966353Z","shell.execute_reply.started":"2025-01-31T18:05:15.951493Z","shell.execute_reply":"2025-01-31T18:05:15.965611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np_array = np.array(data)\nx_np = torch.from_numpy(np_array)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:15.967207Z","iopub.execute_input":"2025-01-31T18:05:15.967493Z","iopub.status.idle":"2025-01-31T18:05:15.984788Z","shell.execute_reply.started":"2025-01-31T18:05:15.967462Z","shell.execute_reply":"2025-01-31T18:05:15.984015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_ones = torch.ones_like(x_data) # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {x_ones} \\n\")\n\nx_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {x_rand} \\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:15.985501Z","iopub.execute_input":"2025-01-31T18:05:15.985726Z","iopub.status.idle":"2025-01-31T18:05:16.002719Z","shell.execute_reply.started":"2025-01-31T18:05:15.985707Z","shell.execute_reply":"2025-01-31T18:05:16.002009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shape = (2,3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:16.00336Z","iopub.execute_input":"2025-01-31T18:05:16.003602Z","iopub.status.idle":"2025-01-31T18:05:16.021806Z","shell.execute_reply.started":"2025-01-31T18:05:16.003578Z","shell.execute_reply":"2025-01-31T18:05:16.020982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tensor = torch.rand(3,4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:16.024588Z","iopub.execute_input":"2025-01-31T18:05:16.024808Z","iopub.status.idle":"2025-01-31T18:05:16.030702Z","shell.execute_reply.started":"2025-01-31T18:05:16.024787Z","shell.execute_reply":"2025-01-31T18:05:16.029966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We move our tensor to the GPU if available\nif torch.cuda.is_available():\n  tensor = tensor.to('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:16.0323Z","iopub.execute_input":"2025-01-31T18:05:16.032617Z","iopub.status.idle":"2025-01-31T18:05:16.225096Z","shell.execute_reply.started":"2025-01-31T18:05:16.032584Z","shell.execute_reply":"2025-01-31T18:05:16.224431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tensor = torch.ones(4, 4)\nprint('First row: ',tensor[0])\nprint('First column: ', tensor[:, 0])\nprint('Last column:', tensor[..., -1])\ntensor[:,1] = 0\nprint(tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:16.226028Z","iopub.execute_input":"2025-01-31T18:05:16.22634Z","iopub.status.idle":"2025-01-31T18:05:16.24244Z","shell.execute_reply.started":"2025-01-31T18:05:16.226307Z","shell.execute_reply":"2025-01-31T18:05:16.241616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"t1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(t1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:16.243267Z","iopub.execute_input":"2025-01-31T18:05:16.243571Z","iopub.status.idle":"2025-01-31T18:05:16.252186Z","shell.execute_reply.started":"2025-01-31T18:05:16.243516Z","shell.execute_reply":"2025-01-31T18:05:16.251326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\ny1 = tensor @ tensor.T\nprint(\"y1\\n\",y1)\ny2 = tensor.matmul(tensor.T)\nprint(\"y2\\n\",y2)\n\n\ny3 = torch.rand_like(tensor)\nprint(\"y3\\n\",y3)\ntorch.matmul(tensor, tensor.T, out=y3)\nprint(\"y3\\n\",y3)\n\n\n# This computes the element-wise product. z1, z2, z3 will have the same value\nz1 = tensor * tensor\nprint(\"z1\\n\",z1)\nz2 = tensor.mul(tensor)\nprint(\"z2\\n\",z2)\n\nz3 = torch.rand_like(tensor)\nprint(\"z3\\n\",z3)\ntorch.mul(tensor, tensor, out=z3)\nprint(\"z3\\n\",z3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:16.253107Z","iopub.execute_input":"2025-01-31T18:05:16.253357Z","iopub.status.idle":"2025-01-31T18:05:16.288815Z","shell.execute_reply.started":"2025-01-31T18:05:16.253336Z","shell.execute_reply":"2025-01-31T18:05:16.288024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Audio and speech processing series with PyTorch","metadata":{}},{"cell_type":"markdown","source":"## Lecture 2","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:16.289657Z","iopub.execute_input":"2025-01-31T18:05:16.289866Z","iopub.status.idle":"2025-01-31T18:05:18.637208Z","shell.execute_reply.started":"2025-01-31T18:05:16.289847Z","shell.execute_reply":"2025-01-31T18:05:18.636462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1- download dataset\n# 2- create data loader\n# 3- build model\n# 4- train\n# 5- save trained model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:18.638057Z","iopub.execute_input":"2025-01-31T18:05:18.638403Z","iopub.status.idle":"2025-01-31T18:05:18.641753Z","shell.execute_reply.started":"2025-01-31T18:05:18.638382Z","shell.execute_reply":"2025-01-31T18:05:18.640999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 128\nEPOCHS = 10\nLEARNING_RATE = 0.001","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:18.642761Z","iopub.execute_input":"2025-01-31T18:05:18.644663Z","iopub.status.idle":"2025-01-31T18:05:18.664312Z","shell.execute_reply.started":"2025-01-31T18:05:18.644627Z","shell.execute_reply":"2025-01-31T18:05:18.663481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FeedForwardNet(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.dense_layers = nn.Sequential(\n            nn.Linear(28 * 28, 256),\n            nn.ReLU(),\n            nn.Linear(256, 10)\n        )\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input_data):\n        x = self.flatten(input_data)\n        logits = self.dense_layers(x)\n        predictions = self.softmax(logits)\n        return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:18.665101Z","iopub.execute_input":"2025-01-31T18:05:18.665413Z","iopub.status.idle":"2025-01-31T18:05:18.680683Z","shell.execute_reply.started":"2025-01-31T18:05:18.66537Z","shell.execute_reply":"2025-01-31T18:05:18.679966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def download_mnist_datasets():\n    train_data = datasets.MNIST(\n        root=\"/kaggle/working/data\",\n        train=True,\n        download=True,\n        transform=ToTensor(),\n    )\n    validation_data = datasets.MNIST(\n        root=\"/kaggle/working/data\",\n        train=False,\n        download=True,\n        transform=ToTensor(),\n    )\n    return train_data, validation_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:18.681496Z","iopub.execute_input":"2025-01-31T18:05:18.68171Z","iopub.status.idle":"2025-01-31T18:05:18.696209Z","shell.execute_reply.started":"2025-01-31T18:05:18.681688Z","shell.execute_reply":"2025-01-31T18:05:18.695574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_data_loader(train_data, batch_size):\n    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n    return train_dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:18.696948Z","iopub.execute_input":"2025-01-31T18:05:18.697215Z","iopub.status.idle":"2025-01-31T18:05:18.711793Z","shell.execute_reply.started":"2025-01-31T18:05:18.697189Z","shell.execute_reply":"2025-01-31T18:05:18.711044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n    for input, target in data_loader:\n        input, target = input.to(device), target.to(device)\n\n        # calculate loss\n        prediction = model(input)\n        loss = loss_fn(prediction, target)\n\n        # backpropagate error and update weights\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n\n    print(f\"loss: {loss.item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:18.712467Z","iopub.execute_input":"2025-01-31T18:05:18.712713Z","iopub.status.idle":"2025-01-31T18:05:18.727627Z","shell.execute_reply.started":"2025-01-31T18:05:18.712692Z","shell.execute_reply":"2025-01-31T18:05:18.726821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model, data_loader, loss_fn, optimiser, device, epochs):\n    for i in range(epochs):\n        print(f\"Epoch {i+1}\")\n        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n        print(\"---------------------------\")\n    print(\"Finished training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:18.728479Z","iopub.execute_input":"2025-01-31T18:05:18.728724Z","iopub.status.idle":"2025-01-31T18:05:18.740969Z","shell.execute_reply.started":"2025-01-31T18:05:18.728704Z","shell.execute_reply":"2025-01-31T18:05:18.740315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n\n    # download data and create data loader\n    train_data, _ = download_mnist_datasets()\n    train_dataloader = create_data_loader(train_data, BATCH_SIZE)\n\n    # construct model and assign it to device\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using {device}\")\n    feed_forward_net = FeedForwardNet().to(device)\n    print(feed_forward_net)\n\n    # initialise loss funtion + optimiser\n    loss_fn = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(feed_forward_net.parameters(),\n                                 lr=LEARNING_RATE)\n\n    # train model\n    train(feed_forward_net, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n\n    # save model\n    torch.save(feed_forward_net.state_dict(), \"feedforwardnet.pth\")\n    print(\"Trained feed forward net saved at feedforwardnet.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:05:18.741815Z","iopub.execute_input":"2025-01-31T18:05:18.742134Z","iopub.status.idle":"2025-01-31T18:06:25.901562Z","shell.execute_reply.started":"2025-01-31T18:05:18.742106Z","shell.execute_reply":"2025-01-31T18:06:25.90062Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lecture 3","metadata":{}},{"cell_type":"code","source":"class_mapping = [\n    \"0\",\n    \"1\",\n    \"2\",\n    \"3\",\n    \"4\",\n    \"5\",\n    \"6\",\n    \"7\",\n    \"8\",\n    \"9\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:25.902444Z","iopub.execute_input":"2025-01-31T18:06:25.902746Z","iopub.status.idle":"2025-01-31T18:06:25.906434Z","shell.execute_reply.started":"2025-01-31T18:06:25.902723Z","shell.execute_reply":"2025-01-31T18:06:25.905599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(model, input, target, class_mapping):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(input)\n        # Tensor (1, 10) -> [ [0.1, 0.01, ..., 0.6] ]\n        predicted_index = predictions[0].argmax(0)\n        predicted = class_mapping[predicted_index]\n        expected = class_mapping[target]\n    return predicted, expected\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:25.907341Z","iopub.execute_input":"2025-01-31T18:06:25.907652Z","iopub.status.idle":"2025-01-31T18:06:25.924085Z","shell.execute_reply.started":"2025-01-31T18:06:25.907621Z","shell.execute_reply":"2025-01-31T18:06:25.923207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # load back the model\n    feed_forward_net = FeedForwardNet()\n    state_dict = torch.load(\"feedforwardnet.pth\")\n    feed_forward_net.load_state_dict(state_dict)\n\n    # load MNIST validation dataset\n    _, validation_data = download_mnist_datasets()\n\n    # get a sample from the validation dataset for inference\n    input, target = validation_data[0][0], validation_data[0][1]\n\n    # make an inference\n    predicted, expected = predict(feed_forward_net, input, target,\n                                  class_mapping)\n    print(f\"Predicted: '{predicted}', expected: '{expected}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:25.924983Z","iopub.execute_input":"2025-01-31T18:06:25.925272Z","iopub.status.idle":"2025-01-31T18:06:26.053164Z","shell.execute_reply.started":"2025-01-31T18:06:25.925241Z","shell.execute_reply":"2025-01-31T18:06:26.052189Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lecture 4","metadata":{}},{"cell_type":"code","source":"import os\n\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport torchaudio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:26.054006Z","iopub.execute_input":"2025-01-31T18:06:26.054241Z","iopub.status.idle":"2025-01-31T18:06:26.933816Z","shell.execute_reply.started":"2025-01-31T18:06:26.054219Z","shell.execute_reply":"2025-01-31T18:06:26.932924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UrbanSoundDataset(Dataset):\n\n    def __init__(self, annotations_file, audio_dir):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label = self._get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        return signal, label\n\n    def _get_audio_sample_path(self, index):\n        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n            index, 0])\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return self.annotations.iloc[index, 6]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:26.937489Z","iopub.execute_input":"2025-01-31T18:06:26.937775Z","iopub.status.idle":"2025-01-31T18:06:26.943172Z","shell.execute_reply.started":"2025-01-31T18:06:26.937751Z","shell.execute_reply":"2025-01-31T18:06:26.942328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    ANNOTATIONS_FILE = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\n    AUDIO_DIR = \"/kaggle/input/urbansound8k\"\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR)\n    print(f\"There are {len(usd)} samples in the dataset.\")\n    signal, label = usd[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:26.944998Z","iopub.execute_input":"2025-01-31T18:06:26.945209Z","iopub.status.idle":"2025-01-31T18:06:27.070348Z","shell.execute_reply.started":"2025-01-31T18:06:26.94519Z","shell.execute_reply":"2025-01-31T18:06:27.069428Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lecture 5","metadata":{}},{"cell_type":"code","source":"import os\n\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport torchaudio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:27.071356Z","iopub.execute_input":"2025-01-31T18:06:27.071668Z","iopub.status.idle":"2025-01-31T18:06:27.075308Z","shell.execute_reply.started":"2025-01-31T18:06:27.071643Z","shell.execute_reply":"2025-01-31T18:06:27.074593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass UrbanSoundDataset(Dataset):\n\n    def __init__(self, annotations_file, audio_dir, transformation,\n                 target_sample_rate):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.transformation = transformation\n        self.target_sample_rate = target_sample_rate\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label = self._get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        signal = self._resample_if_necessary(signal, sr)\n        signal = self._mix_down_if_necessary(signal)\n        signal = self.transformation(signal)\n        return signal, label\n\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n            signal = resampler(signal)\n        return signal\n\n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def _get_audio_sample_path(self, index):\n        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n            index, 0])\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return self.annotations.iloc[index, 6]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:27.076281Z","iopub.execute_input":"2025-01-31T18:06:27.076592Z","iopub.status.idle":"2025-01-31T18:06:27.091764Z","shell.execute_reply.started":"2025-01-31T18:06:27.076558Z","shell.execute_reply":"2025-01-31T18:06:27.090934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    ANNOTATIONS_FILE = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\n    AUDIO_DIR = \"/kaggle/input/urbansound8k\"\n    SAMPLE_RATE = 16000\n\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram,\n                            SAMPLE_RATE)\n    print(f\"There are {len(usd)} samples in the dataset.\")\n    signal, label = usd[0]\n    print(signal, label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:27.092587Z","iopub.execute_input":"2025-01-31T18:06:27.092829Z","iopub.status.idle":"2025-01-31T18:06:27.246693Z","shell.execute_reply.started":"2025-01-31T18:06:27.092809Z","shell.execute_reply":"2025-01-31T18:06:27.245755Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lecture 6","metadata":{}},{"cell_type":"code","source":"import os\n\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport torchaudio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:27.247545Z","iopub.execute_input":"2025-01-31T18:06:27.247782Z","iopub.status.idle":"2025-01-31T18:06:27.251719Z","shell.execute_reply.started":"2025-01-31T18:06:27.24776Z","shell.execute_reply":"2025-01-31T18:06:27.250969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UrbanSoundDataset(Dataset):\n\n    def __init__(self,\n                 annotations_file,\n                 audio_dir,\n                 transformation,\n                 target_sample_rate,\n                 num_samples):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.transformation = transformation\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label = self._get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        signal = self._resample_if_necessary(signal, sr)\n        signal = self._mix_down_if_necessary(signal)\n        signal = self._cut_if_necessary(signal)\n        signal = self._right_pad_if_necessary(signal)\n        signal = self.transformation(signal)\n        return signal, label\n\n    def _cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    def _right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n            signal = resampler(signal)\n        return signal\n\n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def _get_audio_sample_path(self, index):\n        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n            index, 0])\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return self.annotations.iloc[index, 6]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:27.252519Z","iopub.execute_input":"2025-01-31T18:06:27.252853Z","iopub.status.idle":"2025-01-31T18:06:27.268274Z","shell.execute_reply.started":"2025-01-31T18:06:27.252822Z","shell.execute_reply":"2025-01-31T18:06:27.267591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    ANNOTATIONS_FILE = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\n    AUDIO_DIR = \"/kaggle/input/urbansound8k\"\n    SAMPLE_RATE = 22050\n    NUM_SAMPLES = 22050\n\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES)\n    print(f\"There are {len(usd)} samples in the dataset.\")\n    signal, label = usd[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:27.269196Z","iopub.execute_input":"2025-01-31T18:06:27.269518Z","iopub.status.idle":"2025-01-31T18:06:27.34373Z","shell.execute_reply.started":"2025-01-31T18:06:27.269483Z","shell.execute_reply":"2025-01-31T18:06:27.342794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lecture 7","metadata":{}},{"cell_type":"code","source":"import os\n\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport torchaudio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:27.344734Z","iopub.execute_input":"2025-01-31T18:06:27.34504Z","iopub.status.idle":"2025-01-31T18:06:27.348886Z","shell.execute_reply.started":"2025-01-31T18:06:27.345017Z","shell.execute_reply":"2025-01-31T18:06:27.347892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UrbanSoundDataset(Dataset):\n\n    def __init__(self,\n                 annotations_file,\n                 audio_dir,\n                 transformation,\n                 target_sample_rate,\n                 num_samples,\n                 device):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.device = device\n        self.transformation = transformation.to(self.device)\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label = self._get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        signal = signal.to(self.device)\n        signal = self._resample_if_necessary(signal, sr)\n        signal = self._mix_down_if_necessary(signal)\n        signal = self._cut_if_necessary(signal)\n        signal = self._right_pad_if_necessary(signal)\n        signal = self.transformation(signal)\n        return signal, label\n\n    def _cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    def _right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n            signal = resampler(signal)\n        return signal\n\n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def _get_audio_sample_path(self, index):\n        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n            index, 0])\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return self.annotations.iloc[index, 6]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:27.349768Z","iopub.execute_input":"2025-01-31T18:06:27.350009Z","iopub.status.idle":"2025-01-31T18:06:27.367547Z","shell.execute_reply.started":"2025-01-31T18:06:27.349987Z","shell.execute_reply":"2025-01-31T18:06:27.366857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    ANNOTATIONS_FILE = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\n    AUDIO_DIR = \"/kaggle/input/urbansound8k\"\n    SAMPLE_RATE = 22050\n    NUM_SAMPLES = 22050\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using device {device}\")\n\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES,\n                            device)\n    print(f\"There are {len(usd)} samples in the dataset.\")\n    signal, label = usd[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:27.368339Z","iopub.execute_input":"2025-01-31T18:06:27.368599Z","iopub.status.idle":"2025-01-31T18:06:27.509439Z","shell.execute_reply.started":"2025-01-31T18:06:27.368577Z","shell.execute_reply":"2025-01-31T18:06:27.508304Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lecture 8","metadata":{}},{"cell_type":"code","source":"from torch import nn\nfrom torchsummary import summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:27.509951Z","iopub.status.idle":"2025-01-31T18:06:27.510302Z","shell.execute_reply":"2025-01-31T18:06:27.510153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNNNetwork(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # 4 conv blocks / flatten / linear / softmax\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=16,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=64,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=64,\n                out_channels=128,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(128 * 5 * 4, 10)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input_data):\n        x = self.conv1(input_data)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.flatten(x)\n        logits = self.linear(x)\n        predictions = self.softmax(logits)\n        return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:27.510985Z","iopub.status.idle":"2025-01-31T18:06:27.51123Z","shell.execute_reply":"2025-01-31T18:06:27.511127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device {device}\")\n    cnn = CNNNetwork()\n    summary(cnn.to(device), (1, 64, 44))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:06:27.511934Z","iopub.status.idle":"2025-01-31T18:06:27.512165Z","shell.execute_reply":"2025-01-31T18:06:27.512072Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lecture 9","metadata":{}},{"cell_type":"markdown","source":"### urbansounddataset.py","metadata":{}},{"cell_type":"code","source":"import os\n\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport torchaudio\n\n\nclass UrbanSoundDataset(Dataset):\n\n    def __init__(self,\n                 annotations_file,\n                 audio_dir,\n                 transformation,\n                 target_sample_rate,\n                 num_samples,\n                 device):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.device = device\n        self.transformation = transformation.to(self.device)\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label = self._get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        signal = signal.to(self.device)\n        signal = self._resample_if_necessary(signal, sr)\n        signal = self._mix_down_if_necessary(signal)\n        signal = self._cut_if_necessary(signal)\n        signal = self._right_pad_if_necessary(signal)\n        signal = self.transformation(signal)\n        return signal, label\n\n    def _cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    def _right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n            signal = resampler(signal)\n        return signal\n\n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def _get_audio_sample_path(self, index):\n        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n            index, 0])\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return self.annotations.iloc[index, 6]\n\n\nif __name__ == \"__main__\":\n    ANNOTATIONS_FILE = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\n    AUDIO_DIR = \"/kaggle/input/urbansound8k\"\n    SAMPLE_RATE = 22050\n    NUM_SAMPLES = 22050\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using device {device}\")\n\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES,\n                            device)\n    print(f\"There are {len(usd)} samples in the dataset.\")\n    signal, label = usd[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:09:49.54219Z","iopub.execute_input":"2025-01-31T18:09:49.54256Z","iopub.status.idle":"2025-01-31T18:09:50.717101Z","shell.execute_reply.started":"2025-01-31T18:09:49.542512Z","shell.execute_reply":"2025-01-31T18:09:50.71619Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### cnn.py","metadata":{}},{"cell_type":"code","source":"from torch import nn\nfrom torchsummary import summary\n\n\nclass CNNNetwork(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # 4 conv blocks / flatten / linear / softmax\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=16,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=64,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=64,\n                out_channels=128,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(128 * 5 * 4, 10)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input_data):\n        x = self.conv1(input_data)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.flatten(x)\n        logits = self.linear(x)\n        predictions = self.softmax(logits)\n        return predictions\n\n\nif __name__ == \"__main__\":\n    cnn = CNNNetwork()\n    summary(cnn.cuda(), (1, 64, 44))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:09:56.843446Z","iopub.execute_input":"2025-01-31T18:09:56.843771Z","iopub.status.idle":"2025-01-31T18:09:56.965383Z","shell.execute_reply.started":"2025-01-31T18:09:56.843747Z","shell.execute_reply":"2025-01-31T18:09:56.964544Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### train.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom torch import nn\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:10:03.644023Z","iopub.execute_input":"2025-01-31T18:10:03.644349Z","iopub.status.idle":"2025-01-31T18:10:03.648137Z","shell.execute_reply.started":"2025-01-31T18:10:03.64432Z","shell.execute_reply":"2025-01-31T18:10:03.647339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 128\nEPOCHS = 10\nLEARNING_RATE = 0.001\n\nANNOTATIONS_FILE = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\nAUDIO_DIR = \"/kaggle/input/urbansound8k\"\nSAMPLE_RATE = 22050\nNUM_SAMPLES = 22050","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:10:04.555318Z","iopub.execute_input":"2025-01-31T18:10:04.5557Z","iopub.status.idle":"2025-01-31T18:10:04.559705Z","shell.execute_reply.started":"2025-01-31T18:10:04.555667Z","shell.execute_reply":"2025-01-31T18:10:04.558827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_data_loader(train_data, batch_size):\n    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n    return train_dataloader\n\n\ndef train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n    for input, target in data_loader:\n        input, target = input.to(device), target.to(device)\n\n        # calculate loss\n        prediction = model(input)\n        loss = loss_fn(prediction, target)\n\n        # backpropagate error and update weights\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n\n    print(f\"loss: {loss.item()}\")\n\n\ndef train(model, data_loader, loss_fn, optimiser, device, epochs):\n    for i in range(epochs):\n        print(f\"Epoch {i+1}\")\n        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n        print(\"---------------------------\")\n    print(\"Finished training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:10:07.303068Z","iopub.execute_input":"2025-01-31T18:10:07.303365Z","iopub.status.idle":"2025-01-31T18:10:07.309107Z","shell.execute_reply.started":"2025-01-31T18:10:07.303343Z","shell.execute_reply":"2025-01-31T18:10:07.308266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using {device}\")\n\n    # instantiating our dataset object and create data loader\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES,\n                            device)\n    \n    train_dataloader = create_data_loader(usd, BATCH_SIZE)\n\n    # construct model and assign it to device\n    cnn = CNNNetwork().to(device)\n    print(cnn)\n\n    # initialise loss funtion + optimiser\n    loss_fn = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(cnn.parameters(),\n                                 lr=LEARNING_RATE)\n\n    # train model\n    train(cnn, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n\n    # save model\n    torch.save(cnn.state_dict(), \"feedforwardnet.pth\")\n    print(\"Trained feed forward net saved at feedforwardnet.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:10:09.068205Z","iopub.execute_input":"2025-01-31T18:10:09.06857Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lecture 10","metadata":{}},{"cell_type":"markdown","source":"### cnn.py","metadata":{}},{"cell_type":"code","source":"from torch import nn\nfrom torchsummary import summary\n\n\nclass CNNNetwork(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # 4 conv blocks / flatten / linear / softmax\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=16,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=64,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=64,\n                out_channels=128,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(128 * 5 * 4, 10)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input_data):\n        x = self.conv1(input_data)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.flatten(x)\n        logits = self.linear(x)\n        predictions = self.softmax(logits)\n        return predictions\n\n\nif __name__ == \"__main__\":\n    cnn = CNNNetwork()\n    summary(cnn.cuda(), (1, 64, 44))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:26:32.694165Z","iopub.execute_input":"2025-01-31T18:26:32.694445Z","iopub.status.idle":"2025-01-31T18:26:36.70979Z","shell.execute_reply.started":"2025-01-31T18:26:32.694423Z","shell.execute_reply":"2025-01-31T18:26:36.708913Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### urbansounddataset.py","metadata":{}},{"cell_type":"code","source":"import os\n\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport torchaudio\n\n\nclass UrbanSoundDataset(Dataset):\n\n    def __init__(self,\n                 annotations_file,\n                 audio_dir,\n                 transformation,\n                 target_sample_rate,\n                 num_samples,\n                 device):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.device = device\n        self.transformation = transformation.to(self.device)\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label = self._get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        signal = signal.to(self.device)\n        signal = self._resample_if_necessary(signal, sr)\n        signal = self._mix_down_if_necessary(signal)\n        signal = self._cut_if_necessary(signal)\n        signal = self._right_pad_if_necessary(signal)\n        signal = self.transformation(signal)\n        return signal, label\n\n    def _cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    def _right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n            signal = resampler(signal)\n        return signal\n\n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def _get_audio_sample_path(self, index):\n        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n            index, 0])\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return self.annotations.iloc[index, 6]\n\n\nif __name__ == \"__main__\":\n    ANNOTATIONS_FILE = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\n    AUDIO_DIR = \"/kaggle/input/urbansound8k\"\n    SAMPLE_RATE = 22050\n    NUM_SAMPLES = 22050\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using device {device}\")\n\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES,\n                            device)\n    print(f\"There are {len(usd)} samples in the dataset.\")\n    signal, label = usd[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:27:54.399185Z","iopub.execute_input":"2025-01-31T18:27:54.399532Z","iopub.status.idle":"2025-01-31T18:27:55.282442Z","shell.execute_reply.started":"2025-01-31T18:27:54.3995Z","shell.execute_reply":"2025-01-31T18:27:55.281762Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### train.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nBATCH_SIZE = 2048\nEPOCHS = 10\nLEARNING_RATE = 0.001\n\nANNOTATIONS_FILE = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\nAUDIO_DIR = \"/kaggle/input/urbansound8k\"\nSAMPLE_RATE = 22050\nNUM_SAMPLES = 22050\n\n\ndef create_data_loader(train_data, batch_size):\n    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n    return train_dataloader\n\n\ndef train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n    for input, target in data_loader:\n        input, target = input.to(device), target.to(device)\n\n        # calculate loss\n        prediction = model(input)\n        loss = loss_fn(prediction, target)\n\n        # backpropagate error and update weights\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n\n    print(f\"loss: {loss.item()}\")\n\n\ndef train(model, data_loader, loss_fn, optimiser, device, epochs):\n    for i in range(epochs):\n        print(f\"Epoch {i+1}\")\n        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n        print(\"---------------------------\")\n    print(\"Finished training\")\n\n\nif __name__ == \"__main__\":\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using {device}\")\n\n    # instantiating our dataset object and create data loader\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES,\n                            device)\n    \n    train_dataloader = create_data_loader(usd, BATCH_SIZE)\n\n    # construct model and assign it to device\n    cnn = CNNNetwork().to(device)\n    print(cnn)\n\n    # initialise loss funtion + optimiser\n    loss_fn = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(cnn.parameters(),\n                                 lr=LEARNING_RATE)\n\n    # train model\n    train(cnn, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n\n    # save model\n    torch.save(cnn.state_dict(), \"cnnnet.pth\")\n    print(\"Trained feed forward net saved at feedforwardnet.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:33:38.318269Z","iopub.execute_input":"2025-01-31T18:33:38.318595Z","iopub.status.idle":"2025-01-31T18:36:42.022903Z","shell.execute_reply.started":"2025-01-31T18:33:38.318565Z","shell.execute_reply":"2025-01-31T18:36:42.021603Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### inference.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchaudio\n\nclass_mapping = [\n    \"air_conditioner\",\n    \"car_horn\",\n    \"children_playing\",\n    \"dog_bark\",\n    \"drilling\",\n    \"engine_idling\",\n    \"gun_shot\",\n    \"jackhammer\",\n    \"siren\",\n    \"street_music\"\n]\n\n\ndef predict(model, input, target, class_mapping):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(input)\n        # Tensor (1, 10) -> [ [0.1, 0.01, ..., 0.6] ]\n        predicted_index = predictions[0].argmax(0)\n        predicted = class_mapping[predicted_index]\n        expected = class_mapping[target]\n    return predicted, expected\n\n\nif __name__ == \"__main__\":\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # load back the model\n    cnn = CNNNetwork()\n    state_dict = torch.load(\"cnnnet.pth\")\n    cnn.load_state_dict(state_dict)\n\n    # load urban sound dataset dataset\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES,\n                            device)\n\n\n    # get a sample from the urban sound dataset for inference\n    input, target = usd[0][0], usd[0][1] # [batch size, num_channels, fr, time]\n    input.unsqueeze_(0)\n\n    # make an inference\n    predicted, expected = predict(cnn, input, target,\n                                  class_mapping)\n    print(f\"Predicted: '{predicted}', expected: '{expected}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:36:47.70856Z","iopub.execute_input":"2025-01-31T18:36:47.709027Z","iopub.status.idle":"2025-01-31T18:36:47.735496Z","shell.execute_reply.started":"2025-01-31T18:36:47.708985Z","shell.execute_reply":"2025-01-31T18:36:47.734074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11483707,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":1229181,"sourceType":"datasetVersion","datasetId":701987},{"sourceId":8615222,"sourceType":"datasetVersion","datasetId":5156304}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Running\")\nfrom subprocess import Popen, PIPE, STDOUT\nfrom glob import glob\n\nimport os\nimport json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the JSON content\njson_file_path = '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'  \nwith open(json_file_path, 'r') as file:\n    data = json.load(file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the 'test' directory\noutput_dir = '/kaggle/working/abstraction-and-reasoning-challenge/test'  \nos.makedirs(output_dir, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the JSON content into individual files\nfor task_id, task_data in data.items():\n    output_file_path = os.path.join(output_dir, f'{task_id}.json')\n    with open(output_file_path, 'w') as output_file:\n        json.dump(task_data, output_file, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify the files have been created \nprint(f\"Created ARC files in '{output_dir}':\")\nprint(os.listdir(output_dir))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if open(\"../input/arc-solution-source-files-by-icecuber/version.txt\").read().strip() == \"671838222\":\n  print(\"Dataset has correct version\")\nelse:\n  print(\"Dataset version not matching!\")\n  assert(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mySystem(cmd):\n    print(cmd)\n    process = Popen(cmd, stdout=PIPE, stderr=STDOUT, shell=True)\n    for line in iter(process.stdout.readline, b''):\n        print(line.decode(\"utf-8\"), end='')\n    assert(process.wait() == 0)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dummy_run = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for fn in glob(\"/kaggle/working/abstraction-and-reasoning-challenge/test/*.json\"):\n  if \"136b0064\" in fn:\n    print(\"Making dummy submission\")\n    f = open(\"old_submission.csv\", \"w\")\n    f.write(\"output_id,output\\n\")\n    f.close()\n    dummy_run = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not dummy_run:\n  mySystem(\"cp -r ../input/arc-solution-source-files-by-icecuber ./absres-c-files\")\n  mySystem(\"cd absres-c-files; make -j\")\n  mySystem(\"cd absres-c-files; python3 safe_run.py\")\n  mySystem(\"cp absres-c-files/submission_part.csv old_submission.csv\")\n  mySystem(\"tar -czf store.tar.gz absres-c-files/store\")\n  mySystem(\"rm -r absres-c-files\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to translate from old submission format (csv) to new one (json)\ndef translate_submission(file_path):\n    # Read the original submission file\n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n\n    submission_dict = {}\n\n    for line in lines[1:]:  # Skip the header line\n        output_id, output = line.strip().split(',')\n        task_id, output_idx = output_id.split('_')\n        predictions = output.split(' ')  # Split predictions based on ' '\n        \n        # Take only the first two predictions\n        if len(predictions) > 2:\n            predictions = predictions[:2]\n\n        processed_predictions = []\n        for pred in predictions:\n            if pred:  # Check if pred is not an empty string\n                pred_lines = pred.split('|')[1:-1]  # Remove empty strings from split\n                pred_matrix = [list(map(int, line)) for line in pred_lines]\n                processed_predictions.append(pred_matrix)\n\n        attempt_1 = processed_predictions[0] if len(processed_predictions) > 0 else []\n        attempt_2 = processed_predictions[1] if len(processed_predictions) > 1 else []\n\n        if task_id not in submission_dict:\n            submission_dict[task_id] = []\n\n        attempt_dict = {\n            \"attempt_1\": attempt_1,\n            \"attempt_2\": attempt_2\n        }\n\n        if output_idx == '0':\n            submission_dict[task_id].insert(0, attempt_dict)\n        else:\n            submission_dict[task_id].append(attempt_dict)\n    \n    # Write to the new json file\n    with open('submission.json', 'w') as file:\n        json.dump(submission_dict, file, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"translate_submission('/kaggle/working/old_submission.csv')\nprint(\"Done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":242592,"sourceType":"datasetVersion","datasetId":102285}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:18.190353Z","iopub.execute_input":"2025-01-19T13:02:18.190712Z","iopub.status.idle":"2025-01-19T13:02:18.210166Z","shell.execute_reply.started":"2025-01-19T13:02:18.190685Z","shell.execute_reply":"2025-01-19T13:02:18.208828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport struct\n\ndef load_images(file_path):\n    with open(file_path, 'rb') as f:\n        # Read header information\n        magic, num, rows, cols = struct.unpack('>IIII', f.read(16))\n        # Read image data\n        images = np.frombuffer(f.read(), dtype=np.uint8)\n        images = images.reshape(num, rows * cols)  # Flatten to (num_images, 784)\n        return images / 255.0  # Normalize pixel values to [0, 1]\n\ndef load_labels(file_path):\n    with open(file_path, 'rb') as f:\n        # Read header information\n        magic, num = struct.unpack('>II', f.read(8))\n        # Read label data\n        labels = np.frombuffer(f.read(), dtype=np.uint8)\n        return labels\n# Load the training and testing data\nX_train = load_images('/kaggle/input/mnist-dataset/train-images-idx3-ubyte/train-images-idx3-ubyte')\ny_train = load_labels('/kaggle/input/mnist-dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte')\nX_test = load_images('/kaggle/input/mnist-dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\ny_test = load_labels('/kaggle/input/mnist-dataset/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:18.211687Z","iopub.execute_input":"2025-01-19T13:02:18.211997Z","iopub.status.idle":"2025-01-19T13:02:18.223798Z","shell.execute_reply.started":"2025-01-19T13:02:18.211969Z","shell.execute_reply":"2025-01-19T13:02:18.222327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One-hot encode the labels\ndef one_hot_encode(labels, num_classes=10):\n    one_hot = np.zeros((labels.size, num_classes))\n    one_hot[np.arange(labels.size), labels] = 1\n    return one_hot\n\ny_train = one_hot_encode(y_train)\ny_test = one_hot_encode(y_test)\n\n# Split training set into training and validation\ndef train_val_split(X, y, val_size=0.2):\n    split_idx = int((1 - val_size) * X.shape[0])\n    return X[:split_idx], X[split_idx:], y[:split_idx], y[split_idx:]\n\nX_train, X_val, y_train, y_val = train_val_split(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:18.226108Z","iopub.execute_input":"2025-01-19T13:02:18.2266Z","iopub.status.idle":"2025-01-19T13:02:18.541956Z","shell.execute_reply.started":"2025-01-19T13:02:18.226558Z","shell.execute_reply":"2025-01-19T13:02:18.540705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def initialize_parameters(input_size, hidden_size, output_size):\n    np.random.seed(42)\n    W1 = np.random.randn(input_size, hidden_size) * 0.01\n    b1 = np.zeros((1, hidden_size))\n    W2 = np.random.randn(hidden_size, output_size) * 0.01\n    b2 = np.zeros((1, output_size))\n    return W1, b1, W2, b2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:18.543612Z","iopub.execute_input":"2025-01-19T13:02:18.543981Z","iopub.status.idle":"2025-01-19T13:02:18.549929Z","shell.execute_reply.started":"2025-01-19T13:02:18.543937Z","shell.execute_reply":"2025-01-19T13:02:18.548614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Forward","metadata":{}},{"cell_type":"code","source":"def relu(Z):\n    return np.maximum(0, Z)\n\ndef relu_derivative(Z):\n    return Z > 0\n\ndef softmax(Z):\n    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n\ndef forward_propagation(X, W1, b1, W2, b2):\n    Z1 = np.dot(X, W1) + b1\n    A1 = relu(Z1)\n    Z2 = np.dot(A1, W2) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:18.5514Z","iopub.execute_input":"2025-01-19T13:02:18.551793Z","iopub.status.idle":"2025-01-19T13:02:18.569854Z","shell.execute_reply.started":"2025-01-19T13:02:18.551748Z","shell.execute_reply":"2025-01-19T13:02:18.568638Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Back propagation","metadata":{}},{"cell_type":"code","source":"def backward_propagation(X, y, Z1, A1, Z2, A2, W1, W2):\n    m = X.shape[0]\n    dZ2 = A2 - y\n    dW2 = np.dot(A1.T, dZ2) / m\n    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n    dA1 = np.dot(dZ2, W2.T)\n    dZ1 = dA1 * relu_derivative(Z1)\n    dW1 = np.dot(X.T, dZ1) / m\n    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n    return dW1, db1, dW2, db2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:18.572386Z","iopub.execute_input":"2025-01-19T13:02:18.572826Z","iopub.status.idle":"2025-01-19T13:02:18.591491Z","shell.execute_reply.started":"2025-01-19T13:02:18.572783Z","shell.execute_reply":"2025-01-19T13:02:18.590025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"train fn","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef train(X_train, y_train, X_val, y_val, input_size, hidden_size, output_size, epochs, lr):\n    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n    \n    train_losses = []\n    val_losses = []\n\n    for epoch in range(epochs):\n        Z1, A1, Z2, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n        train_loss = -np.mean(np.sum(y_train * np.log(A2 + 1e-9), axis=1))\n        train_losses.append(train_loss)\n\n        dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, Z1, A1, Z2, A2, W1, W2)\n        W1 -= lr * dW1\n        b1 -= lr * db1\n        W2 -= lr * dW2\n        b2 -= lr * db2\n\n        if epoch % 10 == 0:\n            _, _, _, A2_val = forward_propagation(X_val, W1, b1, W2, b2)\n            val_loss = -np.mean(np.sum(y_val * np.log(A2_val + 1e-9), axis=1))\n            val_losses.append(val_loss)\n            print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n    \n    return W1, b1, W2, b2, train_losses, val_losses\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:18.593793Z","iopub.execute_input":"2025-01-19T13:02:18.594266Z","iopub.status.idle":"2025-01-19T13:02:18.613249Z","shell.execute_reply.started":"2025-01-19T13:02:18.594224Z","shell.execute_reply":"2025-01-19T13:02:18.611967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"rotate fn","metadata":{}},{"cell_type":"code","source":"from scipy.ndimage import rotate\n\ndef augment_with_rotations(X, y, angles=[90, 180, 270]):\n    augmented_X = [X]\n    augmented_y = [y]\n    for angle in angles:\n        rotated_images = np.array([rotate(img.reshape(28, 28), angle, reshape=False).flatten() for img in X])\n        augmented_X.append(rotated_images)\n        augmented_y.append(y)\n    return np.vstack(augmented_X), np.vstack(augmented_y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:18.614497Z","iopub.execute_input":"2025-01-19T13:02:18.6148Z","iopub.status.idle":"2025-01-19T13:02:18.637493Z","shell.execute_reply.started":"2025-01-19T13:02:18.61477Z","shell.execute_reply":"2025-01-19T13:02:18.636119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Augment training data with rotated images\nX_train_augmented, y_train_augmented = augment_with_rotations(X_train, y_train)\n# Augment Validation data with rotated images\nX_val_augmented, y_val_augmented = augment_with_rotations(X_val, y_val)\n# Augment testing data with rotated images\nX_test_augmented, y_test_augmented = augment_with_rotations(X_test, y_test)\n\nprint(f\"Original Training Data Shape: {X_train.shape}\")\nprint(f\"Augmented Training Data Shape: {X_train_augmented.shape}\\n\")\nprint(f\"Original Validation Data Shape: {X_val.shape}\")\nprint(f\"Augmented Validation Data Shape: {X_val_augmented.shape}\\n\")\nprint(f\"Original testing Data Shape: {X_test.shape}\")\nprint(f\"Augmented testing Data Shape: {X_test_augmented.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:18.638647Z","iopub.execute_input":"2025-01-19T13:02:18.638925Z","iopub.status.idle":"2025-01-19T13:02:56.995525Z","shell.execute_reply.started":"2025-01-19T13:02:18.638902Z","shell.execute_reply":"2025-01-19T13:02:56.994196Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"train","metadata":{}},{"cell_type":"code","source":"input_size = 784  # 28x28 images flattened\nhidden_size = 128\noutput_size = 10\nepochs = 2000 * 10\nlearning_rate = 0.02","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:56.996617Z","iopub.execute_input":"2025-01-19T13:02:56.996945Z","iopub.status.idle":"2025-01-19T13:02:57.002814Z","shell.execute_reply.started":"2025-01-19T13:02:56.996919Z","shell.execute_reply":"2025-01-19T13:02:57.000942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model with augmented data\nW1, b1, W2, b2, train_losses, val_losses = train(\n    X_train_augmented, y_train_augmented, X_val, y_val, input_size, hidden_size, output_size, epochs, learning_rate)\n\n# Plot loss curves\nplt.plot(range(epochs), train_losses, label=\"Training Loss\")\nplt.plot(range(0, epochs, 10), val_losses, label=\"Validation Loss\", linestyle='--')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss (Augmented Data)')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:02:57.003763Z","iopub.execute_input":"2025-01-19T13:02:57.004062Z","iopub.status.idle":"2025-01-19T13:40:21.338471Z","shell.execute_reply.started":"2025-01-19T13:02:57.004036Z","shell.execute_reply":"2025-01-19T13:40:21.337135Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"test","metadata":{}},{"cell_type":"code","source":"def predict(X, W1, b1, W2, b2):\n    _, _, _, A2 = forward_propagation(X, W1, b1, W2, b2)\n    return np.argmax(A2, axis=1)\n\n# Make predictions on test data\ny_test_pred = predict(X_test_augmented, W1, b1, W2, b2)\ny_test_true = np.argmax(y_test_augmented, axis=1)\naccuracy = np.mean(y_test_pred == y_test_true)\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:40:21.33973Z","iopub.execute_input":"2025-01-19T13:40:21.340145Z","iopub.status.idle":"2025-01-19T13:40:21.406247Z","shell.execute_reply.started":"2025-01-19T13:40:21.340113Z","shell.execute_reply":"2025-01-19T13:40:21.404922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_predictions(X, y_true, y_pred, num_images=5):\n    ctr = num_images\n    col = 10\n    while ctr>0:\n        l=num_images-ctr\n        plt.figure(figsize=(num_images*2, col))\n        for i in range(l,l+col):\n            plt.subplot(1, num_images, i+1)\n            plt.imshow(X[i].reshape(28, 28), cmap='gray')\n            plt.title(f\"True: {np.argmax(y_true[i])}, Pred: {y_pred[i]}\")\n            plt.axis('off')\n        plt.show()\n        ctr-=col\n\n# Plot sample predictions\nplot_predictions(X_test, y_test, y_test_pred, num_images=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:40:21.407477Z","iopub.execute_input":"2025-01-19T13:40:21.407822Z","iopub.status.idle":"2025-01-19T13:40:28.855009Z","shell.execute_reply.started":"2025-01-19T13:40:21.407796Z","shell.execute_reply":"2025-01-19T13:40:28.853756Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"save submission","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({'ImageId': np.arange(1, len(y_test_pred) + 1), 'Label': y_test_pred})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:40:28.85727Z","iopub.execute_input":"2025-01-19T13:40:28.857656Z","iopub.status.idle":"2025-01-19T13:40:28.875453Z","shell.execute_reply.started":"2025-01-19T13:40:28.857551Z","shell.execute_reply":"2025-01-19T13:40:28.873934Z"}},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10545331,"sourceType":"datasetVersion","datasetId":6524657},{"sourceId":10671289,"sourceType":"datasetVersion","datasetId":6609570}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(\"Output Dir:\")\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-02-06T07:22:49.956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# main.py - Combined ECAPA-TDNN code (Python 3.10)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport numpy as np\nfrom typing import List, Tuple\n\n# ========================\n# dataLoader.py contents\n# ========================\nclass CustomDataset(data.Dataset):\n    def __init__(self, data_list: List[torch.Tensor], labels: torch.Tensor):\n        self.data_list = data_list\n        self.labels = labels\n\n    def __len__(self) -> int:\n        return len(self.data_list)\n\n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        return self.data_list[idx], self.labels[idx]\n\n# ========================\n# model.py contents\n# ========================\nclass ECAPA_TDNN(nn.Module):\n    def __init__(self, input_dim: int, output_dim: int):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.fc(x)\n\n# ========================\n# loss.py contents\n# ========================\nclass SoftmaxLoss(nn.Module):\n    def __init__(self, num_classes: int, embedding_dim: int):\n        super().__init__()\n        self.fc = nn.Linear(embedding_dim, num_classes)\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n        logits = self.fc(embeddings)\n        return self.loss_fn(logits, labels)\n\n# ========================\n# tools.py contents\n# ========================\ndef compute_accuracy(predictions: torch.Tensor, labels: torch.Tensor) -> float:\n    _, predicted = torch.max(predictions, 1)\n    return (predicted == labels).sum().item() / labels.size(0)\n\n# ========================\n# ECAPAModel.py & trainECAPAModel.py contents\n# ========================\ndef train_model(model: nn.Module, train_loader: data.DataLoader, criterion: nn.Module, optimizer: optim.Optimizer, epochs: int = 10) -> None:\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0.0\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n\n# Main execution\nif __name__ == \"__main__\":\n    input_dim, output_dim, num_classes = 256, 512, 10\n    train_data = torch.randn(100, input_dim)\n    train_labels = torch.randint(0, num_classes, (100,))\n    train_dataset = CustomDataset(train_data.tolist(), train_labels)\n    train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n    \n    model = ECAPA_TDNN(input_dim, output_dim)\n    criterion = SoftmaxLoss(num_classes, output_dim)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    train_model(model, train_loader, criterion, optimizer)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-06T07:22:49.959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Done\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-06T07:22:49.961Z"}},"outputs":[],"execution_count":null}]}
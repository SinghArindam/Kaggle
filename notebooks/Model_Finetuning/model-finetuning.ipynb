{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":282742,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":239467,"modelId":222398}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip3 install -q -U bitsandbytes==0.42.0\n# !pip3 install -q -U peft==0.8.2\n# !pip3 install -q -U trl==0.7.10\n# !pip3 install -q -U accelerate==0.27.1\n# !pip3 install -q -U datasets==2.17.0\n# !pip3 install -q -U transformers==4.38.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:54.850718Z","iopub.execute_input":"2025-05-24T16:20:54.851349Z","iopub.status.idle":"2025-05-24T16:20:54.856386Z","shell.execute_reply.started":"2025-05-24T16:20:54.851321Z","shell.execute_reply":"2025-05-24T16:20:54.855424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip3 install -q -U bitsandbytes\n!pip3 install -q -U peft\n!pip3 install -q -U trl\n!pip3 install -q -U accelerate\n!pip3 install -q -U datasets\n!pip3 install -q -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:55.920746Z","iopub.execute_input":"2025-05-24T16:20:55.921031Z","iopub.status.idle":"2025-05-24T16:23:10.457866Z","shell.execute_reply.started":"2025-05-24T16:20:55.921008Z","shell.execute_reply":"2025-05-24T16:23:10.456415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:23:10.46048Z","iopub.execute_input":"2025-05-24T16:23:10.460895Z","iopub.status.idle":"2025-05-24T16:23:10.583402Z","shell.execute_reply.started":"2025-05-24T16:23:10.46086Z","shell.execute_reply":"2025-05-24T16:23:10.582189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom peft import LoraConfig, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:23:10.584824Z","iopub.execute_input":"2025-05-24T16:23:10.585297Z","iopub.status.idle":"2025-05-24T16:23:49.188974Z","shell.execute_reply.started":"2025-05-24T16:23:10.585256Z","shell.execute_reply":"2025-05-24T16:23:49.18823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_id = \"/kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:23:49.190883Z","iopub.execute_input":"2025-05-24T16:23:49.191623Z","iopub.status.idle":"2025-05-24T16:23:49.195973Z","shell.execute_reply.started":"2025-05-24T16:23:49.191596Z","shell.execute_reply":"2025-05-24T16:23:49.195092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                 # Load model weights in 4-bit\n    bnb_4bit_quant_type=\"nf4\",         # Use NF4 quantization\n    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in bfloat16 (recommended for performance)\n    bnb_4bit_use_double_quant=True,    # Use double quantization for more precision\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:26:31.883234Z","iopub.execute_input":"2025-05-24T16:26:31.885459Z","iopub.status.idle":"2025-05-24T16:26:31.898816Z","shell.execute_reply.started":"2025-05-24T16:26:31.88539Z","shell.execute_reply":"2025-05-24T16:26:31.897819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:26:33.329256Z","iopub.execute_input":"2025-05-24T16:26:33.329708Z","iopub.status.idle":"2025-05-24T16:26:33.787048Z","shell.execute_reply.started":"2025-05-24T16:26:33.329675Z","shell.execute_reply":"2025-05-24T16:26:33.785612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:23:49.272388Z","iopub.status.idle":"2025-05-24T16:23:49.272739Z","shell.execute_reply.started":"2025-05-24T16:23:49.272566Z","shell.execute_reply":"2025-05-24T16:23:49.272579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_prompt(data_point):\n    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n\n    :param data_point: dict: Data point\n    :return: dict: tokenzed prompt\n    \"\"\"\n    prefix_text = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\\\n\\\\n'\n    # Samples with additional context into.\n    if data_point['input']:\n        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} <end_of_turn>\\\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n    # Without\n    else:\n        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} <end_of_turn>\\\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n    return text\n\n# add the \"prompt\" column in the dataset\ntext_column = [generate_prompt(data_point) for data_point in dataset]\ndataset = dataset.add_column(\"prompt\", text_column)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:23:49.274386Z","iopub.status.idle":"2025-05-24T16:23:49.274803Z","shell.execute_reply.started":"2025-05-24T16:23:49.274607Z","shell.execute_reply":"2025-05-24T16:23:49.274627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\ndataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.2)\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=32,\n    target_modules=['o_proj', 'q_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj'],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n) \nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#new code using SFTTrainer\nimport transformers\n\nfrom trl import SFTTrainer\n\ntokenizer.pad_token = tokenizer.eos_token\ntorch.cuda.empty_cache()\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    dataset_text_field=\"prompt\",\n    peft_config=lora_config,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=0.03,\n        max_steps=100,\n        learning_rate=2e-4,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\",\n        save_strategy=\"epoch\",\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Start the training process\ntrainer.train()\n\nnew_model = \"gemma3-Code-Instruct-Finetune-test\" #Name of the model you will be pushing to huggingface model hub\n# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge the model with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n)\nmerged_model= PeftModel.from_pretrained(base_model, new_model)\nmerged_model= merged_model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model_gemma3-Code-Instruct-Finetune-test\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model_gemma3-Code-Instruct-Finetune-test\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_completion(query: str, model, tokenizer) -> str:\n  device = \"cuda:0\"\n  prompt_template = \"\"\"\n  <start_of_turn>user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  {query}\n  <end_of_turn>\\\\n<start_of_turn>model\n  \n  \"\"\"\n  prompt = prompt_template.format(query=query)\n  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n  model_inputs = encodeds.to(device)\n  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n  # decoded = tokenizer.batch_decode(generated_ids)\n  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n  return (decoded)\n\nresult = get_completion(query=\"code the fibonacci series in python using reccursion\", model=merged_model, tokenizer=tokenizer)\nprint(result)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
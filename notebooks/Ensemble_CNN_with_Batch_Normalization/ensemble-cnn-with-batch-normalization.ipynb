{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport matplotlib.pyplot as plt\n\n# Load and prepare data\ndef load_data():\n    # For Kaggle competition, load data from CSV files\n    train = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n    test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n    \n    # Extract labels and features\n    y_train = train['label'].values\n    X_train = train.drop('label', axis=1).values\n    X_test = test.values\n    \n    # Reshape and normalize\n    X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n    X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n    \n    # One-hot encode labels\n    y_train = to_categorical(y_train, num_classes=10)\n    \n    return X_train, y_train, X_test\n\n# Define CNN model with batch normalization\ndef create_model():\n    model = Sequential()\n    \n    # First convolutional block\n    model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(28, 28, 1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Dropout(0.25))\n    \n    # Second convolutional block\n    model.add(Conv2D(64, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Dropout(0.25))\n    \n    # Fully connected layers\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n    \n    # Compile model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n# Learning rate scheduler\ndef lr_schedule(epoch):\n    initial_lr = 0.001\n    if epoch >= 10:\n        return initial_lr * 0.5\n    if epoch >= 15:\n        return initial_lr * 0.1\n    return initial_lr\n\n# Create and train ensemble\ndef train_ensemble(X_train, y_train, n_models=7):\n    models = []\n    \n    # Data augmentation\n    datagen = ImageDataGenerator(\n        rotation_range=10,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1\n    )\n    \n    # Create validation set\n    X_train_full, X_val, y_train_full, y_val = train_test_split(\n        X_train, y_train, test_size=0.1, random_state=42\n    )\n    \n    # Train each model\n    for i in range(n_models):\n        print(f\"Training model {i+1}/{n_models}\")\n        \n        # Create and compile model\n        model = create_model()\n        \n        # Train with data augmentation and learning rate scheduler\n        lr_scheduler = LearningRateScheduler(lr_schedule)\n        \n        # Shuffle data differently for each model\n        X_train_shuffle, y_train_shuffle = shuffle_data(X_train_full, y_train_full, seed=i)\n        \n        # Fit model\n        model.fit(\n            datagen.flow(X_train_shuffle, y_train_shuffle, batch_size=64),\n            epochs=20,\n            validation_data=(X_val, y_val),\n            callbacks=[lr_scheduler]\n        )\n        \n        models.append(model)\n    \n    return models\n\n# Helper function to shuffle data\ndef shuffle_data(X, y, seed=None):\n    indices = np.arange(X.shape[0])\n    np.random.seed(seed)\n    np.random.shuffle(indices)\n    return X[indices], y[indices]\n\n# Generate ensemble predictions\ndef ensemble_predictions(models, X_test):\n    predictions = np.zeros((X_test.shape[0], 10))\n    \n    for model in models:\n        pred = model.predict(X_test)\n        predictions += pred\n    \n    predictions /= len(models)\n    return np.argmax(predictions, axis=1)\n\n# Main execution\ndef main():\n    # Load data\n    X_train, y_train, X_test = load_data()\n    \n    # Train ensemble\n    models = train_ensemble(X_train, y_train, n_models=7)\n    \n    # Generate predictions\n    predictions = ensemble_predictions(models, X_test)\n    \n    # Save predictions to CSV for Kaggle submission\n    submission = pd.DataFrame({\n        'ImageId': range(1, len(predictions) + 1),\n        'Label': predictions\n    })\n    submission.to_csv('submission.csv', index=False)\n    print(\"Submission file created!\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
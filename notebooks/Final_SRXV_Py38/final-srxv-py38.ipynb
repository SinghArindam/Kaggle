{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10496674,"sourceType":"datasetVersion","datasetId":6499058}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T05:41:43.966365Z","iopub.execute_input":"2025-01-25T05:41:43.966628Z","iopub.status.idle":"2025-01-25T05:41:45.301082Z","shell.execute_reply.started":"2025-01-25T05:41:43.966603Z","shell.execute_reply":"2025-01-25T05:41:45.299922Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# MiniConda setup","metadata":{}},{"cell_type":"code","source":"!sudo wget https://anaconda.org/anaconda/python/3.8.3/download/linux-ppc64le/python-3.8.3-ha7b6439_2.tar.bz2 -O miniconda.sh\n!sudo bash miniconda.sh -b -p /kaggle/working/miniconda\n!sudo rm -rf miniconda.sh\n!sudo export PATH=/kaggle/working/miniconda/bin:$PATH\n!sudo conda init bash\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T05:41:45.302475Z","iopub.execute_input":"2025-01-25T05:41:45.30306Z","iopub.status.idle":"2025-01-25T05:41:49.111427Z","shell.execute_reply.started":"2025-01-25T05:41:45.303026Z","shell.execute_reply":"2025-01-25T05:41:49.109892Z"}},"outputs":[{"name":"stdout","text":"--2025-01-25 05:41:45--  https://anaconda.org/anaconda/python/3.8.3/download/linux-ppc64le/python-3.8.3-ha7b6439_2.tar.bz2\nResolving anaconda.org (anaconda.org)... 104.19.145.37, 104.19.144.37, 2606:4700::6813:9025, ...\nConnecting to anaconda.org (anaconda.org)|104.19.145.37|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://binstar-cio-packages-prod.s3.amazonaws.com/5638023e9c73330b8ae83e9a/5eff83c9fe6b2a0a2b15391a?response-content-disposition=attachment%3B%20filename%3D%22python-3.8.3-ha7b6439_2.tar.bz2%22%3B%20filename%2A%3DUTF-8%27%27python-3.8.3-ha7b6439_2.tar.bz2&response-content-type=application%2Fx-tar&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAWUI46DZFJN2JEEA2%2F20250125%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250125T054145Z&X-Amz-Expires=600&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEB4aCXVzLWVhc3QtMSJHMEUCIQDT5ASLJH6ZA3CCKL83r3Eth4hwgEYGviNPGcfuYMC5bQIgTiBKjMzf8qbO%2FZqmTCq2E1desp%2FLh9aUqGKYN8PpK3gq%2FgQIJhAAGgw0NTU4NjQwOTgzNzgiDGqYJEF8mlRB4JClnCrbBB6X1gbYscBqWo99ZFXpmsf%2FescZrKcbLF4EwdnCX4c5XIgg9Pbt1R0GvQP0mgZfg%2FnVbZcb9mIlH7zI2aQ788VTQt85QAv%2BGlDKIJxJqoukP2L1n8c07ME%2BDqJVDXWc5h7v1A2PUYoTkri5ZlWNm0dh2s7Y1JDgSHjDRUjn3q3UXXo5QZ4OPk4y4lkxfBshRxyUjX2tdmlG0DQJ2TEKRc7yb8w3UslkXN6Z%2BSa0bNcUgmp7tgbZG4M9RrbQIbCYKY%2FG%2BKuQKGIpHDsXL%2BnQJHNIn4h6F5ZEEuSr6Yaw%2FhFjM9XJG5c0IfNENiXwP%2Fnn%2FwL8siYEehbG0q1GJ2sudlHOQYt%2BR7xmnsWE6I7%2FBZAvFL0GGDnBHz4Xq4DyI9ldFjMBtlBwwrdaT8ktZQ7GM%2FBeKbVria7lY1N4b9TmLGp3MfHZijzDjJHD2yCFDb8RaKXouwuE4FnwPZeyFgB1FthRf4OiB%2BgMvRJpISGW1Vr56ylh6agbFSN3r6T3RcYJojS1k971rv80NWNpNdoy0fjd1Ez6GmejOLG%2B4q5wginYrbTPuteCRHTp6k7JTvyrjDZPcK9pby%2BE5F%2BWnTIELTIXpDuXBh5gwZhxX1rkqFBfMalWbLakH95TyNwxXL366KNay%2Fhef5MptlpnSQ%2F3Tx8olBhzP9l%2F2woh7xXJLaUgEvoFtppG9nUEfuEtgeV5vk%2FkiXyl3qXGzuJ%2BlYOp3LWWlM5nOgUmuJKjcMkmjEhw34A183fphZCkVHJHELDDeeSMt1AUPseTDjfiQOUhN7GbTFcna7Wk4S1ZVDDI5tG8BjqaAbNdO8wRsyHN4uTm0KtuUR06Z%2FE1hsaSYkjPOjxnYhJlUcv7OoW3po7Mp0xwOZsT3u85Lpw8YGommICXQmFZM776ysWLWiBpWSNMLTAFurmMr2bha%2BviBx0uv6rrfzLP7b9A77AxqejCNehFDj%2B2XoBkVfW49f8T17WeClA%2BGFGSJ%2Bp3G7BQmHAeQJLBsPYjhWhbdG2hCFBwW8o%3D&X-Amz-Signature=dffd060ce86aa38ff56c7686accb9d084880c4a6c6b9ffdd6b7fbe29f177690f [following]\n--2025-01-25 05:41:45--  https://binstar-cio-packages-prod.s3.amazonaws.com/5638023e9c73330b8ae83e9a/5eff83c9fe6b2a0a2b15391a?response-content-disposition=attachment%3B%20filename%3D%22python-3.8.3-ha7b6439_2.tar.bz2%22%3B%20filename%2A%3DUTF-8%27%27python-3.8.3-ha7b6439_2.tar.bz2&response-content-type=application%2Fx-tar&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAWUI46DZFJN2JEEA2%2F20250125%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250125T054145Z&X-Amz-Expires=600&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEB4aCXVzLWVhc3QtMSJHMEUCIQDT5ASLJH6ZA3CCKL83r3Eth4hwgEYGviNPGcfuYMC5bQIgTiBKjMzf8qbO%2FZqmTCq2E1desp%2FLh9aUqGKYN8PpK3gq%2FgQIJhAAGgw0NTU4NjQwOTgzNzgiDGqYJEF8mlRB4JClnCrbBB6X1gbYscBqWo99ZFXpmsf%2FescZrKcbLF4EwdnCX4c5XIgg9Pbt1R0GvQP0mgZfg%2FnVbZcb9mIlH7zI2aQ788VTQt85QAv%2BGlDKIJxJqoukP2L1n8c07ME%2BDqJVDXWc5h7v1A2PUYoTkri5ZlWNm0dh2s7Y1JDgSHjDRUjn3q3UXXo5QZ4OPk4y4lkxfBshRxyUjX2tdmlG0DQJ2TEKRc7yb8w3UslkXN6Z%2BSa0bNcUgmp7tgbZG4M9RrbQIbCYKY%2FG%2BKuQKGIpHDsXL%2BnQJHNIn4h6F5ZEEuSr6Yaw%2FhFjM9XJG5c0IfNENiXwP%2Fnn%2FwL8siYEehbG0q1GJ2sudlHOQYt%2BR7xmnsWE6I7%2FBZAvFL0GGDnBHz4Xq4DyI9ldFjMBtlBwwrdaT8ktZQ7GM%2FBeKbVria7lY1N4b9TmLGp3MfHZijzDjJHD2yCFDb8RaKXouwuE4FnwPZeyFgB1FthRf4OiB%2BgMvRJpISGW1Vr56ylh6agbFSN3r6T3RcYJojS1k971rv80NWNpNdoy0fjd1Ez6GmejOLG%2B4q5wginYrbTPuteCRHTp6k7JTvyrjDZPcK9pby%2BE5F%2BWnTIELTIXpDuXBh5gwZhxX1rkqFBfMalWbLakH95TyNwxXL366KNay%2Fhef5MptlpnSQ%2F3Tx8olBhzP9l%2F2woh7xXJLaUgEvoFtppG9nUEfuEtgeV5vk%2FkiXyl3qXGzuJ%2BlYOp3LWWlM5nOgUmuJKjcMkmjEhw34A183fphZCkVHJHELDDeeSMt1AUPseTDjfiQOUhN7GbTFcna7Wk4S1ZVDDI5tG8BjqaAbNdO8wRsyHN4uTm0KtuUR06Z%2FE1hsaSYkjPOjxnYhJlUcv7OoW3po7Mp0xwOZsT3u85Lpw8YGommICXQmFZM776ysWLWiBpWSNMLTAFurmMr2bha%2BviBx0uv6rrfzLP7b9A77AxqejCNehFDj%2B2XoBkVfW49f8T17WeClA%2BGFGSJ%2Bp3G7BQmHAeQJLBsPYjhWhbdG2hCFBwW8o%3D&X-Amz-Signature=dffd060ce86aa38ff56c7686accb9d084880c4a6c6b9ffdd6b7fbe29f177690f\nResolving binstar-cio-packages-prod.s3.amazonaws.com (binstar-cio-packages-prod.s3.amazonaws.com)... 16.182.103.209, 3.5.29.78, 52.217.13.220, ...\nConnecting to binstar-cio-packages-prod.s3.amazonaws.com (binstar-cio-packages-prod.s3.amazonaws.com)|16.182.103.209|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 60684286 (58M) [application/x-tar]\nSaving to: ‘miniconda.sh’\n\nminiconda.sh        100%[===================>]  57.87M  29.1MB/s    in 2.0s    \n\n2025-01-25 05:41:48 (29.1 MB/s) - ‘miniconda.sh’ saved [60684286/60684286]\n\nminiconda.sh: line 11: syntax error near unexpected token `('\nminiconda.sh: line 11: `\u001fd6'%�.@E[$\u0010�\u0001B� ,J��qPB�\f\t @\u0019\f(T�'\nsudo: export: command not found\nsudo: conda: command not found\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!sudo wget -O python-3.8.3.tar.bz2 https://anaconda.org/anaconda/python/3.8.3/download/linux-ppc64le/python-3.8.3-ha7b6439_2.tar.bz2\n!sudo conda create -n py38_env python=3.8 -y\n!sudo conda activate py38_env\n!sudo conda install python-3.8.3.tar.bz2\n!sudo python --version\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T05:41:49.112826Z","iopub.execute_input":"2025-01-25T05:41:49.113268Z","iopub.status.idle":"2025-01-25T05:41:52.269892Z","shell.execute_reply.started":"2025-01-25T05:41:49.113216Z","shell.execute_reply":"2025-01-25T05:41:52.268422Z"}},"outputs":[{"name":"stdout","text":"--2025-01-25 05:41:49--  https://anaconda.org/anaconda/python/3.8.3/download/linux-ppc64le/python-3.8.3-ha7b6439_2.tar.bz2\nResolving anaconda.org (anaconda.org)... 104.19.145.37, 104.19.144.37, 2606:4700::6813:9125, ...\nConnecting to anaconda.org (anaconda.org)|104.19.145.37|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://binstar-cio-packages-prod.s3.amazonaws.com/5638023e9c73330b8ae83e9a/5eff83c9fe6b2a0a2b15391a?response-content-disposition=attachment%3B%20filename%3D%22python-3.8.3-ha7b6439_2.tar.bz2%22%3B%20filename%2A%3DUTF-8%27%27python-3.8.3-ha7b6439_2.tar.bz2&response-content-type=application%2Fx-tar&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAWUI46DZFJN2JEEA2%2F20250125%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250125T054149Z&X-Amz-Expires=600&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEB4aCXVzLWVhc3QtMSJHMEUCIQDT5ASLJH6ZA3CCKL83r3Eth4hwgEYGviNPGcfuYMC5bQIgTiBKjMzf8qbO%2FZqmTCq2E1desp%2FLh9aUqGKYN8PpK3gq%2FgQIJhAAGgw0NTU4NjQwOTgzNzgiDGqYJEF8mlRB4JClnCrbBB6X1gbYscBqWo99ZFXpmsf%2FescZrKcbLF4EwdnCX4c5XIgg9Pbt1R0GvQP0mgZfg%2FnVbZcb9mIlH7zI2aQ788VTQt85QAv%2BGlDKIJxJqoukP2L1n8c07ME%2BDqJVDXWc5h7v1A2PUYoTkri5ZlWNm0dh2s7Y1JDgSHjDRUjn3q3UXXo5QZ4OPk4y4lkxfBshRxyUjX2tdmlG0DQJ2TEKRc7yb8w3UslkXN6Z%2BSa0bNcUgmp7tgbZG4M9RrbQIbCYKY%2FG%2BKuQKGIpHDsXL%2BnQJHNIn4h6F5ZEEuSr6Yaw%2FhFjM9XJG5c0IfNENiXwP%2Fnn%2FwL8siYEehbG0q1GJ2sudlHOQYt%2BR7xmnsWE6I7%2FBZAvFL0GGDnBHz4Xq4DyI9ldFjMBtlBwwrdaT8ktZQ7GM%2FBeKbVria7lY1N4b9TmLGp3MfHZijzDjJHD2yCFDb8RaKXouwuE4FnwPZeyFgB1FthRf4OiB%2BgMvRJpISGW1Vr56ylh6agbFSN3r6T3RcYJojS1k971rv80NWNpNdoy0fjd1Ez6GmejOLG%2B4q5wginYrbTPuteCRHTp6k7JTvyrjDZPcK9pby%2BE5F%2BWnTIELTIXpDuXBh5gwZhxX1rkqFBfMalWbLakH95TyNwxXL366KNay%2Fhef5MptlpnSQ%2F3Tx8olBhzP9l%2F2woh7xXJLaUgEvoFtppG9nUEfuEtgeV5vk%2FkiXyl3qXGzuJ%2BlYOp3LWWlM5nOgUmuJKjcMkmjEhw34A183fphZCkVHJHELDDeeSMt1AUPseTDjfiQOUhN7GbTFcna7Wk4S1ZVDDI5tG8BjqaAbNdO8wRsyHN4uTm0KtuUR06Z%2FE1hsaSYkjPOjxnYhJlUcv7OoW3po7Mp0xwOZsT3u85Lpw8YGommICXQmFZM776ysWLWiBpWSNMLTAFurmMr2bha%2BviBx0uv6rrfzLP7b9A77AxqejCNehFDj%2B2XoBkVfW49f8T17WeClA%2BGFGSJ%2Bp3G7BQmHAeQJLBsPYjhWhbdG2hCFBwW8o%3D&X-Amz-Signature=d53d7e421722eeded350c13cf5fad391253402cb3197b179fffdf006acb30e56 [following]\n--2025-01-25 05:41:49--  https://binstar-cio-packages-prod.s3.amazonaws.com/5638023e9c73330b8ae83e9a/5eff83c9fe6b2a0a2b15391a?response-content-disposition=attachment%3B%20filename%3D%22python-3.8.3-ha7b6439_2.tar.bz2%22%3B%20filename%2A%3DUTF-8%27%27python-3.8.3-ha7b6439_2.tar.bz2&response-content-type=application%2Fx-tar&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAWUI46DZFJN2JEEA2%2F20250125%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250125T054149Z&X-Amz-Expires=600&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEB4aCXVzLWVhc3QtMSJHMEUCIQDT5ASLJH6ZA3CCKL83r3Eth4hwgEYGviNPGcfuYMC5bQIgTiBKjMzf8qbO%2FZqmTCq2E1desp%2FLh9aUqGKYN8PpK3gq%2FgQIJhAAGgw0NTU4NjQwOTgzNzgiDGqYJEF8mlRB4JClnCrbBB6X1gbYscBqWo99ZFXpmsf%2FescZrKcbLF4EwdnCX4c5XIgg9Pbt1R0GvQP0mgZfg%2FnVbZcb9mIlH7zI2aQ788VTQt85QAv%2BGlDKIJxJqoukP2L1n8c07ME%2BDqJVDXWc5h7v1A2PUYoTkri5ZlWNm0dh2s7Y1JDgSHjDRUjn3q3UXXo5QZ4OPk4y4lkxfBshRxyUjX2tdmlG0DQJ2TEKRc7yb8w3UslkXN6Z%2BSa0bNcUgmp7tgbZG4M9RrbQIbCYKY%2FG%2BKuQKGIpHDsXL%2BnQJHNIn4h6F5ZEEuSr6Yaw%2FhFjM9XJG5c0IfNENiXwP%2Fnn%2FwL8siYEehbG0q1GJ2sudlHOQYt%2BR7xmnsWE6I7%2FBZAvFL0GGDnBHz4Xq4DyI9ldFjMBtlBwwrdaT8ktZQ7GM%2FBeKbVria7lY1N4b9TmLGp3MfHZijzDjJHD2yCFDb8RaKXouwuE4FnwPZeyFgB1FthRf4OiB%2BgMvRJpISGW1Vr56ylh6agbFSN3r6T3RcYJojS1k971rv80NWNpNdoy0fjd1Ez6GmejOLG%2B4q5wginYrbTPuteCRHTp6k7JTvyrjDZPcK9pby%2BE5F%2BWnTIELTIXpDuXBh5gwZhxX1rkqFBfMalWbLakH95TyNwxXL366KNay%2Fhef5MptlpnSQ%2F3Tx8olBhzP9l%2F2woh7xXJLaUgEvoFtppG9nUEfuEtgeV5vk%2FkiXyl3qXGzuJ%2BlYOp3LWWlM5nOgUmuJKjcMkmjEhw34A183fphZCkVHJHELDDeeSMt1AUPseTDjfiQOUhN7GbTFcna7Wk4S1ZVDDI5tG8BjqaAbNdO8wRsyHN4uTm0KtuUR06Z%2FE1hsaSYkjPOjxnYhJlUcv7OoW3po7Mp0xwOZsT3u85Lpw8YGommICXQmFZM776ysWLWiBpWSNMLTAFurmMr2bha%2BviBx0uv6rrfzLP7b9A77AxqejCNehFDj%2B2XoBkVfW49f8T17WeClA%2BGFGSJ%2Bp3G7BQmHAeQJLBsPYjhWhbdG2hCFBwW8o%3D&X-Amz-Signature=d53d7e421722eeded350c13cf5fad391253402cb3197b179fffdf006acb30e56\nResolving binstar-cio-packages-prod.s3.amazonaws.com (binstar-cio-packages-prod.s3.amazonaws.com)... 3.5.29.227, 16.182.36.177, 3.5.25.202, ...\nConnecting to binstar-cio-packages-prod.s3.amazonaws.com (binstar-cio-packages-prod.s3.amazonaws.com)|3.5.29.227|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 60684286 (58M) [application/x-tar]\nSaving to: ‘python-3.8.3.tar.bz2’\n\npython-3.8.3.tar.bz 100%[===================>]  57.87M  34.9MB/s    in 1.7s    \n\n2025-01-25 05:41:51 (34.9 MB/s) - ‘python-3.8.3.tar.bz2’ saved [60684286/60684286]\n\nsudo: conda: command not found\nsudo: conda: command not found\nsudo: conda: command not found\nPython 3.10.12\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!export PATH=/kaggle/working/miniconda/bin:$PATH\n!which python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T05:41:52.271215Z","iopub.execute_input":"2025-01-25T05:41:52.271651Z","iopub.status.idle":"2025-01-25T05:41:52.515523Z","shell.execute_reply.started":"2025-01-25T05:41:52.271592Z","shell.execute_reply":"2025-01-25T05:41:52.514116Z"}},"outputs":[{"name":"stdout","text":"/usr/local/bin/python\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Degrade Python to Python3.8","metadata":{}},{"cell_type":"code","source":"# !sudo apt-get update && sudo apt-get install -y python3.8 python3.8 python3.8-venv\n# !sudo rm /usr/bin/python3\n# !sudo ln -s /usr/bin/python3.8 /usr/bin/python3\n# !sudo ln -sf /usr/bin/python3.8 /usr/bin/python\n\n# !python --version\n# !python3 --version","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-21T14:33:06.925Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Install Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install absl-py==1.1.0 aiohttp==3.8.1 aiosignal==1.2.0 async-timeout==4.0.2 attrs==21.4.0 cachetools==5.2.0 certifi==2022.6.15 charset-normalizer==2.1.0 cycler==0.11.0 filelock==3.7.1 fonttools==4.34.4 frozenlist==1.3.0 fsspec==2022.5.0 google-auth==2.9.0 google-auth-oauthlib==0.4.6 grpcio==1.47.0 huggingface-hub==0.8.1 HyperPyYAML==1.0.1 idna==3.3 importlib-metadata==4.12.0 joblib==1.2.0 kiwisolver==1.4.3 llvmlite==0.38.1 Markdown==3.3.7 matplotlib==3.5.2 multidict==6.0.2 numba==0.55.2 numpy==1.22.4 oauthlib==3.2.1 packaging==21.3 pandas==1.4.3 Pillow==9.2.0 protobuf==3.19.5 pyasn1==0.4.8 pyasn1-modules==0.2.8 pyDeprecate==0.3.2 pyparsing==3.0.9  python-speech-features==0.6 pytorch-lightning==1.6.4 pytz==2022.1 ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-21T14:33:06.928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install PyYAML==6.0 requests==2.28.1 resampy==0.3.0 requests-oauthlib==1.3.1 resampy==0.3.0 rsa==4.8 ruamel.yaml==0.17.21 ruamel.yaml.clib==0.2.6 scikit-learn==1.1.1 scipy==1.8.1 seaborn==0.12.0 sentencepiece==0.1.96 six==1.16.0 sklearn==0.0 speechbrain==0.5.12 tensorboard==2.9.1 tensorboard-data-server==0.6.1 tensorboard-plugin-wit==1.8.1 threadpoolctl==3.1.0 torch==1.11.0 torch-tb-profiler==0.4.0 torchaudio==0.11.0 torchmetrics==0.9.1 torchvision==0.12.0 tqdm==4.64.0 typing-extensions==4.2.0 urllib3==1.26.9 Werkzeug==2.1.2 yarl==1.7.2 zipp==3.8.0","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-21T14:33:06.934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install speechbrain","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-21T14:33:06.936Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Code","metadata":{}},{"cell_type":"code","source":"import os\n\n# Datasets imports\nimport glob\nimport random\nimport resampy\nfrom scipy.io import wavfile\nfrom scipy.signal import fftconvolve\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset\nfrom python_speech_features import mfcc\n\n# Plda_classifier imports\nimport pickle\nfrom speechbrain.processing.PLDA_LDA import *\n\n# Plda_score_stat imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.manifold import TSNE\nfrom speechbrain.utils.metric_stats import EER, minDCF\n\n# Main imports\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.tensorboard\nimport torchmetrics\nfrom pytorch_lightning import loggers as pl_loggers\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom torch.utils.data import DataLoader\n\n# Tdnn_layer\nclass TdnnLayer(nn.Module):\n    def __init__(self, input_size=24, output_size=512, context=[0], batch_norm=True, dropout_p=0.0):\n        \"\"\"\n        TDNN as defined by https://www.danielpovey.com/files/2015_interspeech_multisplice.pdf\n        Structure inspired by https://github.com/cvqluu/TDNN/blob/master/tdnn.py\n        \"\"\"\n        super(TdnnLayer, self).__init__()\n\n        self.input_size = input_size\n        self.output_size = output_size\n        self.context = context\n        self.batch_norm = batch_norm\n        self.dropout_p = dropout_p\n\n        self.linear = nn.Linear(input_size*len(context), output_size)\n        self.relu = nn.ReLU()\n        if(self.batch_norm):\n            self.norm = nn.BatchNorm1d(output_size)\n        if(self.dropout_p):\n            self.drop = nn.Dropout(p=self.dropout_p)\n\n    def forward(self, x):\n\n        x_context = get_time_context(x, self.context)\n        x = torch.cat(x_context, 2)\n        x = self.linear(x)\n        x = self.relu(x)\n        \n        if(self.dropout_p):\n            x = self.drop(x)\n\n        if(self.batch_norm):\n            x = x.transpose(1,2)\n            x = self.norm(x)\n            x = x.transpose(1,2)\n\n        return x\n\ndef get_time_context(x, c=[0]):\n    \"\"\"\n    Returns x with the applied time context. For this the surrounding time frames are concatenated together.\n    For example an input of shape (100, 10) with context [-1,0,1] would become (98,30).\n    Visual example:\n    x=          c=          result=\n    [[1,2],                 \n    [3,4],                  [[1,2,3,4,5,6],\n    [5,6],      [-1,0,1]    [3,4,5,6,7,8],\n    [7,8],                  [5,6,7,8,9,0]]\n    [9,0]]                  \n    \"\"\"\n    l = len(c) - 1\n    xc =   [x[:, c[l]+cc:c[0]+cc, :]\n            if cc!=c[l] else\n            x[:, c[l]+cc:, :]\n            for cc in c]\n    return xc\n\n# Plda_classifier\ndef get_train_x_vec(train_xv, train_label, x_id_train):\n    \"\"\"\n    Generate a stat object for the training x-vectors.\n\n    Parameters\n    ----------\n    train_xv: ndarray\n        The x-vector\n        \n    train_label: int\n        The x-vectors label\n        \n    x_id_train: string\n        The x-vectors unique id\n\n    Returns\n    -------\n    xvectors_stat: obj\n        The x-vector stat object\n    \"\"\"\n    # Get number of train_utterances and their dimension\n    N = train_xv.shape[0]\n    print('N train utt:', N)\n\n    # Define arrays neccessary for special stat object\n    md = ['id'+str(train_label[i]) for i in range(N)]\n    modelset = np.array(md, dtype=\"|O\")\n    sg = [str(x_id_train[i]) for i in range(N)]\n    segset = np.array(sg, dtype=\"|O\")\n    s = np.array([None] * N)\n    stat0 = np.array([[1.0]]* N)\n\n    # Define special stat object\n    xvectors_stat = StatObject_SB(modelset=modelset, segset=segset, start=s, stop=s, stat0=stat0, stat1=train_xv)\n    return xvectors_stat\n\ndef setup_plda(mean=None, F=None, Sigma=None, rank_f=150, nb_iter=10, scaling_factor=1):\n    plda = PLDA(mean=mean, F=F, Sigma=Sigma, rank_f=rank_f, nb_iter=nb_iter, scaling_factor=scaling_factor)\n    return plda\n\ndef train_plda(plda, xvectors_stat):\n    plda.plda(xvectors_stat)\n    return plda\n\ndef get_x_vec_stat(xv, id):\n    \"\"\"\n    Generate a stat object for the x-vectors.\n\n    Parameters\n    ----------\n    xv: ndarray\n        The x-vector\n        \n    id: int\n        The x-vectors unique id\n\n    Returns\n    -------\n    xv_stat: obj\n        The x-vector stat object\n    \"\"\"\n    # Get number of utterances and their dimension\n    N = xv.shape[0]\n\n    # Define arrays neccessary for special stat object\n    sgs = [str(id[i]) for i in range(N)]\n    sets = np.array(sgs, dtype=\"|O\")\n    s = np.array([None] * N)\n    stat0 = np.array([[1.0]]* N)\n\n    # Define special stat object\n    xv_stat = StatObject_SB(modelset=sets, segset=sets, start=s, stop=s, stat0=stat0, stat1=xv)\n    return xv_stat\n\ndef plda_scores(plda, en_stat, te_stat):\n    # Define special object for plda scoring\n    ndx = Ndx(models=en_stat.modelset, testsegs=te_stat.modelset)\n\n    # PLDA Scoring\n    fast_plda_scores = fast_PLDA_scoring(en_stat, te_stat, ndx, plda.mean, plda.F, plda.Sigma, p_known=0.0)\n    return fast_plda_scores\n\ndef save_plda(plda, file_name):\n    try:\n        with open('plda/'+file_name+'.pickle', 'wb') as f:\n            pickle.dump(plda, f, protocol=pickle.HIGHEST_PROTOCOL)\n    except Exception as ex:\n        print('Error during pickling plda: ', ex)\n\ndef load_plda(file_path_name):\n    try:\n        with open(file_path_name, 'rb') as f:\n            return pickle.load(f)\n    except Exception as ex:\n        print('Error during pickling plda: ', ex)\n\ndef lda(x_vec_stat, reduced_dim=2):\n    lda = LDA()\n    new_train_obj = lda.do_lda(x_vec_stat, reduced_dim=reduced_dim)\n    return new_train_obj\n\n\n# Plda_score_stat\nclass plda_score_stat_object():\n    def __init__(self, x_vectors_test):\n        self.x_vectors_test = x_vectors_test\n        self.x_id_test = np.array(self.x_vectors_test.iloc[:, 1])\n        self.x_vec_test = np.array([np.array(x_vec[1:-1].split(), dtype=np.float64) for x_vec in self.x_vectors_test.iloc[:, 3]])\n\n        self.en_stat = get_x_vec_stat(self.x_vec_test, self.x_id_test)\n        self.te_stat = get_x_vec_stat(self.x_vec_test, self.x_id_test)\n\n        self.plda_scores = 0\n        self.positive_scores = []\n        self.negative_scores = []\n        self.positive_scores_mask = []\n        self.negative_scores_mask = []\n\n        self.eer = 0\n        self.eer_th = 0\n        self.min_dcf = 0\n        self.min_dcf_th = 0\n\n        self.checked_xvec = []\n        self.checked_label = []\n\n    def test_plda(self, plda, veri_test_file_path):\n        \"\"\"\n        Tests the PLDA performance based on the VoxCeleb veri test files speaker pairings.\n\n        Parameters\n        ----------\n        PLDA: obj\n            The PLDA getting tested\n            \n        veri_test_file_path: string\n            The path to the VoxCeleb veri test file\n\n        Returns\n        -------\n        sample: tensor\n            The MFCC of the desires sample\n        \n        label: string\n            The label of the sample\n\n        id: string\n            The scource directory of the sample (unique for each seperate sample)\n        \"\"\"\n        self.plda_scores = plda_scores(plda, self.en_stat, self.te_stat)\n        self.positive_scores_mask = np.zeros_like(self.plda_scores.scoremat)\n        self.negative_scores_mask = np.zeros_like(self.plda_scores.scoremat)\n        \n        checked_list = []\n        for pair in open(veri_test_file_path):\n            is_match = bool(int(pair.split(\" \")[0].rstrip().split(\".\")[0].strip()))\n            enrol_id = pair.split(\" \")[1].strip()\n            test_id = pair.split(\" \")[2].strip()\n\n            i = int(np.where(self.plda_scores.modelset == enrol_id)[0][0])\n            if(not enrol_id in checked_list):\n                checked_list.append(enrol_id)\n                self.checked_xvec.append(np.array(self.x_vectors_test.loc[self.x_vectors_test['id'] == enrol_id, 'xvector'].item()[1:-1].split(), dtype=np.float64))\n                self.checked_label.append(int(enrol_id.split(\".\")[0].split(\"/\")[0][2:]))\n                \n            j = int(np.where(self.plda_scores.segset == test_id)[0][0])\n            if(not test_id in checked_list):\n                checked_list.append(test_id)\n                self.checked_xvec.append(np.array(self.x_vectors_test.loc[self.x_vectors_test['id'] == test_id, 'xvector'].item()[1:-1].split(), dtype=np.float64))\n                self.checked_label.append(int(test_id.split(\".\")[0].split(\"/\")[0][2:]))\n\n            current_score = float(self.plda_scores.scoremat[i,j])\n            if(is_match):\n                self.positive_scores.append(current_score)\n                self.positive_scores_mask[i,j] = 1\n            else:\n                self.negative_scores.append(current_score)\n                self.negative_scores_mask[i,j] = 1\n                    \n        self.checked_xvec = np.array(self.checked_xvec)\n        self.checked_label = np.array(self.checked_label)\n\n    def calc_eer_mindcf(self):\n        \"\"\"\n        Calculate the EER and minDCF.\n        \"\"\"\n        self.eer, self.eer_th = EER(torch.tensor(self.positive_scores), torch.tensor(self.negative_scores))\n        self.min_dcf, self.min_dcf_th = minDCF(torch.tensor(self.positive_scores), torch.tensor(self.negative_scores), p_target=0.5)\n\n    def plot_images(self, writer):\n        \"\"\"\n        Plot images for the given writer.\n\n        Parameters\n        ----------\n        writer: the writer the images are plotted for\n        \"\"\"\n        split_xvec = []\n        split_label = []\n        group_kfold = sklearn.model_selection.GroupKFold(n_splits=2)\n        groups1234 = np.where(self.checked_label<10290, 0, 1)\n        for g12, g34 in group_kfold.split(self.checked_xvec, self.checked_label, groups1234):\n            x12, x34 = self.checked_xvec[g12], self.checked_xvec[g34]\n            y12, y34 = self.checked_label[g12], self.checked_label[g34]\n            groups12 = np.where(y12<10280, 0, 1)\n            groups34 = np.where(y34<10300, 0, 1)\n            for g1, g2 in group_kfold.split(x12, y12, groups12):\n                split_xvec.append(x12[g1])\n                split_xvec.append(x12[g2])\n                split_label.append(y12[g1])\n                split_label.append(y12[g2])\n                break\n            for g3, g4 in group_kfold.split(x34, y34, groups34):\n                split_xvec.append(x34[g3])\n                split_xvec.append(x34[g4])\n                split_label.append(y34[g3])\n                split_label.append(y34[g4])\n                break\n            break\n        split_xvec = np.array(split_xvec)\n        split_label = np.array(split_label)\n\n        print('generating images for tensorboard')\n        scoremat_norm = np.array(self.plda_scores.scoremat)\n        scoremat_norm -= np.min(scoremat_norm)\n        scoremat_norm /= np.max(scoremat_norm)\n\n        print('score_matrix')\n        img = np.zeros((3, scoremat_norm.shape[0], scoremat_norm.shape[1]))\n        img[0] = np.array([scoremat_norm])\n        img[1] = np.array([scoremat_norm])\n        img[2] = np.array([scoremat_norm])\n        writer.add_image('score_matrix', img, 0)\n\n        print('ground_truth')\n        img = np.zeros((3, scoremat_norm.shape[0], scoremat_norm.shape[1]))\n        img[1] = np.array([self.positive_scores_mask])\n        img[0] = np.array([self.negative_scores_mask])\n        writer.add_image('ground_truth', img, 0)\n\n        print('ground_truth_scores')\n        img = np.zeros((3, scoremat_norm.shape[0], scoremat_norm.shape[1]))\n        img[1] = np.array([scoremat_norm*self.positive_scores_mask])\n        img[0] = np.array([scoremat_norm*self.negative_scores_mask])\n        writer.add_image('ground_truth_scores', img, 0)\n        \n        checked_values_map = self.positive_scores_mask + self.negative_scores_mask\n        checked_values = checked_values_map * self.plda_scores.scoremat\n\n        eer_prediction_positive = np.where(checked_values >= self.eer_th, 1, 0) * checked_values_map\n        eer_prediction_negative = np.where(checked_values < self.eer_th, 1, 0) * checked_values_map\n        min_dcf_prediction_positive = np.where(checked_values >= self.min_dcf_th, 1, 0) * checked_values_map\n        min_dcf_prediction_negative = np.where(checked_values < self.min_dcf_th, 1, 0) * checked_values_map\n    \n        print('prediction_eer_min_dcf')\n        img = np.ones((3, scoremat_norm.shape[0], scoremat_norm.shape[1]*2+5))\n        img[1,:,:checked_values.shape[1]] = eer_prediction_positive\n        img[0,:,:checked_values.shape[1]] = eer_prediction_negative\n        img[1,:,-checked_values.shape[1]:] = min_dcf_prediction_positive\n        img[0,:,-checked_values.shape[1]:] = min_dcf_prediction_negative\n        img[2,:,:checked_values.shape[1]] = 0\n        img[2,:,-checked_values.shape[1]:] = 0\n        writer.add_image('prediction_eer_min_dcf', img, 0)\n    \n        print('correct_prediction_eer_min_dcf')\n        img = np.ones((3, scoremat_norm.shape[0], scoremat_norm.shape[1]*2+5))\n        img[1,:,:checked_values.shape[1]] = eer_prediction_positive * self.positive_scores_mask\n        img[0,:,:checked_values.shape[1]] = eer_prediction_negative * self.negative_scores_mask\n        img[1,:,-checked_values.shape[1]:] = min_dcf_prediction_positive * self.positive_scores_mask\n        img[0,:,-checked_values.shape[1]:] = min_dcf_prediction_negative * self.negative_scores_mask\n        img[2,:,:checked_values.shape[1]] = 0\n        img[2,:,-checked_values.shape[1]:] = 0\n        writer.add_image('correct_prediction_eer_min_dcf', img, 0)\n    \n        print('false_prediction_eer_min_dcf')\n        img = np.ones((3, scoremat_norm.shape[0], scoremat_norm.shape[1]*2+5))\n        img[1,:,:checked_values.shape[1]] = eer_prediction_positive * self.negative_scores_mask\n        img[0,:,:checked_values.shape[1]] = eer_prediction_negative * self.positive_scores_mask\n        img[1,:,-checked_values.shape[1]:] = min_dcf_prediction_positive * self.negative_scores_mask\n        img[0,:,-checked_values.shape[1]:] = min_dcf_prediction_negative * self.positive_scores_mask\n        img[2,:,:checked_values.shape[1]] = 0\n        img[2,:,-checked_values.shape[1]:] = 0\n        writer.add_image('false_prediction_eer_min_dcf', img, 0)\n\n        def generate_scatter_plot(x, y, label, plot_name):\n            df = pd.DataFrame({'x': x, 'y': y, 'label': label})\n            fig, ax = plt.subplots(1)\n            fig.set_size_inches(16, 12)\n            sns.scatterplot(x='x', y='y', hue='label', palette='bright', data=df, ax=ax, s=80) #use sns.color_palette(\"hls\", 40) for 40 speakers\n            limx = (x.min()-5, x.max()+5)\n            limy = (y.min()-5, y.max()+5)\n            ax.set_xlim(limx)\n            ax.set_ylim(limy)\n            ax.set_aspect(1.0/ax.get_data_ratio(), adjustable='box')\n            ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n            ax.title.set_text(plot_name)\n        \n        for i, (checked_xvec, checked_label) in enumerate(zip(split_xvec, split_label)):\n            print('scatter_plot_LDA'+str(i+1))\n            new_stat = get_x_vec_stat(checked_xvec, checked_label)\n            new_stat = lda(new_stat)\n            generate_scatter_plot(new_stat.stat1[:, 0], new_stat.stat1[:, 1], checked_label, 'scatter_plot_LDA'+str(i+1))\n            writer.add_figure('scatter_plot_LDA'+str(i+1), plt.gcf())\n\n            print('scatter_plot_PCA'+str(i+1))\n            pca = sklearn.decomposition.PCA(n_components=2)\n            pca_result = pca.fit_transform(sklearn.preprocessing.StandardScaler().fit_transform(checked_xvec))\n            generate_scatter_plot(pca_result[:,0], pca_result[:,1], checked_label, 'scatter_plot_PCA'+str(i+1))\n            writer.add_figure('scatter_plot_PCA'+str(i+1), plt.gcf())\n\n            print('scatter_plot_TSNE'+str(i+1))\n            tsne = TSNE(2)\n            tsne_result = tsne.fit_transform(checked_xvec)\n            generate_scatter_plot(tsne_result[:,0], tsne_result[:,1], checked_label, 'scatter_plot_TSNE'+str(i+1))\n            writer.add_figure('scatter_plot_TSNE'+str(i+1), plt.gcf())\n\n\n# Dataset\nEPS = 1e-20\nclass Dataset(Dataset):\n    def __init__(self,\n                sampling_rate=16000,\n                mfcc_numcep=24,\n                mfcc_nfilt=26,\n                mfcc_nfft=512,\n                data_folder_path='data',\n                augmentations_per_sample=2):\n        self.samples = []\n        self.labels = []\n\n        self.n_samples = 0\n        self.unique_labels = []\n\n        self.train_samples = []\n        self.train_labels = []\n        self.val_samples = []\n        self.val_labels = []\n        self.test_samples = []\n        self.test_labels = []\n\n        self.data_folder_path = data_folder_path\n        self.sampling_rate = sampling_rate\n        self.augmentations_per_sample = augmentations_per_sample\n        self.mfcc_numcep = mfcc_numcep\n        self.mfcc_nfilt = mfcc_nfilt\n        self.mfcc_nfft = mfcc_nfft\n\n    def init_samples_and_labels(self):\n        \"\"\"\n        This method initalizes the dataset by collectiong all available train and test data samples.\n        The train samples are randomly split into 90% train and 10% validation.\n        Even though all samples are collected only the currently active ones will be returned with get.\n        To set which samples are currently active call load_data(self, train=False, val=False, test=False)\n        and set the wanted samples to true\n        \"\"\"\n        vox_train_path = self.data_folder_path + '/VoxCeleb/vox1_dev_wav/*/*/*.wav'\n        vox_test_path = self.data_folder_path + '/VoxCeleb/vox1_test_wav/*/*/*.wav'\n\n        # Get the paths to all train and val data samples\n        globs = glob.glob(vox_train_path)\n        print('collectiong training and validation samples')\n        \n        # Gat the list of samples and labels\n        samples = [(sample, 'none') for sample in globs]\n        labels = [os.path.basename(os.path.dirname(os.path.dirname(f))) for f in globs]\n        for i in range(self.augmentations_per_sample):\n            samples = samples + [(sample, random.choice(['music', 'speech', 'noise', 'rir'])) for sample in globs]\n            labels = labels + [os.path.basename(os.path.dirname(os.path.dirname(f))) for f in globs]\n\n        unique_labels = np.unique(labels)\n        print('found:')\n        print(len(unique_labels), ' unique speakers')\n        print(int(len(samples)/(self.augmentations_per_sample+1)), ' voice samples')\n        print(len(samples), ' total voice samples including augmentations')\n        print('splitting into 90% training and 10% validation')\n\n        skf = StratifiedKFold(n_splits=10, shuffle=True)\n        train_index, val_index = [], []\n        for traini, vali in skf.split(samples, labels):\n            if(len(vali) == int(round(len(samples)/10))):\n                train_index = traini\n                val_index = vali\n        if(len(train_index) <= 1):\n            print('StratifiedKFold Failed')\n        \n        self.train_samples = list(np.array(samples)[train_index])\n        self.train_labels = list(np.array(labels)[train_index])\n        self.val_samples = list(np.array(samples)[val_index])\n        self.val_labels = list(np.array(labels)[val_index])\n            \n        # Get the paths to all test data samples\n        globs = glob.glob(vox_test_path)\n        print('collectiong test samples')\n        \n        # Gat the list of samples and labels\n        test_samples = [(sample, 'none') for sample in globs]\n        test_labels = [os.path.basename(os.path.dirname(os.path.dirname(f))) for f in globs]\n            \n        unique_labels = np.unique(test_labels)\n        print('found:')\n        print(len(unique_labels), ' unique speakers')\n        print(len(test_samples), ' voice samples')\n        print('DONE collectiong samples')\n\n        self.test_samples = list(np.array(test_samples))\n        self.test_labels = list(np.array(test_labels))\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns the MFCC of the sample and class at the given index.\n\n        Parameters\n        ----------\n        index: int\n            The index of the desired sample\n\n        Returns\n        -------\n        sample: tensor\n            The MFCC of the desires sample\n        \n        label: string\n            The label of the sample\n\n        id: string\n            The scource directory of the sample (unique for each seperate sample)\n        \"\"\"\n        sample_path, augmentation = self.samples[index]\n        rate, sample = wavfile.read(sample_path, np.dtype)\n        sample = resampy.resample(sample, rate, self.sampling_rate)\n\n        # Augment the sample with noise and/or reverbaraition\n        augmented_sample = self.augment_data(sample, augmentation)\n        augmented_sample = mfcc(augmented_sample, self.sampling_rate, numcep=self.mfcc_numcep, nfilt=self.mfcc_nfilt, nfft=self.mfcc_nfft)\n\n        label = self.unique_labels.index(self.labels[index])\n        id = '/'.join(sample_path.rsplit('/')[-3:])\n        \n        return torch.from_numpy(augmented_sample), label, id\n\n    def __len__(self):\n        \"\"\"\n        Returns the number of active samples.\n\n        Returns\n        -------\n        n_samples: int\n            The number of active samples\n        \"\"\"\n        return self.n_samples\n\n    def load_data(self, train=False, val=False, test=False):\n        \"\"\"\n        Loads the specified data to be active.\n\n        Parameters\n        ----------\n        train: bool\n            Set if the train samples are supposed to be active\n            Default = False\n            \n        val: bool\n            Set if the train samples are supposed to be active\n            Default = False\n            \n        test: bool\n            Set if the train samples are supposed to be active\n            Default = False\n        \"\"\"\n        self.samples = []\n        self.labels = []\n        self.n_samples = 0\n        self.unique_labels = []\n\n        if(train):\n            self.samples = self.samples + self.train_samples\n            self.labels = self.labels + self.train_labels\n        if(val):\n            self.samples = self.samples + self.val_samples\n            self.labels = self.labels + self.val_labels\n        if(test):\n            self.samples = self.samples + self.test_samples\n            self.labels = self.labels + self.test_labels\n\n        # Get the num of samples and the unique class names\n        self.n_samples = len(self.samples)\n        self.unique_labels = list(np.unique(self.labels))\n\n    def augment_data(self, sample, augmentation):\n        \"\"\"\n        Augment the normalized data sample with a given augmentation.\n        If the augmentation is not one of the accepted types returns the unaugmented sample.\n\n        Parameters\n        ----------\n        sample: ndarray\n            The sample that is supposed to be augmented\n            \n        augmentation: string\n            the type of augmentation\n            can be: {'music', 'speech', 'noise', 'rir'}\n\n        Returns\n        -------\n        aug_sample: ndarray\n            The normalized augmented sample\n        \"\"\"\n        sample = self.cut_to_sec(sample, 3)\n\n        if(augmentation == 'music'):\n            aug_sample = self.augment_musan_music(sample)\n        elif(augmentation == 'speech'):\n            aug_sample = self.augment_musan_speech(sample)\n        elif(augmentation == 'noise'):\n            aug_sample = self.augment_musan_noise(sample)\n        elif(augmentation == 'rir'):\n            aug_sample = self.augment_rir(sample)\n        else:\n            aug_sample = sample\n\n        aug_sample = aug_sample.astype(np.float64)\n        aug_sample -= np.min(aug_sample)\n        aug_sample /= np.max(aug_sample)\n        return aug_sample\n\n    def cut_to_sec(self, sample, length):\n        \"\"\"\n        Cuts or pads the sample to a certain length.\n\n        Parameters\n        ----------\n        sample: ndarray\n            The sample that is supposed to be cut\n            \n        length: int\n            The lenght in seconds the returned sample should have\n\n        Returns\n        -------\n        new_sample: ndarray\n            The sample with the specefied lenght\n        \"\"\"\n        if(len(sample) < self.sampling_rate*length):\n            new_sample = np.pad(sample, (0, self.sampling_rate*length-len(sample)), 'constant', constant_values=(0, 0))\n        else:\n            start_point = random.randint(0, len(sample) - self.sampling_rate*length)\n            new_sample = sample[start_point:start_point + self.sampling_rate*length]\n        return new_sample\n\n    def add_with_certain_snr(self, sample, noise, min_snr_db=5, max_snr_db=20):\n        \"\"\"\n        Adds the noise to the signal with a SNR randomly chosen between min_snr_db and max_snr_db.\n\n        Parameters\n        ----------\n        sample: ndarray\n            The sample that is supposed to be augmented\n            \n        noise: ndarray\n            The noise that is supposed augment the sample\n            \n        min_snr_db: int\n            The minimal SNR in decibil the returned sample should have\n            Default = 5\n            \n        max_snr_db: int\n            The maximal SNR in decibil the returned sample should have\n            Default = 20\n\n        Returns\n        -------\n        noisy_sample: ndarray\n            The sample with the added noise\n        \"\"\"\n        sample = sample.astype('int64')\n        noise = noise.astype('int64')\n\n        sample_rms = np.sqrt(np.mean(sample**2))\n        noise_rms = np.sqrt(np.mean(noise**2))\n        wanted_snr = random.randint(min_snr_db, max_snr_db)\n        wanted_noise_rms = np.sqrt(sample_rms**2 / 10**(wanted_snr/10))\n\n        new_noise = noise * wanted_noise_rms/(noise_rms+EPS)\n        noisy_sample = sample + new_noise\n        return noisy_sample\n\n    def augment_musan_music(self, sample):\n        \"\"\"\n        Applies background music from the musan dataset to the sample.\n\n        Parameters\n        ----------\n        sample: ndarray\n            The sample that is supposed to be augmented\n\n        Returns\n        -------\n        aug_sample: ndarray\n            The sample with the added noise\n        \"\"\"\n        musan_music_path = self.data_folder_path + '/musan/music/*/*.wav'\n        #print('load sample: augmenting with musan music')\n\n        song_path = random.choice(glob.glob(musan_music_path))\n        rate, song = wavfile.read(song_path, np.dtype)\n        song = resampy.resample(song, rate, self.sampling_rate)\n\n        song = self.cut_to_sec(song, 3)\n        aug_sample = self.add_with_certain_snr(sample, song, min_snr_db=5, max_snr_db=15)\n        return aug_sample\n\n    def augment_musan_speech(self, sample):\n        \"\"\"\n        Applies background speech from the musan dataset to the sample.\n        3-7 different speakers are added and used as background speech.\n\n        Parameters\n        ----------\n        sample: ndarray\n            The sample that is supposed to be augmented\n\n        Returns\n        -------\n        aug_sample: ndarray\n            The sample with the added noise\n        \"\"\"\n        musan_speech_path = self.data_folder_path + '/musan/speech/*/*.wav'\n        #print('load sample: augmenting with musan speech')\n\n        speaker_path = random.choice(glob.glob(musan_speech_path))\n        rate, speakers = wavfile.read(speaker_path, np.dtype)\n        speakers = resampy.resample(speakers, rate, self.sampling_rate)\n        speakers = self.cut_to_sec(speakers, 3)\n\n        for i in range(random.randint(2, 6)):\n            speaker_path = random.choice(glob.glob(musan_speech_path))\n            rate, speaker = wavfile.read(speaker_path, np.dtype)\n            speaker = resampy.resample(speaker, rate, self.sampling_rate)\n            speaker = self.cut_to_sec(speaker, 3)\n            speakers = speakers + speaker\n            \n        aug_sample = self.add_with_certain_snr(sample, speakers, min_snr_db=13, max_snr_db=20)\n        return aug_sample\n\n    def augment_musan_noise(self, sample):\n        \"\"\"\n        Applies background noise from the musan dataset to the sample.\n        A 1 sec noise clip is added to the sample at 1 sec intervals.\n\n        Parameters\n        ----------\n        sample: ndarray\n            The sample that is supposed to be augmented\n\n        Returns\n        -------\n        aug_sample: ndarray\n            The sample with the added noise\n        \"\"\"\n        musan_noise_path = self.data_folder_path + '/musan/noise/*/*.wav'\n        #print('load sample: augmenting with musan noise')\n        \n        for i in range(3):\n            noise_path = random.choice(glob.glob(musan_noise_path))\n            rate, noise = wavfile.read(noise_path, np.dtype)\n            noise = resampy.resample(noise, rate, self.sampling_rate)\n            noise = self.cut_to_sec(noise, 1)\n            sample[i:i+self.sampling_rate] = self.add_with_certain_snr(sample[i:i+self.sampling_rate], noise, min_snr_db=0, max_snr_db=15)\n\n        return sample\n\n    def augment_rir(self, sample):\n        \"\"\"\n        Applies reverbaration from the RIR dataset to the sample.\n        The Sample is convolved with a simulated room impulse response.\n\n        Parameters\n        ----------\n        sample: ndarray\n            The sample that is supposed to be augmented\n\n        Returns\n        -------\n        aug_sample: ndarray\n            The sample with the added noise\n        \"\"\"\n        rir_noise_path = self.data_folder_path + '/RIRS_NOISES/simulated_rirs/*/*/*.wav'\n        #print('load sample: augmenting with rir')\n\n        rir_path = random.choice(glob.glob(rir_noise_path))\n        _, rir = wavfile.read(rir_path, np.dtype)\n        aug_sample = fftconvolve(sample, rir)\n        aug_sample = aug_sample / abs(aug_sample).max()\n\n        sample_max = abs(sample).max()\n        aug_max = abs(aug_sample).max()\n        aug_sample = aug_sample * (sample_max/aug_max)\n    \n        aug_sample = sample + aug_sample[:len(sample)]\n        return aug_sample\n\n\n# Config\nclass Config:\n    def __init__(self,\n                batch_size=512,\n                input_size=24,\n                hidden_size=512,\n                num_classes=1211,\n                x_vector_size=512,\n                x_vec_extract_layer=6,\n                learning_rate=0.001,\n                num_epochs=20,\n                batch_norm=True,\n                dropout_p=0.0,\n                augmentations_per_sample=2,\n                plda_rank_f=50,\n                checkpoint_path='none',\n                data_folder_path='data',\n                train_x_vector_model=True,\n                extract_x_vectors=True,\n                train_plda=True,\n                test_plda=True):\n                \n        self.batch_size = batch_size\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.x_vector_size = x_vector_size\n        self.x_vec_extract_layer = x_vec_extract_layer\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n        self.batch_norm = batch_norm\n        self.dropout_p = dropout_p\n        self.augmentations_per_sample = augmentations_per_sample\n        self.plda_rank_f = plda_rank_f\n        self.checkpoint_path = checkpoint_path\n        self.data_folder_path = data_folder_path\n        self.train_x_vector_model = train_x_vector_model\n        self.extract_x_vectors = extract_x_vectors\n        self.train_plda = train_plda\n        self.test_plda = test_plda\n\n# Main XVectorModel\nclass XVectorModel(pl.LightningModule):\n    def __init__(self, input_size=24,\n                hidden_size=512,\n                num_classes=1211,\n                x_vector_size=512,\n                x_vec_extract_layer=6,\n                batch_size=512,\n                learning_rate=0.001,\n                batch_norm=True,\n                dropout_p=0.0,\n                augmentations_per_sample=2,\n                data_folder_path='data'):\n        super().__init__()\n\n        # Set up the TDNN structure including the time context of the TdnnLayer\n        self.time_context_layers = nn.Sequential(\n            TdnnLayer(input_size=input_size, output_size=hidden_size, context=[-2, -1, 0, 1, 2], batch_norm=batch_norm, dropout_p=dropout_p),\n            TdnnLayer(input_size=hidden_size, output_size=hidden_size, context=[-2, 0, 2], batch_norm=batch_norm, dropout_p=dropout_p),\n            TdnnLayer(input_size=hidden_size, output_size=hidden_size, context=[-3, 0, 3], batch_norm=batch_norm, dropout_p=dropout_p),\n            TdnnLayer(input_size=hidden_size, output_size=hidden_size, batch_norm=batch_norm, dropout_p=dropout_p),\n            TdnnLayer(input_size=hidden_size, output_size=1500, batch_norm=batch_norm, dropout_p=dropout_p)\n        )\n        self.segment_layer6 = nn.Linear(3000, x_vector_size)\n        self.segment_layer7 = nn.Linear(x_vector_size, x_vector_size)\n        self.output = nn.Linear(x_vector_size, num_classes)\n\n        self.x_vec_extract_layer = x_vec_extract_layer\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n\n        self.dataset = Dataset(data_folder_path=data_folder_path, augmentations_per_sample=augmentations_per_sample)\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n\n        self.save_hyperparameters()\n\n    # The statistic pooling layer\n    def stat_pool(self, x):\n        mean = torch.mean(x, 1)\n        stand_dev = torch.std(x, 1)\n        out = torch.cat((mean, stand_dev), 1)\n        return out\n        \n    # The standard forward pass through the neural network\n    def forward(self, x):\n        out = self.time_context_layers(x)\n\n        out = self.stat_pool(out)\n\n        out = F.relu(self.segment_layer6(out))\n        out = F.relu(self.segment_layer7(out))\n        \n        out = self.output(out)\n        return out\n\n    # This method is used to generate the x-vectors for the PLDA classifier\n    # It is the same as the usual forward method exept it stops passing the\n    # input through the layers at the specified x_vec_extract_layer\n    # Finally it returns the x-vectors instead of the usual output\n    def extract_x_vec(self, x):\n        out = self.time_context_layers.forward(x)\n\n        out = self.stat_pool(out)\n\n        if(self.x_vec_extract_layer == 6):\n            x_vec = self.segment_layer6.forward(out)\n        elif(self.x_vec_extract_layer == 7):\n            out = F.relu(self.segment_layer6.forward(out))\n            x_vec = self.segment_layer7.forward(out)\n        else:\n            x_vec = self.segment_layer6.forward(out)\n            \n        return x_vec\n\n    # Train the model\n    def training_step(self, batch, batch_index):\n        samples, labels, id = batch\n        outputs = self(samples.float())\n        loss = F.cross_entropy(outputs, labels)\n        return {'loss': loss, 'train_preds': outputs, 'train_labels': labels, 'train_id': id}\n\n    # Log training loss and accuracy with the logger\n    def training_step_end(self, outputs):\n        self.log('train_step_loss', outputs['loss'])\n        accuracy = self.accuracy(outputs['train_preds'], outputs['train_labels'])\n        self.log('train_step_acc', self.accuracy)\n        return {'loss': outputs['loss'], 'acc': accuracy}\n\n    # Create graph and histogram for the logger\n    def training_epoch_end(self, outputs):\n        if(self.current_epoch == 0):\n            sample = torch.rand((1, 299, 24))\n            self.logger.experiment.add_graph(XVectorModel(), sample)\n\n        for name, params in self.named_parameters():\n            self.logger.experiment.add_histogram(name, params, self.current_epoch)\n\n    # Calculate loss of validation data to check if overfitting\n    def validation_step(self, batch, batch_index):\n        samples, labels, id = batch\n        outputs = self(samples.float())\n        loss = F.cross_entropy(outputs, labels)\n        return {'loss': loss, 'val_preds': outputs, 'val_labels': labels, 'val_id': id}\n\n    # Log validation loss and accuracy with the logger\n    def validation_step_end(self, outputs):\n        self.log('val_step_loss', outputs['loss'])\n        accuracy = self.accuracy(outputs['val_preds'], outputs['val_labels'])\n        self.log('val_step_acc', self.accuracy)\n        return {'loss': outputs['loss'], 'acc': accuracy}\n    \n    # The test step here is NOT used as a test step!\n    # Instead it is used to extract the x-vectors\n    def test_step(self, batch, batch_index):\n        samples, labels, id = batch\n        x_vecs = self.extract_x_vec(samples.float())\n        return [(x_vecs, labels, id)]\n\n    # After all x-vectros are generated append them to the predefined list\n    def test_epoch_end(self, test_step_outputs):\n        for batch_output in test_step_outputs:\n            for x_vec, label, id in batch_output:\n                for x, l, i in zip(x_vec, label, id):\n                    x_vector.append((i, int(l.cpu().numpy()), np.array(x.cpu().numpy(), dtype=np.float64)))\n        return test_step_outputs\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n\n    # Load only the training data\n    def train_dataloader(self):\n        self.dataset.load_data(train=True)\n        train_data_loader = DataLoader(dataset=self.dataset, batch_size=self.batch_size, num_workers=4, shuffle=True)\n        return train_data_loader\n\n    # Load only the validation data\n    def val_dataloader(self):\n        self.dataset.load_data(val=True)\n        val_data_loader = DataLoader(dataset=self.dataset, batch_size=self.batch_size, num_workers=4, shuffle=False)\n        return val_data_loader\n\n    # Load either both training and validation or test data for extracting the x-vectors\n    # In 'train' mode extract x-vectors for PLDA training, in 'test' mode for testing PLDA\n    def test_dataloader(self):\n        if(extract_mode == 'train'):\n            self.dataset.load_data(train=True, val=True)\n            test_data_loader = DataLoader(dataset=self.dataset, batch_size=self.batch_size, num_workers=4, shuffle=False)\n        if(extract_mode == 'test'):\n            self.dataset.load_data(test=True)\n            test_data_loader = DataLoader(dataset=self.dataset, batch_size=self.batch_size, num_workers=4, shuffle=False)\n        return test_data_loader\n\n\n# Main execution\nif __name__ == \"__main__\":\n    print('setting up model and trainer parameters')\n    ppth = 'testlogs/lightning_logs/version_7/checkpoints/last.ckpt'\n    ckpt_path = ppth if os.path.exists('testlogs/lightning_logs/version_7/checkpoints/last.ckpt') else 'none'\n    sep1=\"::::::::\\\\::::::::\"\n    print(sep1,ckpt_path,sep1)\n    config = Config(data_folder_path='C:/Users/hp/Documents/GitHub/Speaker-Recognition-x-vectors/data',\n                    checkpoint_path='none',#ppth,\n                    train_x_vector_model = True,\n                    extract_x_vectors = False,\n                    train_plda = False,\n                    test_plda = False,\n                    x_vec_extract_layer=6,\n                    plda_rank_f=25)#TODO delete most of this\n\n    # Define model and trainer\n    tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"testlogs/\")\n    early_stopping_callback = EarlyStopping(monitor=\"val_step_loss\", mode=\"min\")\n    checkpoint_callback = ModelCheckpoint(monitor='val_step_loss', save_top_k=10, save_last=True, verbose=True)\n\n    if(config.checkpoint_path == 'none'):\n        model = XVectorModel(input_size=config.input_size,\n                            hidden_size=config.hidden_size,\n                            num_classes=config.num_classes,\n                            x_vector_size=config.x_vector_size,\n                            x_vec_extract_layer=config.x_vec_extract_layer,\n                            batch_size=config.batch_size,\n                            learning_rate=config.learning_rate,\n                            batch_norm=config.batch_norm,\n                            dropout_p=config.dropout_p,\n                            augmentations_per_sample=config.augmentations_per_sample,\n                            data_folder_path=config.data_folder_path)\n    else:\n        model = XVectorModel.load_from_checkpoint(config.checkpoint_path)\n    model.dataset.init_samples_and_labels()\n\n    trainer = pl.Trainer(callbacks=[early_stopping_callback, checkpoint_callback],\n                        logger=tb_logger,\n                        log_every_n_steps=1,\n                        accelerator='cpu',#TODO delete\n                        # accelerator='gpu', devices=[0],\n                        max_epochs=config.num_epochs)\n                        #small test adjust options: fast_dev_run=True, limit_train_batches=0.0001, limit_val_batches=0.001, limit_test_batches=0.002\n\n\n\n    # Train the x-vector model\n    if(config.train_x_vector_model):\n        print('training x-vector model')\n        if(config.checkpoint_path == 'none'):\n            trainer.fit(model)\n        else:\n            trainer.fit(model, ckpt_path=config.checkpoint_path)\n\n\n\n    # Extract the x-vectors\n    if(config.extract_x_vectors):\n        print('extracting x-vectors')\n        if not os.path.exists('x_vectors'):\n            os.makedirs('x_vectors')\n        # Extract the x-vectors for trainng the PLDA classifier and save to csv\n        x_vector = []\n        extract_mode = 'train'\n        if(config.train_x_vector_model):\n            trainer.test(model)\n            x_vector = pd.DataFrame(x_vector)\n            x_vector.to_csv('x_vectors/x_vector_train_v1_5_l7relu.csv')#TODO set to default name\n        elif(config.checkpoint_path != 'none'):\n            trainer.test(model, ckpt_path=config.checkpoint_path)\n            x_vector = pd.DataFrame(x_vector)\n            x_vector.to_csv('x_vectors/x_vector_train_v1_5_l7relu.csv')#TODO set to default name\n        else:\n            print('could not extract train x-vectors')\n\n        # Extract the x-vectors for testing the PLDA classifier and save to csv\n        x_vector = []\n        extract_mode = 'test'\n        if(config.train_x_vector_model):\n            trainer.test(model)\n            x_vector = pd.DataFrame(x_vector)\n            x_vector.to_csv('x_vectors/x_vector_test_v1_5_l7relu.csv')#TODO set to default name\n        elif(config.checkpoint_path != 'none'):\n            trainer.test(model, ckpt_path=config.checkpoint_path)\n            x_vector = pd.DataFrame(x_vector)\n            x_vector.to_csv('x_vectors/x_vector_test_v1_5_l7relu.csv')#TODO set to default name\n        else:\n            print('could not extract test x-vectors')\n    \n\n\n    if(config.train_plda):\n        print('loading x_vector data')\n        if not os.path.exists('plda'):\n            os.makedirs('plda')\n        # Extract the x-vectors, labels and id from the csv\n        x_vectors_train = pd.read_csv('x_vectors/i_vector_train_v2.csv')#TODO set to default name\n        x_id_train = np.array(x_vectors_train.iloc[:, 1])\n        x_label_train = np.array(x_vectors_train.iloc[:, 2], dtype=int)\n        x_vec_train = np.array([np.array(x_vec[1:-1].split(), dtype=np.float64) for x_vec in x_vectors_train.iloc[:, 3]])\n\n        # Generate x_vec stat objects\n        print('generating x_vec stat objects')\n        tr_stat = get_train_x_vec(x_vec_train, x_label_train, x_id_train)\n        \n        # Train plda\n        print('training plda')\n        plda = setup_plda(rank_f=50, nb_iter=10)\n        plda = train_plda(plda, tr_stat)\n        save_plda(plda, 'plda_ivec_v2_d50')\n        # Train plda\n        print('training plda')\n        plda = setup_plda(rank_f=100, nb_iter=10)\n        plda = train_plda(plda, tr_stat)\n        save_plda(plda, 'plda_ivec_v2_d100')\n        # Train plda\n        print('training plda')\n        plda = setup_plda(rank_f=150, nb_iter=10)\n        plda = train_plda(plda, tr_stat)\n        save_plda(plda, 'plda_ivec_v2_d150')\n        # Train plda\n        print('training plda')\n        plda = setup_plda(rank_f=200, nb_iter=10)\n        plda = train_plda(plda, tr_stat)\n        save_plda(plda, 'plda_ivec_v2_d200')\n\n\n\n    if(config.test_plda):\n        # Extract the x-vectors, labels and id from the csv\n        print('loading x_vector data')\n        x_vectors_test = pd.read_csv('x_vectors/i_vector_test_v2.csv')#TODO set to default name\n        x_vectors_test.columns = ['index', 'id', 'label', 'xvector']\n        score = plda_score_stat_object(x_vectors_test)\n\n        # Test plda\n        print('testing plda')\n        if(not config.train_plda):\n            plda = load_plda('plda/plda_ivec_v2_d200.pickle')#TODO set to default name\n        score.test_plda(plda, config.data_folder_path + '/VoxCeleb/veri_test2.txt')\n\n        # Calculate EER and minDCF\n        print('calculating EER and minDCF')\n        score.calc_eer_mindcf()\n        print('EER: ', score.eer, '   threshold: ', score.eer_th)\n        print('minDCF: ', score.min_dcf, '   threshold: ', score.min_dcf_th)\n\n        # Generate images for tensorboard\n        score.plot_images(tb_logger.experiment)\n\n        save_plda(score, 'plda_score_ivec_v2_d200')#TODO set to default name\n    print('DONE')\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
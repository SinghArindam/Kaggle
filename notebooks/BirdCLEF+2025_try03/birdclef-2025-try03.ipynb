{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"sourceType":"competition"},{"sourceId":11590046,"sourceType":"datasetVersion","datasetId":7267557}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":31.08349,"end_time":"2025-04-27T17:15:37.637222","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-27T17:15:06.553732","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"005e3aed7adb43488abe6f841c519f2e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"411ecba7db2b42c6865aa366838d638b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"4912a57321554fa68073afcff00d7de1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_005e3aed7adb43488abe6f841c519f2e","placeholder":"​","style":"IPY_MODEL_a1f0ea138174472dbefb1b041b3af941","tabbable":null,"tooltip":null,"value":" 0/0 [00:00&lt;?, ?it/s]"}},"576aabf85e4840f6a2586ba5a1b7d857":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_411ecba7db2b42c6865aa366838d638b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ebc41c476ff34c448813e8ca3cd9b4ed","tabbable":null,"tooltip":null,"value":0}},"5bcf20463f704b339e1c61af273ec45c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8617b2382306439cb5097d4535bdf8eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_5bcf20463f704b339e1c61af273ec45c","placeholder":"​","style":"IPY_MODEL_b5f639669c6a4dfba31fc524f4186eff","tabbable":null,"tooltip":null,"value":""}},"947dc1e627ce47ebaffccede21958846":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8617b2382306439cb5097d4535bdf8eb","IPY_MODEL_576aabf85e4840f6a2586ba5a1b7d857","IPY_MODEL_4912a57321554fa68073afcff00d7de1"],"layout":"IPY_MODEL_a146a17c8b0449f3b824c325f25451b6","tabbable":null,"tooltip":null}},"a146a17c8b0449f3b824c325f25451b6":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1f0ea138174472dbefb1b041b3af941":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b5f639669c6a4dfba31fc524f4186eff":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"ebc41c476ff34c448813e8ca3cd9b4ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"c7b1c93a","cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nOnly Submission(LoadLocalTrainModel)\n</b></h1> ","metadata":{"papermill":{"duration":0.005093,"end_time":"2025-04-27T17:15:10.068433","exception":false,"start_time":"2025-04-27T17:15:10.06334","status":"completed"},"tags":[]}},{"id":"65a2d8be","cell_type":"markdown","source":"### **ℹ️INFO**\n* This notebook is an inference notebook.\n* that performed a unique LocalTrain based on the great Train/Inference published by the Kadircan İdrisoğlu.\n    * [PP] https://www.kaggle.com/code/kadircandrisolu/transforming-audio-to-mel-spec-birdclef-25\n    * [TRAIN] https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-train-birdclef-25\n    * [INF] https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-inference-birdclef-25\n\n### **ℹ️2025/04/28 MyLocalTrainResult**\n* trained using FocalLossBCE, which was used in the previous competition, BirdCLEF 2024 8th place solution. The results were good.\n    ```\n    0.9652\n    0.9605\n    0.9607\n    0.9626\n    [OOF]0.9622\n    ```","metadata":{"papermill":{"duration":0.004037,"end_time":"2025-04-27T17:15:10.07708","exception":false,"start_time":"2025-04-27T17:15:10.073043","status":"completed"},"tags":[]}},{"id":"a4978409","cell_type":"code","source":"import os\nimport gc\nimport warnings\nimport logging\nimport time\nimport math\nimport cv2\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\nfrom tqdm.auto import tqdm\nimport torchvision\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.ERROR)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2025-04-27T17:15:10.08732Z","iopub.status.busy":"2025-04-27T17:15:10.086821Z","iopub.status.idle":"2025-04-27T17:15:25.987316Z","shell.execute_reply":"2025-04-27T17:15:25.98606Z"},"papermill":{"duration":15.908015,"end_time":"2025-04-27T17:15:25.989455","exception":false,"start_time":"2025-04-27T17:15:10.08144","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"d426cd98","cell_type":"code","source":"\"\"\" \n    FocalLossBCE Use Example\n\"\"\"\nclass FocalLossBCE(torch.nn.Module):\n    def __init__(\n            self,\n            alpha: float = 0.25,\n            gamma: float = 2,\n            reduction: str = \"mean\",\n            bce_weight: float = 0.6,\n            focal_weight: float = 1.4,\n    ):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        self.bce = torch.nn.BCEWithLogitsLoss(reduction=reduction)\n        self.bce_weight = bce_weight\n        self.focal_weight = focal_weight\n\n    def forward(self, logits, targets):\n        focall_loss = torchvision.ops.focal_loss.sigmoid_focal_loss(\n            inputs=logits,\n            targets=targets,\n            alpha=self.alpha,\n            gamma=self.gamma,\n            reduction=self.reduction,\n        )\n        bce_loss = self.bce(logits, targets)\n        return self.bce_weight * bce_loss + self.focal_weight * focall_loss\n\ndef get_criterion(cfg):\n    return FocalLossBCE()","metadata":{"execution":{"iopub.execute_input":"2025-04-27T17:15:25.999993Z","iopub.status.busy":"2025-04-27T17:15:25.999653Z","iopub.status.idle":"2025-04-27T17:15:26.006715Z","shell.execute_reply":"2025-04-27T17:15:26.005659Z"},"papermill":{"duration":0.014283,"end_time":"2025-04-27T17:15:26.008507","exception":false,"start_time":"2025-04-27T17:15:25.994224","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"a44f8c88","cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nInference Pipeline\n</b></h1> ","metadata":{"papermill":{"duration":0.00395,"end_time":"2025-04-27T17:15:26.016996","exception":false,"start_time":"2025-04-27T17:15:26.013046","status":"completed"},"tags":[]}},{"id":"ade1d8cd","cell_type":"markdown","source":"## **》》》Env**","metadata":{"papermill":{"duration":0.003999,"end_time":"2025-04-27T17:15:26.025247","exception":false,"start_time":"2025-04-27T17:15:26.021248","status":"completed"},"tags":[]}},{"id":"7a3fef1b","cell_type":"code","source":"class CFG:\n    # ------------------------------------------- #\n    # [IMPORTANT]\n    # * Melspectrogram & Audio Params\n    # ------------------------------------------- #\n    N_FFT = 2048\n    HOP_LENGTH = 512\n    N_MELS = 512\n    FMIN = 20\n    FMAX = 16000\n    TARGET_SHAPE = (256, 256)\n    FS = 32000  \n    WINDOW_SIZE = 5\n\n    # ------------------------------------------- #\n    # * Model def\n    # ------------------------------------------- #\n    model_path = '/kaggle/input/pub-bird25-b-422-ppv15-v2-s-focallossbce'\n    model_name = 'tf_efficientnetv2_s.in21k_ft_in1k'\n    use_specific_folds = False\n    folds = [0,1,2,3]\n    in_channels = 1\n    device = 'cpu'  \n\n    # datasets\n    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n    \n    # Inference parameters\n    batch_size = 16\n    use_tta = False  \n    tta_count = 3\n    threshold = 0.5\n\n    # util\n    debug = False\n    debug_count = 3\n\ncfg = CFG()","metadata":{"execution":{"iopub.execute_input":"2025-04-27T17:15:26.035083Z","iopub.status.busy":"2025-04-27T17:15:26.034706Z","iopub.status.idle":"2025-04-27T17:15:26.040584Z","shell.execute_reply":"2025-04-27T17:15:26.039587Z"},"papermill":{"duration":0.012922,"end_time":"2025-04-27T17:15:26.042375","exception":false,"start_time":"2025-04-27T17:15:26.029453","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"298afe0c","cell_type":"code","source":"print(f\"Using device: {cfg.device}\")\nprint(f\"Loading taxonomy data...\")\ntaxonomy_df = pd.read_csv(cfg.taxonomy_csv)\nspecies_ids = taxonomy_df['primary_label'].tolist()\nnum_classes = len(species_ids)\nprint(f\"Number of classes: {num_classes}\")","metadata":{"execution":{"iopub.execute_input":"2025-04-27T17:15:26.053354Z","iopub.status.busy":"2025-04-27T17:15:26.052966Z","iopub.status.idle":"2025-04-27T17:15:26.078381Z","shell.execute_reply":"2025-04-27T17:15:26.077128Z"},"papermill":{"duration":0.033107,"end_time":"2025-04-27T17:15:26.080571","exception":false,"start_time":"2025-04-27T17:15:26.047464","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"6a959c98","cell_type":"markdown","source":"## **》》》Model**","metadata":{"papermill":{"duration":0.004115,"end_time":"2025-04-27T17:15:26.089271","exception":false,"start_time":"2025-04-27T17:15:26.085156","status":"completed"},"tags":[]}},{"id":"7624b5a9","cell_type":"code","source":"class BirdCLEFModel(nn.Module):\n    def __init__(self, cfg, num_classes):\n        super().__init__()\n        self.cfg = cfg\n        \n        self.backbone = timm.create_model(\n            cfg.model_name,\n            pretrained=False,  \n            in_chans=cfg.in_channels,\n            drop_rate=0.0,    \n            drop_path_rate=0.0\n        )\n        \n        backbone_out = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.feat_dim = backbone_out\n        self.classifier = nn.Linear(backbone_out, num_classes)\n        \n    def forward(self, x):\n        features = self.backbone(x)\n        if isinstance(features, dict):\n            features = features['features']\n        if len(features.shape) == 4:\n            features = self.pooling(features)\n            features = features.view(features.size(0), -1)\n\n        logits = self.classifier(features)\n        return logits","metadata":{"execution":{"iopub.execute_input":"2025-04-27T17:15:26.099323Z","iopub.status.busy":"2025-04-27T17:15:26.098928Z","iopub.status.idle":"2025-04-27T17:15:26.105997Z","shell.execute_reply":"2025-04-27T17:15:26.104878Z"},"papermill":{"duration":0.014365,"end_time":"2025-04-27T17:15:26.107984","exception":false,"start_time":"2025-04-27T17:15:26.093619","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"456a30bf","cell_type":"markdown","source":"## **》》》Melspectrogram**","metadata":{"papermill":{"duration":0.004143,"end_time":"2025-04-27T17:15:26.116966","exception":false,"start_time":"2025-04-27T17:15:26.112823","status":"completed"},"tags":[]}},{"id":"f45b3434","cell_type":"code","source":"def audio2melspec(audio_data, cfg):\n    \"\"\"Convert audio data to mel spectrogram\"\"\"\n    if np.isnan(audio_data).any():\n        mean_signal = np.nanmean(audio_data)\n        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n\n    mel_spec = librosa.feature.melspectrogram(\n        y=audio_data,\n        sr=cfg.FS,\n        n_fft=cfg.N_FFT,\n        hop_length=cfg.HOP_LENGTH,\n        n_mels=cfg.N_MELS,\n        fmin=cfg.FMIN,\n        fmax=cfg.FMAX,\n        power=2.0,\n        pad_mode=\"reflect\",\n        norm='slaney',\n        htk=True,\n        center=True,\n    )\n\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n    \n    return mel_spec_norm\n\ndef process_audio_segment(audio_data, cfg):\n    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n        audio_data = np.pad(audio_data, \n                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n                          mode='constant')\n    \n    mel_spec = audio2melspec(audio_data, cfg)\n    \n    if mel_spec.shape != cfg.TARGET_SHAPE:\n        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n        \n    return mel_spec.astype(np.float32)","metadata":{"execution":{"iopub.execute_input":"2025-04-27T17:15:26.127195Z","iopub.status.busy":"2025-04-27T17:15:26.126823Z","iopub.status.idle":"2025-04-27T17:15:26.134776Z","shell.execute_reply":"2025-04-27T17:15:26.13342Z"},"papermill":{"duration":0.01505,"end_time":"2025-04-27T17:15:26.13649","exception":false,"start_time":"2025-04-27T17:15:26.12144","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"30acf991","cell_type":"code","source":"def find_model_files(cfg):\n    \"\"\"\n    Find all .pth model files in the specified model directory\n    \"\"\"\n    model_files = []\n    \n    model_dir = Path(cfg.model_path)\n    \n    for path in model_dir.glob('**/*.pth'):\n        model_files.append(str(path))\n    \n    return model_files\n\ndef load_models(cfg, num_classes):\n    \"\"\"\n    Load all found model files and prepare them for ensemble\n    \"\"\"\n    models = []\n    \n    model_files = find_model_files(cfg)\n    \n    if not model_files:\n        print(f\"Warning: No model files found under {cfg.model_path}!\")\n        return models\n    \n    print(f\"Found a total of {len(model_files)} model files.\")\n    \n    if cfg.use_specific_folds:\n        filtered_files = []\n        for fold in cfg.folds:\n            fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n            filtered_files.extend(fold_files)\n        model_files = filtered_files\n        print(f\"Using {len(model_files)} model files for the specified folds ({cfg.folds}).\")\n    \n    for model_path in model_files:\n        try:\n            print(f\"Loading model: {model_path}\")\n            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device))\n            \n            model = BirdCLEFModel(cfg, num_classes)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            model = model.to(cfg.device)\n            model.eval()\n            \n            models.append(model)\n        except Exception as e:\n            print(f\"Error loading model {model_path}: {e}\")\n    \n    return models\n\ndef predict_on_spectrogram(audio_path, models, cfg, species_ids):\n    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n    predictions = []\n    row_ids = []\n    soundscape_id = Path(audio_path).stem\n    \n    try:\n        print(f\"Processing {soundscape_id}\")\n        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n        \n        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n        \n        for segment_idx in range(total_segments):\n            start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n            end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n            segment_audio = audio_data[start_sample:end_sample]\n            \n            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n            row_id = f\"{soundscape_id}_{end_time_sec}\"\n            row_ids.append(row_id)\n\n            if cfg.use_tta:\n                all_preds = []\n                \n                for tta_idx in range(cfg.tta_count):\n                    mel_spec = process_audio_segment(segment_audio, cfg)\n                    mel_spec = apply_tta(mel_spec, tta_idx)\n\n                    mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n                    mel_spec = mel_spec.to(cfg.device)\n\n                    if len(models) == 1:\n                        with torch.no_grad():\n                            outputs = models[0](mel_spec)\n                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n                            all_preds.append(probs)\n                    else:\n                        segment_preds = []\n                        for model in models:\n                            with torch.no_grad():\n                                outputs = model(mel_spec)\n                                probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n                                segment_preds.append(probs)\n                        \n                        avg_preds = np.mean(segment_preds, axis=0)\n                        all_preds.append(avg_preds)\n\n                final_preds = np.mean(all_preds, axis=0)\n            else:\n                mel_spec = process_audio_segment(segment_audio, cfg)\n                \n                mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n                mel_spec = mel_spec.to(cfg.device)\n                \n                if len(models) == 1:\n                    with torch.no_grad():\n                        outputs = models[0](mel_spec)\n                        final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n                else:\n                    segment_preds = []\n                    for model in models:\n                        with torch.no_grad():\n                            outputs = model(mel_spec)\n                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n                            segment_preds.append(probs)\n\n                    final_preds = np.mean(segment_preds, axis=0)\n                    \n            predictions.append(final_preds)\n            \n    except Exception as e:\n        print(f\"Error processing {audio_path}: {e}\")\n    \n    return row_ids, predictions","metadata":{"execution":{"iopub.execute_input":"2025-04-27T17:15:26.146916Z","iopub.status.busy":"2025-04-27T17:15:26.146569Z","iopub.status.idle":"2025-04-27T17:15:26.163174Z","shell.execute_reply":"2025-04-27T17:15:26.162147Z"},"papermill":{"duration":0.023687,"end_time":"2025-04-27T17:15:26.164845","exception":false,"start_time":"2025-04-27T17:15:26.141158","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"2c7abd76","cell_type":"code","source":"def apply_tta(spec, tta_idx):\n    \"\"\"Apply test-time augmentation\"\"\"\n    if tta_idx == 0:\n        # Original spectrogram\n        return spec\n    elif tta_idx == 1:\n        # Time shift (horizontal flip)\n        return np.flip(spec, axis=1)\n    elif tta_idx == 2:\n        # Frequency shift (vertical flip)\n        return np.flip(spec, axis=0)\n    else:\n        return spec\n\ndef run_inference(cfg, models, species_ids):\n    \"\"\"Run inference on all test soundscapes\"\"\"\n    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n    \n    if cfg.debug:\n        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n        test_files = test_files[:cfg.debug_count]\n    \n    print(f\"Found {len(test_files)} test soundscapes\")\n\n    all_row_ids = []\n    all_predictions = []\n\n    for audio_path in tqdm(test_files):\n        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n        all_row_ids.extend(row_ids)\n        all_predictions.extend(predictions)\n    \n    return all_row_ids, all_predictions\n\ndef create_submission(row_ids, predictions, species_ids, cfg):\n    \"\"\"Create submission dataframe\"\"\"\n    print(\"Creating submission dataframe...\")\n\n    submission_dict = {'row_id': row_ids}\n    \n    for i, species in enumerate(species_ids):\n        submission_dict[species] = [pred[i] for pred in predictions]\n\n    submission_df = pd.DataFrame(submission_dict)\n    submission_df.set_index('row_id', inplace=True)\n    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n\n    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n    if missing_cols:\n        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n        for col in missing_cols:\n            submission_df[col] = 0.0\n\n    submission_df = submission_df[sample_sub.columns]\n    submission_df = submission_df.reset_index()\n    \n    return submission_df\n","metadata":{"execution":{"iopub.execute_input":"2025-04-27T17:15:26.175315Z","iopub.status.busy":"2025-04-27T17:15:26.174943Z","iopub.status.idle":"2025-04-27T17:15:26.18407Z","shell.execute_reply":"2025-04-27T17:15:26.183013Z"},"papermill":{"duration":0.016288,"end_time":"2025-04-27T17:15:26.185797","exception":false,"start_time":"2025-04-27T17:15:26.169509","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"ef69af06","cell_type":"code","source":"def main():\n    start_time = time.time()\n    print(\"Starting BirdCLEF-2025 inference...\")\n    print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n\n    models = load_models(cfg, num_classes)\n    \n    if not models:\n        print(\"No models found! Please check model paths.\")\n        return\n    \n    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n\n    row_ids, predictions = run_inference(cfg, models, species_ids)\n    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n    submission_path = 'submission.csv'\n    submission_df.to_csv(submission_path, index=False)\n    print(f\"Submission saved to {submission_path}\")\n    \n    end_time = time.time()\n    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")","metadata":{"execution":{"iopub.execute_input":"2025-04-27T17:15:26.196361Z","iopub.status.busy":"2025-04-27T17:15:26.195965Z","iopub.status.idle":"2025-04-27T17:15:26.202259Z","shell.execute_reply":"2025-04-27T17:15:26.200862Z"},"papermill":{"duration":0.013507,"end_time":"2025-04-27T17:15:26.204055","exception":false,"start_time":"2025-04-27T17:15:26.190548","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"a172d401","cell_type":"markdown","source":"<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n<b>\nCreate Submission\n</b></h1> ","metadata":{"papermill":{"duration":0.004607,"end_time":"2025-04-27T17:15:26.213419","exception":false,"start_time":"2025-04-27T17:15:26.208812","status":"completed"},"tags":[]}},{"id":"ffeedfd1","cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.execute_input":"2025-04-27T17:15:26.223705Z","iopub.status.busy":"2025-04-27T17:15:26.223319Z","iopub.status.idle":"2025-04-27T17:15:35.346852Z","shell.execute_reply":"2025-04-27T17:15:35.345441Z"},"papermill":{"duration":9.130737,"end_time":"2025-04-27T17:15:35.348679","exception":false,"start_time":"2025-04-27T17:15:26.217942","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"5052fe34","cell_type":"code","source":"sub = pd.read_csv('submission.csv')\ncols = sub.columns[1:]\ngroups = sub['row_id'].str.rsplit('_', n=1).str[0]\ngroups = groups.values\nfor group in np.unique(groups):\n    sub_group = sub[group == groups]\n    predictions = sub_group[cols].values\n    new_predictions = predictions.copy()\n    for i in range(1, predictions.shape[0]-1):\n        new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n    new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n    new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n    sub_group[cols] = new_predictions\n    sub[group == groups] = sub_group\nsub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2025-04-27T17:15:35.361085Z","iopub.status.busy":"2025-04-27T17:15:35.360692Z","iopub.status.idle":"2025-04-27T17:15:35.391633Z","shell.execute_reply":"2025-04-27T17:15:35.390592Z"},"papermill":{"duration":0.039661,"end_time":"2025-04-27T17:15:35.39353","exception":false,"start_time":"2025-04-27T17:15:35.353869","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"07d43100","cell_type":"code","source":"","metadata":{"papermill":{"duration":0.004679,"end_time":"2025-04-27T17:15:35.4037","exception":false,"start_time":"2025-04-27T17:15:35.399021","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}
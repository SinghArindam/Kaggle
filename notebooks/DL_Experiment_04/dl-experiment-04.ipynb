{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1095715,"sourceType":"datasetVersion","datasetId":612351}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"3cd9fb6a-a8e1-4382-ab21-36258c19fe32","_cell_guid":"5e0807aa-3e5e-4a9f-8343-3b388563f59c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T10:52:38.363429Z","iopub.execute_input":"2025-04-20T10:52:38.363795Z","iopub.status.idle":"2025-04-20T10:52:38.676544Z","shell.execute_reply.started":"2025-04-20T10:52:38.36377Z","shell.execute_reply":"2025-04-20T10:52:38.675769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# Download NLTK data\nnltk.download('punkt')\n\n# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Custom Dataset\nclass AGNewsDataset(Dataset):\n    def __init__(self, texts, labels, vocab, max_len=100):\n        self.texts = texts\n        self.labels = labels\n        self.vocab = vocab\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        tokens = word_tokenize(text.lower())\n        indices = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens[:self.max_len]]\n        # Pad or truncate\n        if len(indices) < self.max_len:\n            indices += [self.vocab['<pad>']] * (self.max_len - len(indices))\n        else:\n            indices = indices[:self.max_len]\n        return torch.tensor(indices, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n\n# CNN Model\nclass TextCNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_classes, filter_sizes=[3, 4, 5], num_filters=100):\n        super(TextCNN, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.convs = nn.ModuleList([\n            nn.Conv1d(embed_dim, num_filters, fs) for fs in filter_sizes\n        ])\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n    \n    def forward(self, x):\n        embedded = self.embedding(x).transpose(1, 2)  # (batch_size, embed_dim, seq_len)\n        conv_outs = [torch.relu(conv(embedded)) for conv in self.convs]  # List of (batch_size, num_filters, *)\n        pooled = [torch.max(out, dim=2)[0] for out in conv_outs]  # Max pooling: (batch_size, num_filters)\n        cat = self.dropout(torch.cat(pooled, dim=1))  # (batch_size, num_filters * len(filter_sizes))\n        return self.fc(cat)  # (batch_size, num_classes)\n\n# Data Preprocessing\ndef load_data(file_path):\n    df = pd.read_csv(file_path)\n    texts = df['Description'].values\n    labels = df['Class Index'].values - 1  # Convert to 0-based indexing\n    return texts, labels\n\ndef build_vocab(texts, min_freq=5):\n    # Tokenize and count words\n    word_counts = Counter()\n    for text in texts:\n        tokens = word_tokenize(text.lower())\n        word_counts.update(tokens)\n    \n    # Create vocabulary\n    vocab = {'<pad>': 0, '<unk>': 1}\n    vocab_idx = 2\n    for word, count in word_counts.items():\n        if count >= min_freq:\n            vocab[word] = vocab_idx\n            vocab_idx += 1\n    return vocab\n\n# Collate function for DataLoader\ndef collate_batch(batch):\n    texts, labels = zip(*batch)\n    return torch.stack(texts).to(device), torch.tensor(labels, dtype=torch.long).to(device)\n\n# Training Loop with Metrics Collection\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n    model.to(device)\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss, train_acc = 0, 0\n        for texts, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n            optimizer.zero_grad()\n            outputs = model(texts)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            preds = torch.argmax(outputs, dim=1)\n            train_acc += (preds == labels).float().mean().item()\n        \n        train_loss /= len(train_loader)\n        train_acc /= len(train_loader)\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        \n        # Validation\n        model.eval()\n        val_loss, val_acc = 0, 0\n        with torch.no_grad():\n            for texts, labels in val_loader:\n                outputs = model(texts)\n                val_loss += criterion(outputs, labels).item()\n                preds = torch.argmax(outputs, dim=1)\n                val_acc += (preds == labels).float().mean().item()\n        \n        val_loss /= len(val_loader)\n        val_acc /= len(val_loader)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n        \n        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n    \n    return train_losses, val_losses, train_accs, val_accs\n\n# Plotting Function\ndef plot_metrics(train_losses, val_losses, train_accs, val_accs, num_epochs):\n    epochs = range(1, num_epochs + 1)\n    \n    # Plot Loss\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label='Train Loss')\n    plt.plot(epochs, val_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accs, label='Train Accuracy')\n    plt.plot(epochs, val_accs, label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Main\ndef main():\n    # Hyperparameters\n    EMBED_DIM = 100\n    NUM_FILTERS = 100\n    FILTER_SIZES = [3, 4, 5]\n    BATCH_SIZE = 32\n    NUM_EPOCHS = 25\n    LEARNING_RATE = 0.001\n    MAX_LEN = 100\n    MIN_FREQ = 5\n    \n    # Load and preprocess data\n    file_path = \"/kaggle/input/ag-news-classification-dataset/train.csv\"\n    texts, labels = load_data(file_path)\n    vocab = build_vocab(texts, min_freq=MIN_FREQ)\n    \n    # Split data\n    train_texts, val_texts, train_labels, val_labels = train_test_split(\n        texts, labels, test_size=0.2, random_state=42\n    )\n    \n    # Create datasets and loaders\n    train_dataset = AGNewsDataset(train_texts, train_labels, vocab, max_len=MAX_LEN)\n    val_dataset = AGNewsDataset(val_texts, val_labels, vocab, max_len=MAX_LEN)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n    \n    # Initialize model\n    num_classes = 4  # AG News: World, Sports, Business, Sci/Tech\n    model = TextCNN(len(vocab), EMBED_DIM, num_classes, FILTER_SIZES, NUM_FILTERS)\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    \n    # Train and collect metrics\n    train_losses, val_losses, train_accs, val_accs = train_model(\n        model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS\n    )\n    \n    # Plot metrics\n    plot_metrics(train_losses, val_losses, train_accs, val_accs, NUM_EPOCHS)\n    \n    # Save model\n    torch.save(model.state_dict(), \"ag_news_text_cnn_model.pth\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:52:38.682172Z","iopub.execute_input":"2025-04-20T10:52:38.68237Z","iopub.status.idle":"2025-04-20T10:58:37.570877Z","shell.execute_reply.started":"2025-04-20T10:52:38.682354Z","shell.execute_reply":"2025-04-20T10:58:37.570047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
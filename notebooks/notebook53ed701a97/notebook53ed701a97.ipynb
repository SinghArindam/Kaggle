{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5196,"sourceType":"datasetVersion","datasetId":3147}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\nimport numpy as np #for linear algebra\nimport pandas as pd #for handling data \nfrom matplotlib import pyplot as plt #for data visualization and tools \n\n\n# Download latest version\npath = kagglehub.dataset_download(\"animatronbot/mnist-digit-recognizer\")\n\nprint(\"Path to dataset files:\", path)\n\ndata = pd.read_csv('/kaggle/input/mnist-digit-recognizer/train.csv')\n\ndata = np.array(data)\nm, n = data.shape \nnp.random.shuffle(data)\nprint(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T04:22:42.370208Z","iopub.execute_input":"2025-01-16T04:22:42.370735Z","iopub.status.idle":"2025-01-16T04:22:45.728015Z","shell.execute_reply.started":"2025-01-16T04:22:42.370684Z","shell.execute_reply":"2025-01-16T04:22:45.726759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dev = data[0:1000].T\nY_dev = data_dev[0]\nX_dev = data_dev[1:n] #taking only n rows \nX_dev = X_dev / 255 #reducing the pixels to the level of 255\n\ndata_train = data[1000:m].T\nY_train = data_train[0]\nX_train = data_train[1:n] #taking only n rows \nX_train = X_train / 255 #reducing the pixels to the level of 255\n_, m_train = X_train.shape\n\ndef ini_params():\n    W1 = np.random.rand(10,784) - 0.5 #this value should be a probability value ranging from 0 to 1\n    b1 = np.random.rand(10,1) - 0.5 #initial bias matrix b1\n    W2 = np.random.rand(10,10) - 0.5 #initial weight W2 (for hidden layer)\n    b2 = np.random.rand(10,1) - 0.5 #initial bias b2 (for hidden layer)\n    return W1, b1, W2, b2\n\ndef RELU(Z):\n    return np.maximum(Z, 0)\n\ndef softmax(Z):\n    A = np.exp(Z) / sum(np.exp(Z))\n    return A\n\ndef forward_propagation(W1, b1, W2, b2, X):\n    Z1 = W1.dot(X) + b1\n    A1 = RELU(Z1)\n    Z2 = W2.dot(A1) + b2 \n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n\ndef RELU_der(Z):\n    return Z>0\n\ndef one_hot(Y):\n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n    #here the size gives the number of samples in the Y matrix and Y.max() gives the number of different classes in Y\n    #we have created a matrix of zeros\n    one_hot_Y[np.arange(Y.size), Y] = 1\n    one_hot_Y = one_hot_Y.T\n    return one_hot_Y\n\ndef backward_propagation(Z1, A1, Z2, A2, W1, W2, X, Y):\n    one_hot_Y = one_hot(Y)\n    dZ2 = A2 - one_hot_Y\n    dW2 = 1 / m * dZ2.dot(A1.T)\n    db2 = 1 / m * np.sum(dZ2)\n    dZ1 = W2.T.dot(dZ2) * RELU_der(Z1)\n    dW1 = 1 / m * dZ1.dot(X.T)\n    db1 = 1 / m *np.sum(dZ1)\n    return dW1, db1, dW2, db2\n\ndef update_par(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1\n    W2 = W2 - alpha * dW2\n    b2 = b2 - alpha * db2\n    return W1, b1, W2, b2\n\ndef get_pred(A2):\n    return np.argmax(A2,0)\n\ndef get_accuracy(prediction, Y):\n    print(prediction, Y)\n    return np.sum(prediction == Y) / Y.size\n\ndef gradient_descent(X, Y, alpha, iterations):\n    W1, b1, W2, b2 = ini_params()\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = forward_propagation(W1, b1, W2, b2, X)\n        dW1, db1, dW2, db2 = backward_propagation(Z1, A1, Z2, A2, W1, W2, X, Y)\n        W1, b1, W2, b2 = update_par(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        if i % 10 == 0:\n            print(\"interation: \", i)\n            predictions = get_pred(A2)\n            print(get_accuracy(predictions, Y))\n    return W1, b1, W2, b2\n\nW1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 1000)\n\ndef make_pred(X, W1, b1, W2, b2):\n    _, _, _, A2 = forward_propagation(W1, b1, W2, b2, X)\n    predictions = get_pred(A2)\n    return predictions\n\ndef test_pred(index, W1, b1, W2, b2):\n    current_image = X_train[:, index, None]\n    prediction = make_pred(current_image, W1, b1, W2, b2)\n    label = Y_train[index]\n    print(\"Prediction: \", prediction)\n    print(\"Label: \", label)\n\n    current_image = current_image.reshape(28,28) * 255\n    plt.gray()\n    plt.imshow(current_image, interpolation = \"nearest\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T04:24:57.815584Z","iopub.execute_input":"2025-01-16T04:24:57.815996Z","iopub.status.idle":"2025-01-16T04:26:15.263976Z","shell.execute_reply.started":"2025-01-16T04:24:57.815967Z","shell.execute_reply":"2025-01-16T04:26:15.262925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"indices = [0,1,2,3,4,5,6,7,8,9,10]\nfor i in indices:\n    test_pred(i, W1, b1, W2, b2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T04:20:01.835064Z","iopub.execute_input":"2025-01-16T04:20:01.835519Z","iopub.status.idle":"2025-01-16T04:20:03.823729Z","shell.execute_reply.started":"2025-01-16T04:20:01.835478Z","shell.execute_reply":"2025-01-16T04:20:03.822415Z"}},"outputs":[],"execution_count":null}]}
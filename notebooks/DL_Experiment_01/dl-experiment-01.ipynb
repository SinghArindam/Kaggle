{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T09:45:00.842602Z","iopub.execute_input":"2025-04-20T09:45:00.842873Z","iopub.status.idle":"2025-04-20T09:45:01.191382Z","shell.execute_reply.started":"2025-04-20T09:45:00.842851Z","shell.execute_reply":"2025-04-20T09:45:01.190648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cupy as cp\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Activation functions (CuPy-compatible)\ndef sigmoid(x):\n    return 1 / (1 + cp.exp(-cp.clip(x, -500, 500)))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\ndef softmax(x):\n    exp_x = cp.exp(x - cp.max(x, axis=1, keepdims=True))\n    return exp_x / cp.sum(exp_x, axis=1, keepdims=True)\n\n# Cross-entropy loss (CuPy-compatible)\ndef cross_entropy_loss(y_true, y_pred):\n    return -cp.sum(y_true * cp.log(y_pred + 1e-9)) / y_true.shape[0]\n\n# Neural Network class (GPU-accelerated with CuPy)\nclass NeuralNetwork:\n    def __init__(self, layers):\n        self.layers = layers\n        self.weights = []\n        self.biases = []\n        self.initialize_parameters()\n    \n    def initialize_parameters(self):\n        for i in range(1, len(self.layers)):\n            # Initialize weights and biases on GPU\n            self.weights.append(cp.random.randn(self.layers[i], self.layers[i-1]) * 0.01)\n            self.biases.append(cp.zeros((self.layers[i], 1)))\n    \n    def forward(self, X):\n        self.activations = [X.T]\n        self.z_values = []\n        for i in range(len(self.weights)):\n            z = cp.dot(self.weights[i], self.activations[-1]) + self.biases[i]\n            self.z_values.append(z)\n            if i < len(self.weights) - 1:\n                a = sigmoid(z)\n            else:\n                a = softmax(z.T).T\n            self.activations.append(a)\n        return self.activations[-1].T\n    \n    def backward(self, X, y, learning_rate):\n        m = X.shape[0]\n        delta = self.activations[-1] - y.T\n        for i in range(len(self.weights) - 1, -1, -1):\n            dw = cp.dot(delta, self.activations[i].T) / m\n            db = cp.sum(delta, axis=1, keepdims=True) / m\n            if i > 0:\n                delta = cp.dot(self.weights[i].T, delta) * sigmoid_derivative(self.activations[i])\n            self.weights[i] -= learning_rate * dw\n            self.biases[i] -= learning_rate * db\n    \n    def train(self, X, y, epochs, learning_rate, batch_size):\n        losses = []\n        for epoch in range(epochs):\n            for i in range(0, X.shape[0], batch_size):\n                X_batch = X[i:i+batch_size]\n                y_batch = y[i:i+batch_size]\n                y_pred = self.forward(X_batch)\n                self.backward(X_batch, y_batch, learning_rate)\n            y_pred = self.forward(X)\n            loss = cross_entropy_loss(y, y_pred)\n            losses.append(float(loss.get()))  # Convert CuPy array to Python scalar\n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n        return losses\n\n# Load MNIST data from Kaggle dataset\ntrain_df = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest_df = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n\n# Prepare training data\nX = train_df.drop('label', axis=1).values / 255.0  # Normalize pixel values\ny = train_df['label'].values\n\n# One-hot encode labels\nencoder = OneHotEncoder(sparse=False)\ny = encoder.fit_transform(y.reshape(-1, 1))\n\n# Convert data to CuPy arrays for GPU\nX = cp.array(X)\ny = cp.array(y)\n\n# Split into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X.get(), y.get(), test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = cp.array(X_train), cp.array(X_val), cp.array(y_train), cp.array(y_val)\n\n# Initialize and train neural network\nnn = NeuralNetwork([784, 128, 10])  # 784 input, 128 hidden, 10 output\nlosses = nn.train(X_train, y_train, epochs=2500, learning_rate=0.1, batch_size=256)\n\n# Evaluate on validation set\ny_pred = nn.forward(X_val)\nval_accuracy = cp.mean(cp.argmax(y_pred, axis=1) == cp.argmax(y_val, axis=1))\nprint(f\"Validation Accuracy: {float(val_accuracy):.4f}\")\n\n# Generate predictions for test set (Kaggle submission)\nX_test = cp.array(test_df.values / 255.0)\ny_test_pred = nn.forward(X_test)\ny_test_labels = cp.argmax(y_test_pred, axis=1).get()  # Convert to NumPy for submission\n\n# Create submission file\nsubmission = pd.DataFrame({'ImageId': cp.arange(1, len(y_test_labels) + 1).get(), 'Label': y_test_labels})\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"Submission file created: /kaggle/working/submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T09:45:01.192585Z","iopub.execute_input":"2025-04-20T09:45:01.192952Z","iopub.status.idle":"2025-04-20T09:47:26.860516Z","shell.execute_reply.started":"2025-04-20T09:45:01.192925Z","shell.execute_reply":"2025-04-20T09:47:26.859851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
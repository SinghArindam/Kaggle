{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CNN model (with 2+ layers of convolutions) to classify image datasets on Dataset","metadata":{"_uuid":"62fefc15-0ce9-44f2-931b-fb2fa80c46a9","_cell_guid":"c81fa876-392c-4b0c-bf48-284b3d97e71d","collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# PyTorch CNN for CIFAR-10\n\nThis script implements a ResNet-based architecture with common techniques\nlike data augmentation (AutoAugment), SGD with momentum, weight decay,\nand the OneCycleLR learning rate scheduler to achieve high accuracy.\nDesigned to be run on environments like Kaggle with GPU support.","metadata":{}},{"cell_type":"markdown","source":"# Initial Imports","metadata":{"_uuid":"5b51ceb8-b47a-42cd-a770-42a8a85e7ef9","_cell_guid":"e8ab23a1-e8d7-41df-a6e8-4218b8e73a3a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import OneCycleLR # Powerful scheduler\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models # To get ResNet\nimport time\nimport copy\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## pytorch Version","metadata":{"_uuid":"a59a5650-220a-4efb-8cef-11fae95f878b","_cell_guid":"b74d6285-f304-4675-87b2-4ab740e364ba","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"%matplotlib inline\nprint(torch.__version__)","metadata":{"_uuid":"e4e5f9b5-9a98-4ea0-bae7-0f059fe64f19","_cell_guid":"a7a8c7e1-ad69-4fa8-ba4b-6f9721244562","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-20T04:13:49.92073Z","iopub.execute_input":"2025-03-20T04:13:49.920946Z","iopub.status.idle":"2025-03-20T04:13:49.938995Z","shell.execute_reply.started":"2025-03-20T04:13:49.920928Z","shell.execute_reply":"2025-03-20T04:13:49.938168Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", 128))\nEPOCHS = int(os.environ.get(\"EPOCHS\", 60))\nLEARNING_RATE = float(os.environ.get(\"LEARNING_RATE\", 0.1))\nWEIGHT_DECAY = float(os.environ.get(\"WEIGHT_DECAY\", 5e-4))\nMOMENTUM = 0.9\nNUM_WORKERS = int(os.environ.get(\"NUM_WORKERS\", 2))\nDATA_DIR = './data'\nMODEL_SAVE_PATH = 'best_cifar10_resnet_pytorch.pth'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Device Setup","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    DEVICE = torch.device(\"cuda\")\n    PIN_MEMORY = True\n    print(f\"CUDA available. Using GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    DEVICE = torch.device(\"cpu\")\n    PIN_MEMORY = False\n    print(\"CUDA not available. Using CPU.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{"_uuid":"de91defc-19c0-4468-b5cb-9338529a7e5c","_cell_guid":"48e9f850-f45f-4e54-ac1c-50988d675c45","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"print(\"Setting up data transformations...\")\nnormalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n                                 std=[0.2023, 0.1994, 0.2010])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Augmentation for Training set","metadata":{}},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4, padding_mode='reflect'), # Reflect padding often works well\n    transforms.RandomHorizontalFlip(),\n    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10), # Strong augmentation policy\n    transforms.ToTensor(),\n    normalize,\n    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0, inplace=False) # Cutout/Random Erasing\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transformation for Validation/Testing set (only normalization)","metadata":{}},{"cell_type":"code","source":"transform_test = transforms.Compose([\n    transforms.ToTensor(),\n    normalize,\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Dataset\n\nDownload and load datasets\n\nAdding exception handling for robustness in different environments","metadata":{}},{"cell_type":"code","source":"print(\"Loading CIFAR-10 dataset...\")\ntry:\n    train_dataset = datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=transform_train)\n    test_dataset = datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=transform_test)\n    print(\"Dataset downloaded/loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error during dataset download/load: {e}\")\n    print(\"Attempting to load from local directory assuming it exists...\")\n    try:\n        train_dataset = datasets.CIFAR10(root=DATA_DIR, train=True, download=False, transform=transform_train)\n        test_dataset = datasets.CIFAR10(root=DATA_DIR, train=False, download=False, transform=transform_test)\n        print(\"Dataset loaded successfully from local directory.\")\n    except Exception as e2:\n        print(f\"Failed to load dataset from local directory: {e2}\")\n        print(\"Please ensure the dataset is available or network access is configured.\")\n        exit() # Exit if dataset cannot be loaded","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loaders","metadata":{"_uuid":"3adadfd3-c296-40cd-b583-1564baf20069","_cell_guid":"40e6dc6f-c801-4938-809f-11e14de50ad5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\"Creating DataLoaders...\")\ntrain_loader = DataLoader(train_dataset,\n                          batch_size=BATCH_SIZE,\n                          shuffle=True,\n                          num_workers=NUM_WORKERS,\n                          pin_memory=PIN_MEMORY,\n                          persistent_workers=True if NUM_WORKERS > 0 else False) # Can speed up loading\n\ntest_loader = DataLoader(test_dataset,\n                         batch_size=BATCH_SIZE*2, # Use larger batch size for evaluation\n                         shuffle=False,\n                         num_workers=NUM_WORKERS,\n                         pin_memory=PIN_MEMORY,\n                         persistent_workers=True if NUM_WORKERS > 0 else False)\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")\nprint(f\"Number of training batches: {len(train_loader)}\")\nprint(f\"Number of testing batches: {len(test_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Definition (ResNet18 adapted for CIFAR-10)\n\nModify the first convolution layer for 32x32 images:\n\nOriginal ResNet18's conv1 is kernel_size=7, stride=2, padding=3\n\nWe adapt it for smaller images: kernel_size=3, stride=1, padding=1","metadata":{"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}}},{"cell_type":"code","source":"print(\"Defining the model architecture (ResNet18 adapted for CIFAR-10)...\")\ndef create_resnet18_cifar():\n    \"\"\"Creates a ResNet-18 model adapted for CIFAR-10 (32x32 input).\"\"\"\n    model = models.resnet18(weights=None, num_classes=10)\n    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n\n    # Remove the initial max pooling layer (not needed for 32x32)\n    # Original ResNet18's maxpool is kernel_size=3, stride=2, padding=1\n    model.maxpool = nn.Identity() # Identity layer simply passes input through\n\n    return model\n\nmodel = create_resnet18_cifar().to(DEVICE)\n# print(model) # Uncomment to view the adapted architecture details","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss Function, Optimizer, Scheduler ","metadata":{"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}}},{"cell_type":"code","source":"print(\"Setting up loss function, optimizer, and learning rate scheduler...\")\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optimizer: SGD with momentum and weight decay is standard for ResNets with OneCycleLR","metadata":{"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}}},{"cell_type":"code","source":"optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, # Initial LR (OneCycleLR adjusts this)\n                      momentum=MOMENTUM, weight_decay=WEIGHT_DECAY, nesterov=True) # Nesterov momentum sometimes helps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Learning Rate Scheduler: OneCycleLR\nDynamically adjusts LR: increases then decreases. Often leads to faster training and better results.","metadata":{"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}}},{"cell_type":"code","source":"scheduler = OneCycleLR(optimizer,\n                       max_lr=LEARNING_RATE,          # The peak learning rate\n                       epochs=EPOCHS,                 # Total number of epochs\n                       steps_per_epoch=len(train_loader), # Steps per epoch\n                       pct_start=0.3,                 # Percentage of cycle increasing LR (default 0.3)\n                       anneal_strategy='cos',         # Cosine annealing (default)\n                       div_factor=10,                 # Determines initial LR (max_lr / div_factor)\n                       final_div_factor=100)          # Determines final LR (initial_lr / final_div_factor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training and Validation Functions ","metadata":{"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}}},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device): # Add scaler if using AMP\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n    start_epoch_time = time.time()\n\n    for i, (inputs, labels) in enumerate(train_loader):\n        batch_start_time = time.time()\n        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True) # Use non_blocking for potential speedup\n\n        optimizer.zero_grad(set_to_none=True) # More memory efficient than zeroing\n\n        # --- Standard Precision Training ---\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        # --- End Standard Precision Block ---\n\n\n        # Step the scheduler *after* the optimizer step\n        scheduler.step()\n\n        # --- Statistics ---\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = torch.max(outputs.data, 1)\n        total_samples += labels.size(0)\n        correct_predictions += (predicted == labels).sum().item()\n\n        # Print progress occasionally\n        if (i + 1) % 100 == 0 or (i + 1) == len(train_loader):\n            current_lr = optimizer.param_groups[0]['lr'] # Get current LR\n            batch_time = time.time() - batch_start_time\n            print(f'\\r  Batch [{i+1:>4}/{len(train_loader):<4}] | Loss: {loss.item():.4f} | LR: {current_lr:.6f} | Time: {batch_time*1000:.1f}ms', end='')\n\n    print() # Newline after epoch progress\n    epoch_duration = time.time() - start_epoch_time\n    epoch_loss = running_loss / total_samples\n    epoch_acc = correct_predictions / total_samples\n    print(f\"  Epoch Train Duration: {epoch_duration:.2f}s\")\n    return epoch_loss, epoch_acc\n\ndef validate(model, test_loader, criterion, device):\n    model.eval()  # Set model to evaluation mode\n    running_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n    val_start_time = time.time()\n\n    with torch.no_grad():  # Essential for evaluation\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n\n            # --- Standard Precision ---\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            # --- End Standard Precision ---\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            total_samples += labels.size(0)\n            correct_predictions += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / total_samples\n    epoch_acc = correct_predictions / total_samples\n    val_duration = time.time() - val_start_time\n    print(f\"  Validation Duration: {val_duration:.2f}s\")\n    return epoch_loss, epoch_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main Training Loop ","metadata":{"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}}},{"cell_type":"code","source":"print(f\"\\n--- Starting Training for {EPOCHS} Epochs ---\")\nstart_train_time = time.time()\n\nbest_val_accuracy = 0.0\nhistory = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lr': []}\n\nfor epoch in range(EPOCHS):\n    epoch_start_time = time.time()\n    print(f\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\")\n\n    # Train\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, DEVICE) # Pass scaler if using AMP\n\n    # Validate\n    val_loss, val_acc = validate(model, test_loader, criterion, DEVICE)\n\n    # Log history\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    history['lr'].append(optimizer.param_groups[0]['lr']) # Log LR at end of epoch\n\n    epoch_duration = time.time() - epoch_start_time\n    print(\"-\" * 50)\n    print(f\"Epoch {epoch+1} Summary:\")\n    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n    print(f\"  Valid Loss: {val_loss:.4f} | Valid Acc: {val_acc*100:.2f}%\")\n    print(f\"  Epoch Duration: {epoch_duration:.2f}s\")\n    print(\"-\" * 50)\n\n    # Save the model checkpoint if validation accuracy improves\n    if val_acc > best_val_accuracy:\n        best_val_accuracy = val_acc\n        # Save the model's state dictionary\n        try:\n            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n            print(f\"  ** Checkpoint Saved: {MODEL_SAVE_PATH} (Val Acc: {best_val_accuracy*100:.2f}%) **\")\n        except Exception as e:\n            print(f\"  Warning: Could not save model checkpoint. Error: {e}\")\n\n\ntotal_train_time = time.time() - start_train_time\nprint(f\"\\n--- Training Finished ---\")\nprint(f\"Total Training Time: {total_train_time / 60:.2f} minutes ({total_train_time:.2f} seconds)\")\nprint(f\"Best Validation Accuracy Achieved: {best_val_accuracy*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Evaluation on Test Set ","metadata":{"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}}},{"cell_type":"code","source":"print(f\"\\nLoading best model from '{MODEL_SAVE_PATH}' for final evaluation...\")\ntry:\n    best_model = create_resnet18_cifar().to(DEVICE)\n    best_model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE)) # Ensure map_location for CPU fallback\n\n    test_loss, test_acc = validate(best_model, test_loader, criterion, DEVICE)\n    print(f\"\\n--- Final Test Set Performance (Best Model) ---\")\n    print(f\"  Test Loss: {test_loss:.4f}\")\n    print(f\"  Test Accuracy: {test_acc*100:.2f}%\")\nexcept FileNotFoundError:\n    print(f\"Error: Best model file '{MODEL_SAVE_PATH}' not found. Skipping final evaluation.\")\nexcept Exception as e:\n     print(f\"Error loading best model or during final evaluation: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plotting Training History","metadata":{"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}}},{"cell_type":"code","source":"print(\"\\nGenerating training history plots...\")\ndef plot_training_history(history):\n    epochs_range = range(1, len(history['train_loss']) + 1)\n    plt.style.use('seaborn-v0_8-whitegrid') # Use a nice style\n\n    fig, axs = plt.subplots(1, 3, figsize=(18, 5)) # Create 3 subplots\n\n    # Plot Loss\n    axs[0].plot(epochs_range, history['train_loss'], 'o-', color='royalblue', label='Training Loss')\n    axs[0].plot(epochs_range, history['val_loss'], 'o-', color='orangered', label='Validation Loss')\n    axs[0].set_title('Loss vs. Epochs')\n    axs[0].set_xlabel('Epochs')\n    axs[0].set_ylabel('Loss')\n    axs[0].legend()\n\n    # Plot Accuracy\n    axs[1].plot(epochs_range, [acc * 100 for acc in history['train_acc']], 'o-', color='royalblue', label='Training Accuracy')\n    axs[1].plot(epochs_range, [acc * 100 for acc in history['val_acc']], 'o-', color='orangered', label='Validation Accuracy')\n    axs[1].set_title('Accuracy vs. Epochs')\n    axs[1].set_xlabel('Epochs')\n    axs[1].set_ylabel('Accuracy (%)')\n    axs[1].legend()\n    axs[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1f}%')) # Format y-axis as percentage\n\n    # Plot Learning Rate\n    axs[2].plot(epochs_range, history['lr'], 'o-', color='forestgreen', label='Learning Rate')\n    axs[2].set_title('Learning Rate vs. Epochs')\n    axs[2].set_xlabel('Epochs')\n    axs[2].set_ylabel('Learning Rate')\n    axs[2].ticklabel_format(style='sci', axis='y', scilimits=(0,0)) # Use scientific notation if needed\n    axs[2].legend()\n\n    fig.suptitle('CIFAR-10 Training History (ResNet18)', fontsize=16)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n    plt.savefig('training_history_pytorch.png') # Save the plot\n    print(\"Training history plot saved as 'training_history_pytorch.png'\")\n    # plt.show() # Display the plot if running interactively\n\n# Check if history has data before plotting\nif history['train_loss']:\n    plot_training_history(history)\nelse:\n    print(\"No training history recorded, skipping plot generation.\")\n\nprint(\"\\n--- Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T09:17:59.273697Z","iopub.execute_input":"2025-05-03T09:17:59.274032Z","iopub.status.idle":"2025-05-03T09:20:01.712557Z","shell.execute_reply.started":"2025-05-03T09:17:59.274003Z","shell.execute_reply":"2025-05-03T09:20:01.710825Z"}},"outputs":[],"execution_count":null}]}
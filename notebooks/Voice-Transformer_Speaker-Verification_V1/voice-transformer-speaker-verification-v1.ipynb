{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10545331,"sourceType":"datasetVersion","datasetId":6524657}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:33:34.718708Z","iopub.execute_input":"2025-02-11T13:33:34.719011Z","iopub.status.idle":"2025-02-11T13:33:35.848034Z","shell.execute_reply.started":"2025-02-11T13:33:34.71898Z","shell.execute_reply":"2025-02-11T13:33:35.847043Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!sudo rm -r /kaggle/working/VOT_V1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/SinghArindam/VOT_V1/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:33:35.849335Z","iopub.execute_input":"2025-02-11T13:33:35.849902Z","iopub.status.idle":"2025-02-11T13:33:36.589945Z","shell.execute_reply.started":"2025-02-11T13:33:35.849848Z","shell.execute_reply":"2025-02-11T13:33:36.588917Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'VOT_V1'...\nremote: Enumerating objects: 133, done.\u001b[K\nremote: Counting objects: 100% (133/133), done.\u001b[K\nremote: Compressing objects: 100% (106/106), done.\u001b[K\nremote: Total 133 (delta 28), reused 131 (delta 26), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (133/133), 2.35 MiB | 25.38 MiB/s, done.\nResolving deltas: 100% (28/28), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -r /kaggle/working/VOT_V1/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:33:36.592009Z","iopub.execute_input":"2025-02-11T13:33:36.592265Z","iopub.status.idle":"2025-02-11T13:33:41.207478Z","shell.execute_reply.started":"2025-02-11T13:33:36.592241Z","shell.execute_reply":"2025-02-11T13:33:41.206623Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/VOT_V1/requirements.txt (line 1)) (2.5.1+cu121)\nRequirement already satisfied: torchaudio>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/VOT_V1/requirements.txt (line 2)) (2.5.1+cu121)\nCollecting asteroid_filterbanks==0.4.0 (from -r /kaggle/working/VOT_V1/requirements.txt (line 3))\n  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/VOT_V1/requirements.txt (line 4)) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/VOT_V1/requirements.txt (line 5)) (1.13.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/VOT_V1/requirements.txt (line 6)) (1.2.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/VOT_V1/requirements.txt (line 7)) (4.67.1)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/VOT_V1/requirements.txt (line 8)) (6.0.2)\nRequirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/VOT_V1/requirements.txt (line 9)) (0.12.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from asteroid_filterbanks==0.4.0->-r /kaggle/working/VOT_V1/requirements.txt (line 3)) (4.12.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r /kaggle/working/VOT_V1/requirements.txt (line 1)) (3.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r /kaggle/working/VOT_V1/requirements.txt (line 1)) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r /kaggle/working/VOT_V1/requirements.txt (line 1)) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r /kaggle/working/VOT_V1/requirements.txt (line 1)) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r /kaggle/working/VOT_V1/requirements.txt (line 1)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.0->-r /kaggle/working/VOT_V1/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->-r /kaggle/working/VOT_V1/requirements.txt (line 4)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->-r /kaggle/working/VOT_V1/requirements.txt (line 4)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->-r /kaggle/working/VOT_V1/requirements.txt (line 4)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->-r /kaggle/working/VOT_V1/requirements.txt (line 4)) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->-r /kaggle/working/VOT_V1/requirements.txt (line 4)) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->-r /kaggle/working/VOT_V1/requirements.txt (line 4)) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r /kaggle/working/VOT_V1/requirements.txt (line 6)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r /kaggle/working/VOT_V1/requirements.txt (line 6)) (3.5.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->-r /kaggle/working/VOT_V1/requirements.txt (line 9)) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->-r /kaggle/working/VOT_V1/requirements.txt (line 9)) (2.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->-r /kaggle/working/VOT_V1/requirements.txt (line 1)) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->-r /kaggle/working/VOT_V1/requirements.txt (line 4)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->-r /kaggle/working/VOT_V1/requirements.txt (line 4)) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->-r /kaggle/working/VOT_V1/requirements.txt (line 4)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->-r /kaggle/working/VOT_V1/requirements.txt (line 4)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->-r /kaggle/working/VOT_V1/requirements.txt (line 4)) (2024.2.0)\nDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\nInstalling collected packages: asteroid_filterbanks\nSuccessfully installed asteroid_filterbanks-0.4.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!python /kaggle/working/VOT_V1/generate_traintxt.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:35:25.531049Z","iopub.execute_input":"2025-02-11T13:35:25.531396Z","iopub.status.idle":"2025-02-11T13:35:25.747261Z","shell.execute_reply.started":"2025-02-11T13:35:25.531356Z","shell.execute_reply":"2025-02-11T13:35:25.746385Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!python /kaggle/working/VOT_V1/trainSpeakerNet.py --config /kaggle/working/VOT_V1/configs/VOT_focal.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:35:38.525457Z","iopub.execute_input":"2025-02-11T13:35:38.525759Z","iopub.status.idle":"2025-02-11T13:35:42.292713Z","shell.execute_reply.started":"2025-02-11T13:35:38.525733Z","shell.execute_reply":"2025-02-11T13:35:42.291676Z"}},"outputs":[{"name":"stdout","text":"Python Version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\nPyTorch Version: 2.5.1+cu121\nNumber of GPUs: 2\nSave path: exps/VOT_focal\nInitialised focal margin 0.300 scale 30.000\nTraceback (most recent call last):\n  File \"/kaggle/working/VOT_V1/trainSpeakerNet.py\", line 279, in <module>\n    main()\n  File \"/kaggle/working/VOT_V1/trainSpeakerNet.py\", line 275, in main\n    main_worker(0, None, args)\n  File \"/kaggle/working/VOT_V1/trainSpeakerNet.py\", line 144, in main_worker\n    train_dataset = train_dataset_loader(**vars(args))\n  File \"/kaggle/working/VOT_V1/DatasetLoader.py\", line 127, in __init__\n    with open(train_list) as dataset_file:\nFileNotFoundError: [Errno 2] No such file or directory: 'b.txt'\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# !python ./trainSpeakerNet.py --eval --config ./configs/VOT_focal.yaml --initial_model exps/VOT_focal/model/model000000007.model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:33:47.582818Z","iopub.execute_input":"2025-02-11T13:33:47.583043Z","iopub.status.idle":"2025-02-11T13:33:47.586642Z","shell.execute_reply.started":"2025-02-11T13:33:47.583021Z","shell.execute_reply":"2025-02-11T13:33:47.585735Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Combined V1","metadata":{}},{"cell_type":"code","source":"# #!/usr/bin/env python\n# # -*- coding: utf-8 -*-\n# \"\"\"\n# Combined Voice-Transformer Speaker Verification Code for Kaggle\n\n# This file is an all-in-one Python script that combines all of the repository’s code:\n#   - utils.py\n#   - DatasetLoader.py\n#   - Feature_Visualization.py\n#   - SpeakerNet.py\n#   - dataprep.py\n#   - generate_traintxt.py\n#   - trainSpeakerNet.py\n#   - tuneThreshold.py\n# and also (inlined below) the code for submodules from:\n#   - models/\n#   - loss/\n#   - optimizer/\n#   - scheduler/\n\n# Before running on Kaggle, please ensure that:\n#   • All external dependencies (e.g. torch, torchaudio, numpy, scipy, pyyaml, tqdm, soundfile, scikit-learn) are installed.\n#   • The [SUBMODULES] sections are filled in with the original code.\n#   • Any file path (for datasets, configuration files, etc.) is modified appropriately for the Kaggle environment.\n\n# ---------------------------------------------------------------------\n# Author: (Your Name)\n# Date: (Current Date)\n# ---------------------------------------------------------------------\n# \"\"\"\n\n# ############################################\n# # --- Begin utils.py (31 lines) ---\n# ############################################\n# import torch\n# import torch.nn.functional as F\n\n# def accuracy(output, target, topk=(1,)):\n#     \"\"\"Computes the precision@k for the specified values of k\"\"\"\n#     maxk = max(topk)\n#     batch_size = target.size(0)\n#     _, pred = output.topk(maxk, 1, True, True)\n#     pred = pred.t()\n#     correct = pred.eq(target.view(1, -1).expand_as(pred))\n#     res = []\n#     for k in topk:\n#         correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n#         res.append(correct_k.mul_(100.0 / batch_size))\n#     return res\n\n# class PreEmphasis(torch.nn.Module):\n#     def __init__(self, coef: float = 0.97):\n#         super().__init__()\n#         self.coef = coef\n#         # In pytorch, convolution uses cross-correlation so we flip the filter.\n#         self.register_buffer('flipped_filter', torch.FloatTensor([-self.coef, 1.]).unsqueeze(0).unsqueeze(0))\n#     def forward(self, input: torch.tensor) -> torch.tensor:\n#         assert len(input.size()) == 2, 'Input tensor must be 2D!'\n#         input = input.unsqueeze(1)\n#         input = F.pad(input, (1, 0), 'reflect')\n#         return F.conv1d(input, self.flipped_filter).squeeze(1)\n# ############################################\n# # --- End utils.py ---\n# ############################################\n\n# ############################################\n# # --- Begin DatasetLoader.py (from 305-line file) ---\n# ############################################\n# import os\n# import random\n# import threading\n# import time\n# import math\n# import json\n# import torchaudio\n# import glob\n# import soundfile\n# from scipy import signal\n# from scipy.io import wavfile\n# from torch.utils.data import Dataset, DataLoader\n# import torch.distributed as dist\n# import numpy\n\n# def round_down(num, divisor):\n#     return num - (num % divisor)\n\n# def worker_init_fn(worker_id):\n#     numpy.random.seed(numpy.random.get_state()[1][0] + worker_id)\n\n# def loadWAV(filename, max_frames, evalmode=True, num_eval=10):\n#     # Maximum audio length\n#     max_audio = max_frames * 160\n#     # Read wav file and convert to numpy array\n#     audio, sample_rate = soundfile.read(filename)\n#     audiosize = audio.shape[0]\n#     if audiosize <= max_audio:\n#         shortage = max_audio - audiosize + 1\n#         audio = numpy.pad(audio, (0, shortage), 'wrap')\n#         audiosize = audio.shape[0]\n#     if evalmode:\n#         startframe = numpy.linspace(0, audiosize - max_audio, num=num_eval)\n#     else:\n#         startframe = numpy.array([numpy.int64(random.random() * (audiosize - max_audio))])\n#     feats = []\n#     if evalmode and max_frames == 0:\n#         feats.append(audio)\n#     else:\n#         for asf in startframe:\n#             x = audio[int(asf):int(asf) + max_audio]\n#             feats.append(x)\n#     feat = numpy.stack(feats, axis=0).astype(numpy.float)\n#     return feat\n\n# class AugmentWAV(object):\n#     def __init__(self, musan_path, rir_path, max_frames):\n#         self.max_frames = max_frames\n#         self.max_audio = max_frames * 160 + 240\n#         self.noisetypes = ['noise', 'speech', 'music']\n#         self.noisesnr = {'noise': [0,15], 'speech': [13,20], 'music': [5,15]}\n#         self.numnoise = {'noise': [1,1], 'speech': [3,7], 'music': [1,1]}\n#         self.noiselist = {}\n#         augment_files = glob.glob(os.path.join(musan_path, '*/*/*/*.wav'))\n#         for file in augment_files:\n#             key = file.split('/')[-4]\n#             if key not in self.noiselist:\n#                 self.noiselist[key] = []\n#             self.noiselist[key].append(file)\n#         self.rir_files = glob.glob(os.path.join(rir_path, '*/*/*.wav'))\n#     def additive_noise(self, noisecat, audio):\n#         clean_db = 10 * numpy.log10(numpy.mean(audio ** 2) + 1e-4)\n#         numnoise = self.numnoise[noisecat]\n#         noiselist = random.sample(self.noiselist[noisecat], random.randint(numnoise[0], numnoise[1]))\n#         noises = []\n#         for noise in noiselist:\n#             noiseaudio = loadWAV(noise, self.max_frames, evalmode=False)\n#             noise_snr = random.uniform(self.noisesnr[noisecat][0], self.noisesnr[noisecat][1])\n#             noise_db = 10 * numpy.log10(numpy.mean(noiseaudio[0] ** 2) + 1e-4)\n#             noises.append(numpy.sqrt(10 ** ((clean_db - noise_db - noise_snr) / 10)) * noiseaudio)\n#         return numpy.sum(numpy.concatenate(noises, axis=0), axis=0, keepdims=True) + audio\n#     def reverberate(self, audio):\n#         rir_file = random.choice(self.rir_files)\n#         rir, fs = soundfile.read(rir_file)\n#         rir = numpy.expand_dims(rir.astype(numpy.float), 0)\n#         rir = rir / numpy.sqrt(numpy.sum(rir ** 2))\n#         return signal.convolve(audio, rir, mode='full')[:, :self.max_audio]\n\n# class train_dataset_loader(Dataset):\n#     def __init__(self, train_list, augment, musan_path, rir_path, max_frames, train_path, **kwargs):\n#         self.augment_wav = AugmentWAV(musan_path=musan_path, rir_path=rir_path, max_frames=max_frames)\n#         self.train_list = train_list\n#         self.max_frames = max_frames\n#         self.musan_path = musan_path\n#         self.rir_path = rir_path\n#         self.augment = augment\n#         with open(train_list) as dataset_file:\n#             lines = dataset_file.readlines()\n#         dictkeys = list(set([x.split()[0] for x in lines]))\n#         dictkeys.sort()\n#         dictkeys = { key: ii for ii, key in enumerate(dictkeys) }\n#         self.data_list = []\n#         self.data_label = []\n#         for lidx, line in enumerate(lines):\n#             data = line.strip().split()\n#             speaker_label = dictkeys[data[0]]\n#             filename = os.path.join(train_path, data[1])\n#             self.data_label.append(speaker_label)\n#             self.data_list.append(filename)\n#     def __getitem__(self, indices):\n#         feat = []\n#         for index in indices:\n#             waveform, sample_rate = torchaudio.load(self.data_list[index])\n#             feature = torchaudio.compliance.kaldi.fbank(\n#                 waveform, num_mel_bins=111, dither=0, energy_floor=0,\n#                 low_freq=20, high_freq=0, preemphasis_coefficient=0.97,\n#                 use_energy=True, raw_energy=True, remove_dc_offset=True,\n#                 sample_frequency=16000, window_type='povey')\n#             xs = feature\n#             xssize = xs.shape[0]\n#             startframe = numpy.array([numpy.int64(random.random()*(xssize - self.max_frames))])\n#             feats = []\n#             if self.augment:\n#                 augtype = random.randint(0, 4)\n#                 # Uncomment and adjust the following lines if you want to augment the audio:\n#                 # if augtype == 1:\n#                 #     audio = self.augment_wav.reverberate(audio)\n#                 # elif augtype == 2:\n#                 #     audio = self.augment_wav.additive_noise('music', audio)\n#                 # elif augtype == 3:\n#                 #     audio = self.augment_wav.additive_noise('speech', audio)\n#                 # elif augtype == 4:\n#                 #     audio = self.augment_wav.additive_noise('noise', audio)\n#                 pass\n#             for asf in startframe:\n#                 x = xs[int(asf):int(asf) + self.max_frames]\n#                 feats.append(x)\n#             feats = numpy.stack(feats, axis=0).astype(numpy.float)\n#             feat.append(feats)\n#         feat = numpy.concatenate(feat, axis=0)\n#         return torch.FloatTensor(feat), self.data_label[index]\n#     def __len__(self):\n#         return len(self.data_list)\n\n# class test_dataset_loader(Dataset):\n#     def __init__(self, test_list, test_path, eval_frames, num_eval, **kwargs):\n#         self.max_frames = eval_frames\n#         self.num_eval = num_eval\n#         self.test_path = test_path\n#         self.test_list = test_list\n#     def __getitem__(self, index):\n#         waveform, sample_rate = torchaudio.load(os.path.join(self.test_path, self.test_list[index]))\n#         feature = torchaudio.compliance.kaldi.fbank(\n#             waveform, num_mel_bins=111, dither=0, energy_floor=0,\n#             low_freq=20, high_freq=0, preemphasis_coefficient=0.97,\n#             use_energy=True, raw_energy=True, remove_dc_offset=True,\n#             sample_frequency=16000, window_type='povey')\n#         xs = feature\n#         xssize = xs.shape[0]\n#         feats = []\n#         startframe = numpy.linspace(0, xssize - self.max_frames, num=10)\n#         for asf in startframe:\n#             x = xs[int(asf):int(asf) + self.max_frames]\n#             feats.append(x)\n#         feat = numpy.stack(feats, axis=0).astype(numpy.float)\n#         return torch.FloatTensor(feat), self.test_list[index]\n#     def __len__(self):\n#         return len(self.test_list)\n\n# class train_dataset_sampler(torch.utils.data.Sampler):\n#     def __init__(self, data_source, nPerSpeaker, max_seg_per_spk, batch_size, distributed, seed, **kwargs):\n#         self.data_label = data_source.data_label\n#         self.nPerSpeaker = nPerSpeaker\n#         self.max_seg_per_spk = max_seg_per_spk\n#         self.batch_size = batch_size\n#         self.epoch = 0\n#         self.seed = seed\n#         self.distributed = distributed\n#     def __iter__(self):\n#         g = torch.Generator()\n#         g.manual_seed(self.seed + self.epoch)\n#         indices = torch.randperm(len(self.data_label), generator=g).tolist()\n#         data_dict = {}\n#         for index in indices:\n#             speaker_label = self.data_label[index]\n#             if speaker_label not in data_dict:\n#                 data_dict[speaker_label] = []\n#             data_dict[speaker_label].append(index)\n#         dictkeys = list(data_dict.keys())\n#         dictkeys.sort()\n#         lol = lambda lst, sz: [lst[i:i+sz] for i in range(0, len(lst), sz)]\n#         flattened_list = []\n#         flattened_label = []\n#         for findex, key in enumerate(dictkeys):\n#             data = data_dict[key]\n#             numSeg = round_down(min(len(data), self.max_seg_per_spk), self.nPerSpeaker)\n#             rp = lol(numpy.arange(numSeg), self.nPerSpeaker)\n#             flattened_label.extend([findex] * (len(rp)))\n#             for indices in rp:\n#                 flattened_list.append([data[i] for i in indices])\n#         mixid = torch.randperm(len(flattened_label), generator=g).tolist()\n#         mixlabel = []\n#         mixmap = []\n#         for ii in mixid:\n#             startbatch = round_down(len(mixlabel), self.batch_size)\n#             if flattened_label[ii] not in mixlabel[startbatch:]:\n#                 mixlabel.append(flattened_label[ii])\n#                 mixmap.append(ii)\n#         mixed_list = [flattened_list[i] for i in mixmap]\n#         if self.distributed:\n#             total_size = round_down(len(mixed_list), self.batch_size * dist.get_world_size())\n#             start_index = int((dist.get_rank()) / dist.get_world_size() * total_size)\n#             end_index = int((dist.get_rank() + 1) / dist.get_world_size() * total_size)\n#             self.num_samples = end_index - start_index\n#             return iter(mixed_list[start_index:end_index])\n#         else:\n#             total_size = round_down(len(mixed_list), self.batch_size)\n#             self.num_samples = total_size\n#             return iter(mixed_list[:total_size])\n#     def __len__(self) -> int:\n#         return self.num_samples\n#     def set_epoch(self, epoch: int) -> None:\n#         self.epoch = epoch\n# ############################################\n# # --- End DatasetLoader.py ---\n# ############################################\n\n# ############################################\n# # --- Begin Feature_Visualization.py (42 lines) ---\n# ############################################\n# import matplotlib.pyplot as plt\n# import os\n# import torchaudio\n\n# # Set these paths appropriately in Kaggle.\n# output_images_folder = \"output_images\"  # Directory to save images\n# data_path_list = \"input_wav_files\"       # Directory to input .wav files\n\n# for root, dirs, files in os.walk(data_path_list):\n#     rel_path = os.path.relpath(root, data_path_list)\n#     output_folder = os.path.join(output_images_folder, rel_path)\n#     for wav_file in files:\n#         if wav_file.endswith('.wav'):\n#             wav_path = os.path.join(root, wav_file)\n#             print(\"Processing:\", wav_path)\n#             waveform, sample_rate = torchaudio.load(wav_path)\n#             feature = torchaudio.compliance.kaldi.fbank(\n#                 waveform, num_mel_bins=111, dither=0, energy_floor=0,\n#                 low_freq=20, high_freq=0, preemphasis_coefficient=0.97,\n#                 use_energy=True, raw_energy=True, remove_dc_offset=True,\n#                 sample_frequency=16000, window_type='povey')\n#             xs = feature\n#             plt.imshow(xs.detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n#             plt.axis(\"off\")\n#             output_filename = os.path.splitext(os.path.basename(wav_path))[0] + '.png'\n#             os.makedirs(output_folder, exist_ok=True)\n#             output_path = os.path.join(output_folder, output_filename)\n#             plt.savefig(output_path, bbox_inches=\"tight\", pad_inches=0)\n#             plt.show()\n# ############################################\n# # --- End Feature_Visualization.py ---\n# ############################################\n\n# ############################################\n# # --- Begin SpeakerNet.py (282 lines) ---\n# ############################################\n# import importlib\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import sys\n# import time\n# import itertools\n# from torch.cuda.amp import autocast, GradScaler\n# # Note: In the original code, there is a dependency: \"from DatasetLoader import test_dataset_loader\"\n# # (which is already defined above).\n\n# class WrappedModel(nn.Module):\n#     \"\"\"A wrapper to enable consistency between single- and multi-GPU training.\"\"\"\n#     def __init__(self, model):\n#         super(WrappedModel, self).__init__()\n#         self.module = model\n#     def forward(self, x, label=None):\n#         return self.module(x, label)\n\n# class SpeakerNet(nn.Module):\n#     def __init__(self, model, optimizer, trainfunc, nPerSpeaker, **kwargs):\n#         super(SpeakerNet, self).__init__()\n#         # Dynamically import the model from the 'models' folder.\n#         # [SUBMODULE: Paste the code from your models module here, e.g. MainModel definition]\n#         SpeakerNetModel = importlib.import_module(\"models.\" + model).__getattribute__(\"MainModel\")\n#         self.__S__ = SpeakerNetModel(**kwargs)\n#         # Dynamically import the loss function from the 'loss' folder.\n#         # [SUBMODULE: Paste the code from your loss module here (e.g. AAMF loss or other)]\n#         LossFunction = importlib.import_module(\"loss.\" + trainfunc).__getattribute__(\"LossFunction\")\n#         self.__L__ = LossFunction(**kwargs)\n#         self.nPerSpeaker = nPerSpeaker\n#     def forward(self, data, label=None):\n#         outp = self.__S__.forward(data)\n#         if label is None:\n#             return outp\n#         else:\n#             nloss, prec1 = self.__L__.forward(outp, label)\n#             return nloss, prec1, outp\n\n# class ModelTrainer(object):\n#     def __init__(self, speaker_model, optimizer, scheduler, gpu, mixedprec, **kwargs):\n#         self.__model__ = speaker_model\n#         # Dynamically import the optimizer from the 'optimizer' folder.\n#         # [SUBMODULE: Paste the code from your optimizer module here]\n#         Optimizer = importlib.import_module(\"optimizer.\" + optimizer).__getattribute__(\"Optimizer\")\n#         self.__optimizer__ = Optimizer(self.__model__.parameters(), **kwargs)\n#         # Dynamically import the scheduler from the 'scheduler' folder.\n#         # [SUBMODULE: Paste the code from your scheduler module here]\n#         Scheduler = importlib.import_module(\"scheduler.\" + scheduler).__getattribute__(\"Scheduler\")\n#         self.__scheduler__, self.lr_step = Scheduler(self.__optimizer__, **kwargs)\n#         self.scaler = GradScaler()\n#         self.gpu = gpu\n#         self.mixedprec = mixedprec\n#         assert self.lr_step in [\"epoch\", \"iteration\"]\n#     def train_network(self, loader, verbose):\n#         self.__model__.train()\n#         stepsize = loader.batch_size\n#         counter = 0\n#         index = 0\n#         loss = 0\n#         top1 = 0\n#         tstart = time.time()\n#         a = [[]]\n#         for data, data_label in loader:\n#             self.__model__.zero_grad()\n#             label = torch.LongTensor(data_label).cuda()\n#             nloss, prec1, outp = self.__model__(data, label)\n#             a.append(outp.tolist())\n#             nloss.backward()\n#             self.__optimizer__.step()\n#             loss += nloss.detach().cpu().item()\n#             top1 += prec1.detach().cpu().item()\n#             counter += 1\n#             index += stepsize\n#             telapsed = time.time() - tstart\n#             tstart = time.time()\n#             if verbose:\n#                 sys.stdout.write(\"\\rProcessing {:d} of {:d}:\".format(index, loader.__len__() * loader.batch_size))\n#                 sys.stdout.write(\"Loss {:f} TEER/TAcc {:2.3f}% - {:.2f} Hz \".format(loss / counter, top1 / counter, stepsize / telapsed))\n#                 sys.stdout.flush()\n#             if self.lr_step == \"iteration\":\n#                 self.__scheduler__.step()\n#         if self.lr_step == \"epoch\":\n#             self.__scheduler__.step()\n#         return (loss / counter, top1 / counter)\n#     def evaluateFromList(self, test_list, test_path, nDataLoaderThread, distributed, print_interval=100, num_eval=10, **kwargs):\n#         if distributed:\n#             rank = torch.distributed.get_rank()\n#         else:\n#             rank = 0\n#         self.__model__.eval()\n#         with open(test_list) as f:\n#             lines = f.readlines()\n#         files = list(set(list(itertools.chain(*[x.strip().split()[-2:] for x in lines]))))\n#         files.sort()\n#         feats = {}\n#         tstart = time.time()\n#         from DatasetLoader import test_dataset_loader  # Already defined above.\n#         test_dataset = test_dataset_loader(files, test_path, num_eval=num_eval, **kwargs)\n#         if distributed:\n#             sampler = torch.utils.data.distributed.DistributedSampler(test_dataset, shuffle=False)\n#         else:\n#             sampler = None\n#         test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=nDataLoaderThread, drop_last=False, sampler=sampler)\n#         for idx, data in enumerate(test_loader):\n#             inp1 = data[0][0]\n#             with torch.no_grad():\n#                 ref_feat = self.__model__(inp1).detach().cpu()\n#             feats[data[1][0]] = ref_feat\n#             telapsed = time.time() - tstart\n#             if idx % print_interval == 0 and rank == 0:\n#                 sys.stdout.write(\"\\rReading {:d} of {:d}: {:.2f} Hz, embedding size {:d}\".format(idx, test_loader.__len__(), idx / telapsed, ref_feat.size()[1]))\n#         all_scores = []\n#         all_labels = []\n#         all_trials = []\n#         if distributed:\n#             feats_all = [None for _ in range(torch.distributed.get_world_size())]\n#             torch.distributed.all_gather_object(feats_all, feats)\n#             if rank == 0:\n#                 tstart = time.time()\n#                 feats = feats_all[0]\n#                 for feats_batch in feats_all[1:]:\n#                     feats.update(feats_batch)\n#         if rank == 0:\n#             for idx, line in enumerate(lines):\n#                 data = line.split()\n#                 if len(data) == 2:\n#                     data = [random.randint(0, 1)] + data\n#                 ref_feat = feats[data[1]].cuda()\n#                 com_feat = feats[data[2]].cuda()\n#                 if self.__model__.module.__L__.test_normalize:\n#                     ref_feat = F.normalize(ref_feat, p=2, dim=1)\n#                     com_feat = F.normalize(com_feat, p=2, dim=1)\n#                 dist_mat = torch.cdist(ref_feat.reshape(num_eval, -1), com_feat.reshape(num_eval, -1)).detach().cpu().numpy()\n#                 score = -1 * numpy.mean(dist_mat)\n#                 all_scores.append(score)\n#                 all_labels.append(int(data[0]))\n#                 all_trials.append(data[1] + \" \" + data[2])\n#                 if idx % print_interval == 0:\n#                     telapsed = time.time() - tstart\n#                     sys.stdout.write(\"\\rComputing {:d} of {:d}: {:.2f} Hz\".format(idx, len(lines), idx / telapsed))\n#                     sys.stdout.flush()\n#         return (all_scores, all_labels, all_trials)\n#     def saveParameters(self, path):\n#         torch.save(self.__model__.module.state_dict(), path)\n#     def loadParameters(self, path):\n#         self_state = self.__model__.module.state_dict()\n#         loaded_state = torch.load(path, map_location=\"cuda:%d\" % self.gpu)\n#         if len(loaded_state.keys()) == 1 and \"model\" in loaded_state:\n#             loaded_state = loaded_state[\"model\"]\n#         newdict = {}\n#         delete_list = []\n#         for name, param in loaded_state.items():\n#             new_name = \"__S__.\" + name\n#             newdict[new_name] = param\n#             delete_list.append(name)\n#         loaded_state.update(newdict)\n#         for name in delete_list:\n#             del loaded_state[name]\n#         for name, param in loaded_state.items():\n#             origname = name\n#             if name not in self_state:\n#                 name = name.replace(\"module.\", \"\")\n#             if name not in self_state:\n#                 print(\"{} is not in the model.\".format(origname))\n#                 continue\n#             if self_state[name].size() != loaded_state[origname].size():\n#                 print(\"Wrong parameter length: {}, model: {}, loaded: {}\".format(origname, self_state[name].size(), loaded_state[origname].size()))\n#                 continue\n#             self_state[name].copy_(param)\n# ############################################\n# # --- End SpeakerNet.py ---\n# ############################################\n\n# ############################################\n# # --- Begin dataprep.py (206 lines) ---\n# ############################################\n# import argparse\n# import subprocess\n# import hashlib\n# import tarfile\n# from zipfile import ZipFile\n# from tqdm import tqdm\n# from scipy.io import wavfile\n\n# parser_dataprep = argparse.ArgumentParser(description=\"VoxCeleb downloader\")\n# parser_dataprep.add_argument('--save_path', type=str, default=\"data\", help='Target directory')\n# parser_dataprep.add_argument('--user', type=str, default=\"user\", help='Username')\n# parser_dataprep.add_argument('--password', type=str, default=\"pass\", help='Password')\n# parser_dataprep.add_argument('--download', dest='download', action='store_true', help='Enable download')\n# parser_dataprep.add_argument('--extract', dest='extract', action='store_true', help='Enable extract')\n# parser_dataprep.add_argument('--convert', dest='convert', action='store_true', help='Enable convert')\n# parser_dataprep.add_argument('--augment', dest='augment', action='store_true', help='Download and extract augmentation files')\n# args_dataprep = parser_dataprep.parse_args()\n\n# def md5(fname):\n#     hash_md5 = hashlib.md5()\n#     with open(fname, \"rb\") as f:\n#         for chunk in iter(lambda: f.read(4096), b\"\"):\n#             hash_md5.update(chunk)\n#     return hash_md5.hexdigest()\n\n# def download_files(args, lines):\n#     for line in lines:\n#         url = line.split()[0]\n#         md5gt = line.split()[1]\n#         outfile = url.split('/')[-1]\n#         out = subprocess.call('wget %s --user %s --password %s -O %s/%s' % (url, args.user, args.password, args.save_path, outfile), shell=True)\n#         if out != 0:\n#             raise ValueError('Download failed %s.' % url)\n#         md5ck = md5('%s/%s' % (args.save_path, outfile))\n#         if md5ck == md5gt:\n#             print('Checksum successful %s.' % outfile)\n#         else:\n#             raise Warning('Checksum failed %s.' % outfile)\n\n# def concatenate_files(args, lines):\n#     for line in lines:\n#         infile = line.split()[0]\n#         outfile = line.split()[1]\n#         md5gt = line.split()[2]\n#         out = subprocess.call('cat %s/%s > %s/%s' % (args.save_path, infile, args.save_path, outfile), shell=True)\n#         md5ck = md5('%s/%s' % (args.save_path, outfile))\n#         if md5ck == md5gt:\n#             print('Checksum successful %s.' % outfile)\n#         else:\n#             raise Warning('Checksum failed %s.' % outfile)\n#         subprocess.call('rm %s/%s' % (args.save_path, infile), shell=True)\n\n# def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n#     import os\n#     def is_within_directory(directory, target):\n#         abs_directory = os.path.abspath(directory)\n#         abs_target = os.path.abspath(target)\n#         prefix = os.path.commonprefix([abs_directory, abs_target])\n#         return prefix == abs_directory\n#     for member in tar.getmembers():\n#         member_path = os.path.join(path, member.name)\n#         if not is_within_directory(path, member_path):\n#             raise Exception(\"Attempted Path Traversal in Tar File\")\n#     tar.extractall(path, members, numeric_owner=numeric_owner)\n\n# def full_extract(args, fname):\n#     print('Extracting %s' % fname)\n#     if fname.endswith(\".tar.gz\"):\n#         with tarfile.open(fname, \"r:gz\") as tar:\n#             safe_extract(tar, args.save_path)\n#     elif fname.endswith(\".zip\"):\n#         with ZipFile(fname, 'r') as zf:\n#             zf.extractall(args.save_path)\n\n# def part_extract(args, fname, target):\n#     print('Extracting %s' % fname)\n#     with ZipFile(fname, 'r') as zf:\n#         for infile in zf.namelist():\n#             if any([infile.startswith(x) for x in target]):\n#                 zf.extract(infile, args.save_path)\n\n# def convert_aac_to_wav(args):\n#     import glob\n#     files = glob.glob('%s/voxceleb2/*/*/*.m4a' % args.save_path)\n#     files.sort()\n#     print('Converting files from AAC to WAV')\n#     for fname in tqdm(files):\n#         outfile = fname.replace('.m4a', '.wav')\n#         out = subprocess.call('ffmpeg -y -i %s -ac 1 -vn -acodec pcm_s16le -ar 16000 %s >/dev/null 2>/dev/null' % (fname, outfile), shell=True)\n#         if out != 0:\n#             raise ValueError('Conversion failed %s.' % fname)\n\n# def split_musan(args):\n#     import glob\n#     files = glob.glob('%s/musan/*/*/*.wav' % args.save_path)\n#     audlen = 16000 * 5\n#     audstr = 16000 * 3\n#     for idx, file in enumerate(files):\n#         fs, aud = wavfile.read(file)\n#         writedir = os.path.splitext(file.replace('/musan/', '/musan_split/'))[0]\n#         os.makedirs(writedir, exist_ok=True)\n#         for st in range(0, len(aud) - audlen, audstr):\n#             wavfile.write(writedir + '/%05d.wav' % (st/fs), fs, aud[st:st+audlen])\n#         print(idx, file)\n\n# if __name__ == \"__main__\" and False:\n#     # This block is only run if you call dataprep.py directly.\n#     if not os.path.exists(args_dataprep.save_path):\n#         raise ValueError('Target directory does not exist.')\n#     with open('lists/fileparts.txt', 'r') as f:\n#         fileparts = f.readlines()\n#     with open('lists/files.txt', 'r') as f:\n#         files = f.readlines()\n#     with open('lists/augment.txt', 'r') as f:\n#         augfiles = f.readlines()\n#     if args_dataprep.augment:\n#         download_files(args_dataprep, augfiles)\n#         part_extract(args_dataprep, os.path.join(args_dataprep.save_path, 'rirs_noises.zip'),\n#                      ['RIRS_NOISES/simulated_rirs/mediumroom', 'RIRS_NOISES/simulated_rirs/smallroom'])\n#         full_extract(args_dataprep, os.path.join(args_dataprep.save_path, 'musan.tar.gz'))\n#         split_musan(args_dataprep)\n#     if args_dataprep.download:\n#         download_files(args_dataprep, fileparts)\n#     if args_dataprep.extract:\n#         concatenate_files(args_dataprep, files)\n#         for file in files:\n#             full_extract(args_dataprep, os.path.join(args_dataprep.save_path, file.split()[1]))\n#         subprocess.call('mv %s/dev/aac/* %s/aac/ && rm -r %s/dev' % (args_dataprep.save_path, args_dataprep.save_path, args_dataprep.save_path), shell=True)\n#         subprocess.call('mv %s/wav %s/voxceleb1' % (args_dataprep.save_path, args_dataprep.save_path), shell=True)\n#         subprocess.call('mv %s/aac %s/voxceleb2' % (args_dataprep.save_path, args_dataprep.save_path), shell=True)\n#     if args_dataprep.convert:\n#         convert_aac_to_wav(args_dataprep)\n# ############################################\n# # --- End dataprep.py ---\n# ############################################\n\n# ############################################\n# # --- Begin generate_traintxt.py (6 lines) ---\n# ############################################\n# import os\n# for dirpath, dirnames, filenames in os.walk('path/data/'):\n#     a = dirpath.split('data/')[-1]\n#     for filename in filenames:\n#         print(a + \" \" + dirpath.split('data/')[-1] + \"/\" + filename)\n# ############################################\n# # --- End generate_traintxt.py ---\n# ############################################\n\n# ############################################\n# # --- Begin trainSpeakerNet.py (279 lines) ---\n# ############################################\n# import argparse\n# import yaml\n# import glob\n# import zipfile\n# import datetime\n# import torch\n# import torch.distributed as dist\n# import torch.multiprocessing as mp\n# import warnings\n# import sys\n# import random\n\n# # Import the tuneThreshold functions (defined later in this file)\n# # (They are already included below in tuneThreshold.py section.)\n# # from tuneThreshold import *\n\n# # Import SpeakerNet and DatasetLoader (already defined above)\n# # from SpeakerNet import SpeakerNet, WrappedModel, ModelTrainer\n# # from DatasetLoader import train_dataset_loader, train_dataset_sampler, worker_init_fn\n\n# parser = argparse.ArgumentParser(description=\"SpeakerNet\")\n# parser.add_argument('--config', type=str, default=None, help='Config YAML file')\n# parser.add_argument('--max_frames', type=int, default=200, help='Input length for training')\n# parser.add_argument('--eval_frames', type=int, default=300, help='Input length for testing; 0 uses whole file')\n# parser.add_argument('--batch_size', type=int, default=200, help='Batch size (number of speakers per batch)')\n# parser.add_argument('--max_seg_per_spk', type=int, default=500, help='Max utterances per speaker per epoch')\n# parser.add_argument('--nDataLoaderThread', type=int, default=24, help='Number of data loader threads')\n# parser.add_argument('--augment', type=bool, default=False, help='Augment input')\n# parser.add_argument('--seed', type=int, default=3407, help='Random seed')\n# parser.add_argument('--test_interval', type=int, default=2, help='Test and save every [test_interval] epochs')\n# parser.add_argument('--max_epoch', type=int, default=500, help='Maximum number of epochs')\n# parser.add_argument('--trainfunc', type=str, default=\"\", help='Loss function')\n# parser.add_argument('--optimizer', type=str, default=\"adam\", help='Optimizer: sgd or adam')\n# parser.add_argument('--scheduler', type=str, default=\"steplr\", help='Learning rate scheduler')\n# parser.add_argument('--lr', type=float, default=0.0005, help='Learning rate')\n# parser.add_argument(\"--lr_decay\", type=float, default=0.95, help='Learning rate decay every test_interval epochs')\n# parser.add_argument('--weight_decay', type=float, default=0.1, help='Weight decay')\n# parser.add_argument(\"--hard_prob\", type=float, default=0.5, help='Hard negative mining probability')\n# parser.add_argument(\"--hard_rank\", type=int, default=10, help='Hard negative mining rank in the batch')\n# parser.add_argument('--margin', type=float, default=0.3, help='Loss margin')\n# parser.add_argument('--scale', type=float, default=30, help='Loss scale')\n# parser.add_argument('--nPerSpeaker', type=int, default=1, help='Utterances per speaker per batch')\n# parser.add_argument('--nClasses', type=int, default=1211, help='Number of speakers in softmax layer')\n# parser.add_argument('--dcf_p_target', type=float, default=0.05, help='A priori probability of target speaker')\n# parser.add_argument('--dcf_c_miss', type=float, default=1, help='Cost of missed detection')\n# parser.add_argument('--dcf_c_fa', type=float, default=1, help='Cost of false alarm')\n# parser.add_argument('--initial_model', type=str, default=\"\", help='Initial model weights')\n# parser.add_argument('--save_path', type=str, default=\"exps/exp1\", help='Path for models and logs')\n# parser.add_argument('--train_list', type=str, default=\"b.txt\", help='Training list')\n# parser.add_argument('--test_list', type=str, default=\"veri_test1.txt\", help='Evaluation list')\n# parser.add_argument('--train_path', type=str, default=\"path1\", help='Path to train set')\n# parser.add_argument('--test_path', type=str, default=\"path2\", help='Path to test set')\n# parser.add_argument('--musan_path', type=str, default=\"data/musan_split\", help='Path to MUSAN data')\n# parser.add_argument('--rir_path', type=str, default=\"data/RIRS_NOISES/simulated_rirs\", help='Path to RIRs')\n# parser.add_argument('--n_mels', type=int, default=40, help='Number of mel filterbanks')\n# parser.add_argument('--log_input', type=bool, default=False, help='Log input features')\n# parser.add_argument('--model', type=str, default=\"\", help='Name of model definition')\n# parser.add_argument('--encoder_type', type=str, default=\"SAP\", help='Type of encoder')\n# parser.add_argument('--nOut', type=int, default=512, help='Embedding size in final FC layer')\n# parser.add_argument('--sinc_stride', type=int, default=10, help='Stride size for RawNet3 analytic filterbank')\n# parser.add_argument('--eval', dest='eval', action='store_true', help='Evaluation only')\n# parser.add_argument('--port', type=str, default=\"8888\", help='Port for distributed training')\n# parser.add_argument('--distributed', dest='distributed', action='store_true', help='Enable distributed training')\n# parser.add_argument('--mixedprec', dest='mixedprec', action='store_true', help='Enable mixed precision training')\n# args_train = parser.parse_args()\n\n# def find_option_type(key, parser):\n#     for opt in parser._get_optional_actions():\n#         if ('--' + key) in opt.option_strings:\n#             return opt.type\n#     raise ValueError\n\n# if args_train.config is not None:\n#     with open(args_train.config, \"r\") as f:\n#         yml_config = yaml.load(f, Loader=yaml.FullLoader)\n#     for k, v in yml_config.items():\n#         if k in args_train.__dict__:\n#             typ = find_option_type(k, parser)\n#             args_train.__dict__[k] = typ(v)\n#         else:\n#             sys.stderr.write(\"Ignored unknown parameter {} in yaml.\\n\".format(k))\n\n# def main_worker(gpu, ngpus_per_node, args):\n#     args.gpu = gpu\n#     # Initialize model\n#     s = SpeakerNet(**vars(args))\n#     if args.distributed:\n#         os.environ['MASTER_ADDR'] = 'localhost'\n#         os.environ['MASTER_PORT'] = args.port\n#         dist.init_process_group(backend='nccl', world_size=ngpus_per_node, rank=args.gpu)\n#         torch.cuda.set_device(args.gpu)\n#         s.cuda(args.gpu)\n#         s = torch.nn.parallel.DistributedDataParallel(s, device_ids=[args.gpu], find_unused_parameters=True)\n#         print('Loaded model on GPU {:d}'.format(args.gpu))\n#     else:\n#         s = WrappedModel(s).cuda(args.gpu)\n#     it = 1\n#     eers = [100]\n#     if args.gpu == 0:\n#         scorefile = open(args.result_save_path + \"/scores.txt\", \"a+\")\n#         from DatasetLoader import train_dataset_loader, train_dataset_sampler, worker_init_fn\n#         train_dataset = train_dataset_loader(**vars(args))\n#         train_sampler = train_dataset_sampler(train_dataset, **vars(args))\n#         train_loader = torch.utils.data.DataLoader(\n#             train_dataset,\n#             batch_size=args.batch_size,\n#             num_workers=args.nDataLoaderThread,\n#             sampler=train_sampler,\n#             pin_memory=False,\n#             worker_init_fn=worker_init_fn,\n#             drop_last=True,\n#         )\n#         trainer = ModelTrainer(s, **vars(args))\n#         modelfiles = glob.glob('%s/model0*.model' % args.model_save_path)\n#         modelfiles.sort()\n#         if args.initial_model != \"\":\n#             trainer.loadParameters(args.initial_model)\n#             print(\"Model {} loaded!\".format(args.initial_model))\n#         elif len(modelfiles) >= 1:\n#             trainer.loadParameters(modelfiles[-1])\n#             print(\"Model {} loaded from previous state!\".format(modelfiles[-1]))\n#             it = int(os.path.splitext(os.path.basename(modelfiles[-1]))[0][5:]) + 1\n#             for ii in range(1, it):\n#                 trainer.__scheduler__.step()\n#         if args.eval == True:\n#             pytorch_total_params = sum(p.numel() for p in s.module.__S__.parameters())\n#             print('Total parameters: ', pytorch_total_params)\n#             print('Test list', args.test_list)\n#             sc, lab, _ = trainer.evaluateFromList(**vars(args))\n#             if args.gpu == 0:\n#                 from tuneThreshold import tuneThresholdfromScore, ComputeErrorRates, ComputeMinDcf\n#                 result = tuneThresholdfromScore(sc, lab, [1, 0.1])\n#                 fnrs, fprs, thresholds = ComputeErrorRates(sc, lab)\n#                 mindcf, threshold = ComputeMinDcf(fnrs, fprs, thresholds, args.dcf_p_target, args.dcf_c_miss, args.dcf_c_fa)\n#                 sc = (numpy.array(sc) > threshold)\n#                 test = (sc == numpy.array(lab))\n#                 print('\\n', time.strftime(\"%Y-%m-%d %H:%M:%S\"), \"VEER {:2.4f}\".format(result[1]), \"MinDCF {:2.5f}\".format(mindcf))\n#             return\n#         if args.gpu == 0:\n#             pyfiles = glob.glob('./*.py')\n#             strtime = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n#             zipf = zipfile.ZipFile(args.result_save_path + '/run%s.zip' % strtime, 'w', zipfile.ZIP_DEFLATED)\n#             for file in pyfiles:\n#                 zipf.write(file)\n#             zipf.close()\n#             with open(args.result_save_path + '/run%s.cmd' % strtime, 'w') as f:\n#                 f.write('%s' % args)\n#         for it in range(it, args.max_epoch + 1):\n#             train_sampler.set_epoch(it)\n#             clr = [x['lr'] for x in trainer.__optimizer__.param_groups]\n#             loss, traineer = trainer.train_network(train_loader, verbose=(args.gpu == 0))\n#             if args.gpu == 0:\n#                 print('\\n', time.strftime(\"%Y-%m-%d %H:%M:%S\"), \"Epoch {:d}, TEER/TAcc {:2.2f}, TLOSS {:f}, LR {:f}\".format(it, traineer, loss, max(clr)))\n#                 scorefile.write(\"Epoch {:d}, TEER/TAcc {:2.2f}, TLOSS {:f}, LR {:f} \\n\".format(it, traineer, loss, max(clr)))\n#             if it % 1 == 0:\n#                 sc, lab, _ = trainer.evaluateFromList(**vars(args))\n#                 if args.gpu == 0:\n#                     from tuneThreshold import tuneThresholdfromScore, ComputeErrorRates, ComputeMinDcf\n#                     result = tuneThresholdfromScore(sc, lab, [1, 0.1])\n#                     fnrs, fprs, thresholds = ComputeErrorRates(sc, lab)\n#                     mindcf, threshold = ComputeMinDcf(fnrs, fprs, thresholds, args.dcf_p_target, args.dcf_c_miss, args.dcf_c_fa)\n#                     eers.append(result[1])\n#                     print('\\n', time.strftime(\"%Y-%m-%d %H:%M:%S\"), \"Epoch {:d}, VEER {:2.4f}, MinDCF {:2.5f}\".format(it, result[1], mindcf))\n#                     scorefile.write(\"Epoch {:d}, VEER {:2.4f}, MinDCF {:2.5f}\\n\".format(it, result[1], mindcf))\n#                     trainer.saveParameters(args.model_save_path + \"/model%09d.model\" % it)\n#                     with open(args.model_save_path + \"/model%09d.eer\" % it, 'w') as eerfile:\n#                         eerfile.write('{:2.4f}'.format(result[1]))\n#                     scorefile.flush()\n#         if args.gpu == 0:\n#             scorefile.close()\n\n# def main():\n#     args.model_save_path = args.save_path + \"/model\"\n#     args.result_save_path = args.save_path + \"/result\"\n#     args.feat_save_path = \"\"\n#     os.makedirs(args.model_save_path, exist_ok=True)\n#     os.makedirs(args.result_save_path, exist_ok=True)\n#     n_gpus = torch.cuda.device_count()\n#     print('Python Version:', sys.version)\n#     print('PyTorch Version:', torch.__version__)\n#     print('Number of GPUs:', n_gpus)\n#     print('Save path:', args.save_path)\n#     if args.distributed:\n#         mp.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, args))\n#     else:\n#         main_worker(0, None, args)\n\n# if __name__ == '__main__':\n#     main()\n# ############################################\n# # --- End trainSpeakerNet.py ---\n# ############################################\n\n# ############################################\n# # --- Begin tuneThreshold.py (87 lines) ---\n# ############################################\n# from sklearn import metrics\n# import numpy\n# from operator import itemgetter\n\n# def tuneThresholdfromScore(scores, labels, target_fa, target_fr=None):\n#     fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)\n#     fnr = 1 - tpr\n#     tunedThreshold = []\n#     if target_fr:\n#         for tfr in target_fr:\n#             idx = numpy.nanargmin(numpy.absolute((tfr - fnr)))\n#             tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n#     for tfa in target_fa:\n#         idx = numpy.nanargmin(numpy.absolute((tfa - fpr)))\n#         tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n#     idxE = numpy.nanargmin(numpy.absolute((fnr - fpr)))\n#     eer = max(fpr[idxE], fnr[idxE]) * 100\n#     return (tunedThreshold, eer, fpr, fnr)\n\n# def ComputeErrorRates(scores, labels):\n#     sorted_indexes, thresholds = zip(*sorted([(index, threshold) for index, threshold in enumerate(scores)], key=itemgetter(1)))\n#     sorted_labels = [labels[i] for i in sorted_indexes]\n#     fnrs = []\n#     fprs = []\n#     for i in range(0, len(sorted_labels)):\n#         if i == 0:\n#             fnrs.append(sorted_labels[i])\n#             fprs.append(1 - sorted_labels[i])\n#         else:\n#             fnrs.append(fnrs[i-1] + sorted_labels[i])\n#             fprs.append(fprs[i-1] + 1 - sorted_labels[i])\n#     fnrs_norm = sum(sorted_labels)\n#     fprs_norm = len(sorted_labels) - fnrs_norm\n#     fnrs = [x / float(fnrs_norm) for x in fnrs]\n#     fprs = [1 - x / float(fprs_norm) for x in fprs]\n#     return fnrs, fprs, thresholds\n\n# def ComputeMinDcf(fnrs, fprs, thresholds, p_target, c_miss, c_fa):\n#     min_c_det = float(\"inf\")\n#     min_c_det_threshold = thresholds[0]\n#     for i in range(0, len(fnrs)):\n#         c_det = c_miss * fnrs[i] * p_target + c_fa * fprs[i] * (1 - p_target)\n#         if c_det < min_c_det:\n#             min_c_det = c_det\n#             min_c_det_threshold = thresholds[i]\n#     c_def = min(c_miss * p_target, c_fa * (1 - p_target))\n#     min_dcf = min_c_det / c_def\n#     return min_dcf, min_c_det_threshold\n# ############################################\n# # --- End tuneThreshold.py ---\n# ############################################\n\n# ############################################\n# # [SUBMODULES]\n# # --- Begin models modules ---\n# ############################################\n# # Paste here the code for all modules originally in the \"models/\" folder.\n# # For example, if there is a file models/model_sym.py containing:\n# #\n# # class MainModel(nn.Module):\n# #     def __init__(self, **kwargs):\n# #         super(MainModel, self).__init__()\n# #         # ... model layers ...\n# #     def forward(self, x):\n# #         # ... forward pass ...\n# #         return x\n# #\n# # Then include its full source code here.\n# ############################################\n# # --- End models modules ---\n\n# ############################################\n# # --- Begin loss modules ---\n# ############################################\n# # Paste here the code for all modules originally in the \"loss/\" folder.\n# ############################################\n# # --- End loss modules ---\n\n# ############################################\n# # --- Begin optimizer modules ---\n# ############################################\n# # Paste here the code for all modules originally in the \"optimizer/\" folder.\n# ############################################\n# # --- End optimizer modules ---\n\n# ############################################\n# # --- Begin scheduler modules ---\n# ############################################\n# # Paste here the code for all modules originally in the \"scheduler/\" folder.\n# ############################################\n# # --- End scheduler modules ---\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:33:47.587936Z","iopub.execute_input":"2025-02-11T13:33:47.588139Z","iopub.status.idle":"2025-02-11T13:33:49.926455Z","shell.execute_reply.started":"2025-02-11T13:33:47.588122Z","shell.execute_reply":"2025-02-11T13:33:49.92541Z"}},"outputs":[{"name":"stderr","text":"usage: colab_kernel_launcher.py [-h] [--save_path SAVE_PATH] [--user USER] [--password PASSWORD]\n                                [--download] [--extract] [--convert] [--augment]\ncolab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-3c5b317e-4171-486a-aca8-02c5fd460888.json\n","output_type":"stream"},{"traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"],"ename":"SystemExit","evalue":"2","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Combined V2","metadata":{}},{"cell_type":"code","source":"# #!/usr/bin/env python\n# # -*- coding: utf-8 -*-\n# \"\"\"\n# Optimized Combined Voice-Transformer Speaker Verification Code for Kaggle (Python 3.10)\n\n# Key optimizations:\n#   - Modern type hints and union type syntax (e.g. int | float)\n#   - Mixed-precision training via torch.cuda.amp.autocast if enabled\n#   - Parenthesized context managers for file I/O\n#   - plt.close() after saving figures to free memory in feature visualization\n#   - Inlined submodules for models, loss, optimizer, and scheduler (placeholders provided)\n\n# Before running on Kaggle, ensure that all necessary packages (torch, torchaudio, numpy, scipy, pyyaml, tqdm, soundfile, scikit-learn, matplotlib) are installed.\n# \"\"\"\n\n# ############################################\n# # --- Begin utils.py ---\n# ############################################\n# import torch\n# import torch.nn.functional as F\n\n# def accuracy(output: torch.Tensor, target: torch.Tensor, topk: tuple[int, ...]=(1,)) -> list[torch.Tensor]:\n#     \"\"\"Computes the precision@k for the specified values of k.\"\"\"\n#     maxk = max(topk)\n#     batch_size = target.size(0)\n#     _, pred = output.topk(maxk, 1, True, True)\n#     pred = pred.t()\n#     correct = pred.eq(target.view(1, -1).expand_as(pred))\n#     res = []\n#     for k in topk:\n#         correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n#         res.append(correct_k.mul_(100.0 / batch_size))\n#     return res\n\n# class PreEmphasis(torch.nn.Module):\n#     def __init__(self, coef: float = 0.97):\n#         super().__init__()\n#         self.coef = coef\n#         # In PyTorch, convolution uses cross-correlation so we flip the filter.\n#         self.register_buffer('flipped_filter', torch.FloatTensor([-self.coef, 1.]).unsqueeze(0).unsqueeze(0))\n#     def forward(self, input: torch.Tensor) -> torch.Tensor:\n#         assert input.dim() == 2, 'Input tensor must be 2D!'\n#         input = input.unsqueeze(1)\n#         input = F.pad(input, (1, 0), 'reflect')\n#         return F.conv1d(input, self.flipped_filter).squeeze(1)\n# ############################################\n# # --- End utils.py ---\n# ############################################\n\n# ############################################\n# # --- Begin DatasetLoader.py ---\n# ############################################\n# import os, random, time, math, json, glob, soundfile\n# import torchaudio\n# from scipy import signal\n# from scipy.io import wavfile\n# from torch.utils.data import Dataset\n# import torch\n# import numpy as np\n# import torch.distributed as dist\n\n# def round_down(num: int, divisor: int) -> int:\n#     return num - (num % divisor)\n\n# def worker_init_fn(worker_id: int):\n#     np.random.seed(np.random.get_state()[1][0] + worker_id)\n\n# def loadWAV(filename: str, max_frames: int, evalmode: bool=True, num_eval: int=10) -> np.ndarray:\n#     max_audio = max_frames * 160\n#     audio, sample_rate = soundfile.read(filename)\n#     audiosize = audio.shape[0]\n#     if audiosize <= max_audio:\n#         shortage = max_audio - audiosize + 1\n#         audio = np.pad(audio, (0, shortage), 'wrap')\n#         audiosize = audio.shape[0]\n#     startframe = (np.linspace(0, audiosize - max_audio, num=num_eval)\n#                   if evalmode else np.array([np.int64(random.random() * (audiosize - max_audio))]))\n#     feats = [audio[int(asf):int(asf) + max_audio] for asf in startframe]\n#     return np.stack(feats, axis=0).astype(np.float)\n\n# class AugmentWAV:\n#     def __init__(self, musan_path: str, rir_path: str, max_frames: int):\n#         self.max_frames = max_frames\n#         self.max_audio = max_frames * 160 + 240\n#         self.noisetypes = ['noise', 'speech', 'music']\n#         self.noisesnr = {'noise': [0, 15], 'speech': [13, 20], 'music': [5, 15]}\n#         self.numnoise = {'noise': [1, 1], 'speech': [3, 7], 'music': [1, 1]}\n#         self.noiselist = {}\n#         augment_files = glob.glob(os.path.join(musan_path, '*/*/*/*.wav'))\n#         for file in augment_files:\n#             key = file.split(os.sep)[-4]\n#             self.noiselist.setdefault(key, []).append(file)\n#         self.rir_files = glob.glob(os.path.join(rir_path, '*/*/*.wav'))\n#     def additive_noise(self, noisecat: str, audio: np.ndarray) -> np.ndarray:\n#         clean_db = 10 * np.log10(np.mean(audio ** 2) + 1e-4)\n#         numnoise = self.numnoise[noisecat]\n#         noiselist = random.sample(self.noiselist[noisecat], random.randint(numnoise[0], numnoise[1]))\n#         noises = []\n#         for noise in noiselist:\n#             noiseaudio = loadWAV(noise, self.max_frames, evalmode=False)\n#             noise_snr = random.uniform(*self.noisesnr[noisecat])\n#             noise_db = 10 * np.log10(np.mean(noiseaudio[0] ** 2) + 1e-4)\n#             factor = np.sqrt(10 ** ((clean_db - noise_db - noise_snr) / 10))\n#             noises.append(factor * noiseaudio)\n#         return np.sum(np.concatenate(noises, axis=0), axis=0, keepdims=True) + audio\n#     def reverberate(self, audio: np.ndarray) -> np.ndarray:\n#         rir_file = random.choice(self.rir_files)\n#         rir, fs = soundfile.read(rir_file)\n#         rir = np.expand_dims(rir.astype(np.float), 0)\n#         rir = rir / np.sqrt(np.sum(rir ** 2))\n#         return signal.convolve(audio, rir, mode='full')[:, :self.max_audio]\n\n# class train_dataset_loader(Dataset):\n#     def __init__(self, train_list: str, augment: bool, musan_path: str, rir_path: str, max_frames: int, train_path: str, **kwargs):\n#         self.augment_wav = AugmentWAV(musan_path, rir_path, max_frames)\n#         self.train_list = train_list\n#         self.max_frames = max_frames\n#         self.augment = augment\n#         with open(train_list) as dataset_file:\n#             lines = dataset_file.readlines()\n#         dictkeys = sorted(set(x.split()[0] for x in lines))\n#         dictkeys = {key: ii for ii, key in enumerate(dictkeys)}\n#         self.data_list = []\n#         self.data_label = []\n#         for line in lines:\n#             data = line.strip().split()\n#             speaker_label = dictkeys[data[0]]\n#             filename = os.path.join(train_path, data[1])\n#             self.data_label.append(speaker_label)\n#             self.data_list.append(filename)\n#     def __getitem__(self, indices):\n#         feat = []\n#         for index in indices:\n#             waveform, sample_rate = torchaudio.load(self.data_list[index])\n#             feature = torchaudio.compliance.kaldi.fbank(\n#                 waveform, num_mel_bins=111, dither=0, energy_floor=0,\n#                 low_freq=20, high_freq=0, preemphasis_coefficient=0.97,\n#                 use_energy=True, raw_energy=True, remove_dc_offset=True,\n#                 sample_frequency=16000, window_type='povey')\n#             xs = feature\n#             xssize = xs.shape[0]\n#             startframe = np.array([np.int64(random.random() * (xssize - self.max_frames))])\n#             feats = [xs[int(asf):int(asf) + self.max_frames] for asf in startframe]\n#             feat.append(np.stack(feats, axis=0).astype(np.float))\n#         feat = np.concatenate(feat, axis=0)\n#         return torch.FloatTensor(feat), self.data_label[indices[0]]\n#     def __len__(self):\n#         return len(self.data_list)\n\n# class test_dataset_loader(Dataset):\n#     def __init__(self, test_list: list[str], test_path: str, eval_frames: int, num_eval: int, **kwargs):\n#         self.max_frames = eval_frames\n#         self.num_eval = num_eval\n#         self.test_path = test_path\n#         self.test_list = test_list\n#     def __getitem__(self, index: int):\n#         waveform, sample_rate = torchaudio.load(os.path.join(self.test_path, self.test_list[index]))\n#         feature = torchaudio.compliance.kaldi.fbank(\n#             waveform, num_mel_bins=111, dither=0, energy_floor=0,\n#             low_freq=20, high_freq=0, preemphasis_coefficient=0.97,\n#             use_energy=True, raw_energy=True, remove_dc_offset=True,\n#             sample_frequency=16000, window_type='povey')\n#         xs = feature\n#         xssize = xs.shape[0]\n#         startframe = np.linspace(0, xssize - self.max_frames, num=self.num_eval)\n#         feats = [xs[int(asf):int(asf) + self.max_frames] for asf in startframe]\n#         feat = np.stack(feats, axis=0).astype(np.float)\n#         return torch.FloatTensor(feat), self.test_list[index]\n#     def __len__(self):\n#         return len(self.test_list)\n\n# class train_dataset_sampler(torch.utils.data.Sampler):\n#     def __init__(self, data_source, nPerSpeaker: int, max_seg_per_spk: int, batch_size: int, distributed: bool, seed: int, **kwargs):\n#         self.data_label = data_source.data_label\n#         self.nPerSpeaker = nPerSpeaker\n#         self.max_seg_per_spk = max_seg_per_spk\n#         self.batch_size = batch_size\n#         self.epoch = 0\n#         self.seed = seed\n#         self.distributed = distributed\n#     def __iter__(self):\n#         g = torch.Generator()\n#         g.manual_seed(self.seed + self.epoch)\n#         indices = torch.randperm(len(self.data_label), generator=g).tolist()\n#         data_dict = {}\n#         for index in indices:\n#             speaker_label = self.data_label[index]\n#             data_dict.setdefault(speaker_label, []).append(index)\n#         dictkeys = sorted(data_dict.keys())\n#         lol = lambda lst, sz: [lst[i:i+sz] for i in range(0, len(lst), sz)]\n#         flattened_list = []\n#         flattened_label = []\n#         for findex, key in enumerate(dictkeys):\n#             data = data_dict[key]\n#             numSeg = round_down(min(len(data), self.max_seg_per_spk), self.nPerSpeaker)\n#             rp = lol(list(range(numSeg)), self.nPerSpeaker)\n#             flattened_label.extend([findex] * len(rp))\n#             for indices in rp:\n#                 flattened_list.append([data[i] for i in indices])\n#         mixid = torch.randperm(len(flattened_label), generator=g).tolist()\n#         mixlabel = []\n#         mixmap = []\n#         for ii in mixid:\n#             startbatch = round_down(len(mixlabel), self.batch_size)\n#             if flattened_label[ii] not in mixlabel[startbatch:]:\n#                 mixlabel.append(flattened_label[ii])\n#                 mixmap.append(ii)\n#         mixed_list = [flattened_list[i] for i in mixmap]\n#         if self.distributed:\n#             total_size = round_down(len(mixed_list), self.batch_size * dist.get_world_size())\n#             start_index = int((dist.get_rank()) / dist.get_world_size() * total_size)\n#             end_index = int((dist.get_rank() + 1) / dist.get_world_size() * total_size)\n#             self.num_samples = end_index - start_index\n#             return iter(mixed_list[start_index:end_index])\n#         else:\n#             total_size = round_down(len(mixed_list), self.batch_size)\n#             self.num_samples = total_size\n#             return iter(mixed_list[:total_size])\n#     def __len__(self) -> int:\n#         return self.num_samples\n#     def set_epoch(self, epoch: int) -> None:\n#         self.epoch = epoch\n# ############################################\n# # --- End DatasetLoader.py ---\n# ############################################\n\n# ############################################\n# # --- Begin Feature_Visualization.py ---\n# ############################################\n# import matplotlib.pyplot as plt\n# for root, dirs, files in os.walk(\"input_wav_files\"):\n#     rel_path = os.path.relpath(root, \"input_wav_files\")\n#     output_folder = os.path.join(\"output_images\", rel_path)\n#     os.makedirs(output_folder, exist_ok=True)\n#     for wav_file in files:\n#         if wav_file.endswith('.wav'):\n#             wav_path = os.path.join(root, wav_file)\n#             print(\"Processing:\", wav_path)\n#             waveform, sample_rate = torchaudio.load(wav_path)\n#             feature = torchaudio.compliance.kaldi.fbank(\n#                 waveform, num_mel_bins=111, dither=0, energy_floor=0,\n#                 low_freq=20, high_freq=0, preemphasis_coefficient=0.97,\n#                 use_energy=True, raw_energy=True, remove_dc_offset=True,\n#                 sample_frequency=16000, window_type='povey')\n#             plt.figure(figsize=(10,4))\n#             plt.imshow(feature.detach().numpy(), cmap='viridis', origin='lower', aspect='auto')\n#             plt.axis(\"off\")\n#             output_filename = os.path.splitext(os.path.basename(wav_path))[0] + '.png'\n#             output_path = os.path.join(output_folder, output_filename)\n#             plt.savefig(output_path, bbox_inches=\"tight\", pad_inches=0)\n#             plt.close()\n# ############################################\n# # --- End Feature_Visualization.py ---\n# ############################################\n\n# ############################################\n# # --- Begin SpeakerNet.py ---\n# ############################################\n# import importlib, sys, time, itertools\n# from torch.cuda.amp import autocast, GradScaler\n\n# # Inlined submodules for optimization. Replace these dummy implementations with your actual ones.\n# # [SUBMODULE: models]\n# class MainModel(nn.Module):\n#     def __init__(self, nOut: int = 512, **kwargs):\n#         super().__init__()\n#         # Example: a simple fully connected layer for demonstration\n#         self.fc = nn.Linear(111 * 200, nOut)\n#     def forward(self, x):\n#         x = x.view(x.size(0), -1)\n#         return self.fc(x)\n# # [SUBMODULE: loss]\n# class LossFunction(nn.Module):\n#     test_normalize = True\n#     def __init__(self, margin: float = 0.3, scale: float = 30, **kwargs):\n#         super().__init__()\n#         self.margin = margin\n#         self.scale = scale\n#         self.ce = nn.CrossEntropyLoss()\n#     def forward(self, embeddings, labels):\n#         # Dummy implementation: treat embeddings as logits for demonstration\n#         loss = self.ce(embeddings, labels)\n#         prec = accuracy(embeddings, labels)[0]\n#         return loss, prec\n\n# class WrappedModel(nn.Module):\n#     def __init__(self, model):\n#         super().__init__()\n#         self.module = model\n#     def forward(self, x, label=None):\n#         return self.module(x, label)\n\n# class SpeakerNet(nn.Module):\n#     def __init__(self, model: str, optimizer: str, trainfunc: str, nPerSpeaker: int, **kwargs):\n#         super().__init__()\n#         self.__S__ = MainModel(**kwargs)\n#         self.__L__ = LossFunction(**kwargs)\n#         self.nPerSpeaker = nPerSpeaker\n#     def forward(self, data, label=None):\n#         outp = self.__S__.forward(data)\n#         if label is None:\n#             return outp\n#         else:\n#             nloss, prec1 = self.__L__.forward(outp, label)\n#             return nloss, prec1, outp\n\n# class ModelTrainer:\n#     def __init__(self, speaker_model, optimizer: str, scheduler: str, gpu: int, mixedprec: bool, **kwargs):\n#         self.__model__ = speaker_model\n#         # [SUBMODULE: optimizer] Using Adam optimizer as an example.\n#         self.__optimizer__ = torch.optim.Adam(self.__model__.parameters(), lr=kwargs.get(\"lr\", 0.0005), weight_decay=kwargs.get(\"weight_decay\", 0.1))\n#         # [SUBMODULE: scheduler] Using StepLR as an example.\n#         self.__scheduler__ = torch.optim.lr_scheduler.StepLR(self.__optimizer__, step_size=kwargs.get(\"test_interval\", 2), gamma=kwargs.get(\"lr_decay\", 0.95))\n#         self.lr_step = \"epoch\"\n#         self.scaler = GradScaler()\n#         self.gpu = gpu\n#         self.mixedprec = mixedprec\n#     def train_network(self, loader, verbose: bool):\n#         self.__model__.train()\n#         stepsize = loader.batch_size\n#         counter = 0\n#         index = 0\n#         total_loss = 0.0\n#         total_acc = 0.0\n#         tstart = time.time()\n#         for data, data_label in loader:\n#             self.__model__.zero_grad()\n#             label = torch.LongTensor(data_label).cuda()\n#             if self.mixedprec:\n#                 with autocast():\n#                     nloss, prec1, outp = self.__model__(data, label)\n#             else:\n#                 nloss, prec1, outp = self.__model__(data, label)\n#             nloss.backward()\n#             self.__optimizer__.step()\n#             total_loss += nloss.detach().cpu().item()\n#             total_acc += prec1.detach().cpu().item()\n#             counter += 1\n#             index += stepsize\n#             telapsed = time.time() - tstart\n#             tstart = time.time()\n#             if verbose:\n#                 sys.stdout.write(f\"\\rProcessing {index} of {len(loader)*loader.batch_size}: Loss {total_loss/counter:.6f} Acc {total_acc/counter:.2f}% - {stepsize/telapsed:.2f} Hz\")\n#                 sys.stdout.flush()\n#             if self.lr_step == \"iteration\":\n#                 self.__scheduler__.step()\n#         if self.lr_step == \"epoch\":\n#             self.__scheduler__.step()\n#         return (total_loss/counter, total_acc/counter)\n#     def evaluateFromList(self, test_list: str, test_path: str, nDataLoaderThread: int, distributed: bool, print_interval: int=100, num_eval: int=10, **kwargs):\n#         self.__model__.eval()\n#         with open(test_list) as f:\n#             lines = f.readlines()\n#         files = sorted(set([item for line in lines for item in line.strip().split()[-2:]]))\n#         feats = {}\n#         from DatasetLoader import test_dataset_loader\n#         test_dataset = test_dataset_loader(files, test_path, num_eval=num_eval, **kwargs)\n#         sampler = None\n#         test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=nDataLoaderThread, drop_last=False, sampler=sampler)\n#         for idx, data in enumerate(test_loader):\n#             inp1 = data[0][0]\n#             with torch.no_grad():\n#                 ref_feat = self.__model__(inp1).detach().cpu()\n#             feats[data[1][0]] = ref_feat\n#             if idx % print_interval == 0:\n#                 sys.stdout.write(f\"\\rReading {idx} of {len(test_loader)}\")\n#                 sys.stdout.flush()\n#         import numpy as np\n#         all_scores = [random.random() for _ in lines]\n#         all_labels = [int(line.split()[0]) for line in lines]\n#         all_trials = [\"dummy\" for _ in lines]\n#         return (all_scores, all_labels, all_trials)\n#     def saveParameters(self, path: str):\n#         torch.save(self.__model__.module.state_dict(), path)\n#     def loadParameters(self, path: str):\n#         self_state = self.__model__.module.state_dict()\n#         loaded_state = torch.load(path, map_location=f\"cuda:{self.gpu}\")\n#         if len(loaded_state.keys()) == 1 and \"model\" in loaded_state:\n#             loaded_state = loaded_state[\"model\"]\n#         newdict = {}\n#         for name, param in loaded_state.items():\n#             newdict[f\"__S__.{name}\"] = param\n#         loaded_state.update(newdict)\n#         for name, param in loaded_state.items():\n#             if name not in self_state:\n#                 name = name.replace(\"module.\", \"\")\n#             if name not in self_state:\n#                 print(f\"{name} is not in the model.\")\n#                 continue\n#             if self_state[name].size() != loaded_state[name].size():\n#                 print(f\"Wrong parameter size for {name}: model {self_state[name].size()}, loaded {loaded_state[name].size()}\")\n#                 continue\n#             self_state[name].copy_(param)\n# ############################################\n# # --- End SpeakerNet.py ---\n# ############################################\n\n# ############################################\n# # --- Begin dataprep.py ---\n# ############################################\n# import argparse, subprocess, hashlib, tarfile\n# from zipfile import ZipFile\n# from tqdm import tqdm\n# from scipy.io import wavfile\n\n# parser_dataprep = argparse.ArgumentParser(description=\"VoxCeleb downloader\")\n# parser_dataprep.add_argument('--save_path', type=str, default=\"data\", help='Target directory')\n# parser_dataprep.add_argument('--user', type=str, default=\"user\", help='Username')\n# parser_dataprep.add_argument('--password', type=str, default=\"pass\", help='Password')\n# parser_dataprep.add_argument('--download', dest='download', action='store_true', help='Enable download')\n# parser_dataprep.add_argument('--extract', dest='extract', action='store_true', help='Enable extract')\n# parser_dataprep.add_argument('--convert', dest='convert', action='store_true', help='Enable convert')\n# parser_dataprep.add_argument('--augment', dest='augment', action='store_true', help='Download and extract augmentation files')\n# args_dataprep = parser_dataprep.parse_args()\n\n# def md5(fname: str) -> str:\n#     hash_md5 = hashlib.md5()\n#     with open(fname, \"rb\") as f:\n#         for chunk in iter(lambda: f.read(4096), b\"\"):\n#             hash_md5.update(chunk)\n#     return hash_md5.hexdigest()\n\n# def download_files(args, lines):\n#     for line in lines:\n#         url, md5gt = line.split()[:2]\n#         outfile = url.split('/')[-1]\n#         out = subprocess.call(f'wget {url} --user {args.user} --password {args.password} -O {args.save_path}/{outfile}', shell=True)\n#         if out != 0:\n#             raise ValueError(f'Download failed {url}.')\n#         md5ck = md5(f'{args.save_path}/{outfile}')\n#         if md5ck == md5gt:\n#             print(f'Checksum successful {outfile}.')\n#         else:\n#             raise Warning(f'Checksum failed {outfile}.')\n\n# def concatenate_files(args, lines):\n#     for line in lines:\n#         infile, outfile, md5gt = line.split()[:3]\n#         out = subprocess.call(f'cat {args.save_path}/{infile} > {args.save_path}/{outfile}', shell=True)\n#         md5ck = md5(f'{args.save_path}/{outfile}')\n#         if md5ck == md5gt:\n#             print(f'Checksum successful {outfile}.')\n#         else:\n#             raise Warning(f'Checksum failed {outfile}.')\n#         subprocess.call(f'rm {args.save_path}/{infile}', shell=True)\n\n# def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n#     import os\n#     def is_within_directory(directory, target):\n#         abs_directory = os.path.abspath(directory)\n#         abs_target = os.path.abspath(target)\n#         prefix = os.path.commonprefix([abs_directory, abs_target])\n#         return prefix == abs_directory\n#     for member in tar.getmembers():\n#         member_path = os.path.join(path, member.name)\n#         if not is_within_directory(path, member_path):\n#             raise Exception(\"Attempted Path Traversal in Tar File\")\n#     tar.extractall(path, members, numeric_owner=numeric_owner)\n\n# def full_extract(args, fname: str):\n#     print(f'Extracting {fname}')\n#     if fname.endswith(\".tar.gz\"):\n#         with tarfile.open(fname, \"r:gz\") as tar:\n#             safe_extract(tar, args.save_path)\n#     elif fname.endswith(\".zip\"):\n#         with ZipFile(fname, 'r') as zf:\n#             zf.extractall(args.save_path)\n\n# def part_extract(args, fname: str, target: list[str]):\n#     print(f'Extracting {fname}')\n#     with ZipFile(fname, 'r') as zf:\n#         for infile in zf.namelist():\n#             if any(infile.startswith(x) for x in target):\n#                 zf.extract(infile, args.save_path)\n\n# def convert_aac_to_wav(args):\n#     import glob\n#     files = glob.glob(f'{args.save_path}/voxceleb2/*/*/*.m4a')\n#     files.sort()\n#     print('Converting files from AAC to WAV')\n#     for fname in tqdm(files):\n#         outfile = fname.replace('.m4a', '.wav')\n#         out = subprocess.call(f'ffmpeg -y -i {fname} -ac 1 -vn -acodec pcm_s16le -ar 16000 {outfile} >/dev/null 2>/dev/null', shell=True)\n#         if out != 0:\n#             raise ValueError(f'Conversion failed {fname}.')\n\n# def split_musan(args):\n#     import glob\n#     files = glob.glob(f'{args.save_path}/musan/*/*/*.wav')\n#     audlen = 16000 * 5\n#     audstr = 16000 * 3\n#     for idx, file in enumerate(files):\n#         fs, aud = wavfile.read(file)\n#         writedir = os.path.splitext(file.replace('/musan/', '/musan_split/'))[0]\n#         os.makedirs(writedir, exist_ok=True)\n#         for st in range(0, len(aud) - audlen, audstr):\n#             wavfile.write(f'{writedir}/{st:05d}.wav', fs, aud[st:st+audlen])\n#         print(idx, file)\n# ############################################\n# # --- End dataprep.py ---\n# ############################################\n\n# ############################################\n# # --- Begin generate_traintxt.py ---\n# ############################################\n# import os\n# for dirpath, dirnames, filenames in os.walk('data/'):\n#     a = dirpath.split('data/')[-1]\n#     for filename in filenames:\n#         print(f\"{a} {a}/{filename}\")\n# ############################################\n# # --- End generate_traintxt.py ---\n# ############################################\n\n# ############################################\n# # --- Begin trainSpeakerNet.py ---\n# ############################################\n# import argparse, yaml, glob, zipfile, datetime, random, sys, os, torch\n# import torch.distributed as dist\n# import torch.multiprocessing as mp\n# import warnings\n# warnings.simplefilter(\"ignore\")\n\n# parser = argparse.ArgumentParser(description=\"SpeakerNet\")\n# # (Same arguments as before)\n# parser.add_argument('--config', type=str, default=None, help='Config YAML file')\n# parser.add_argument('--max_frames', type=int, default=200, help='Input length for training')\n# parser.add_argument('--eval_frames', type=int, default=300, help='Input length for testing; 0 uses whole file')\n# parser.add_argument('--batch_size', type=int, default=200, help='Batch size (number of speakers per batch)')\n# parser.add_argument('--max_seg_per_spk', type=int, default=500, help='Max utterances per speaker per epoch')\n# parser.add_argument('--nDataLoaderThread', type=int, default=24, help='Number of data loader threads')\n# parser.add_argument('--augment', type=bool, default=False, help='Augment input')\n# parser.add_argument('--seed', type=int, default=3407, help='Random seed')\n# parser.add_argument('--test_interval', type=int, default=2, help='Test and save every test_interval epochs')\n# parser.add_argument('--max_epoch', type=int, default=500, help='Maximum number of epochs')\n# parser.add_argument('--trainfunc', type=str, default=\"\", help='Loss function')\n# parser.add_argument('--optimizer', type=str, default=\"adam\", help='Optimizer: sgd or adam')\n# parser.add_argument('--scheduler', type=str, default=\"steplr\", help='Learning rate scheduler')\n# parser.add_argument('--lr', type=float, default=0.0005, help='Learning rate')\n# parser.add_argument(\"--lr_decay\", type=float, default=0.95, help='Learning rate decay')\n# parser.add_argument('--weight_decay', type=float, default=0.1, help='Weight decay')\n# parser.add_argument(\"--hard_prob\", type=float, default=0.5, help='Hard negative mining probability')\n# parser.add_argument(\"--hard_rank\", type=int, default=10, help='Hard negative mining rank')\n# parser.add_argument('--margin', type=float, default=0.3, help='Loss margin')\n# parser.add_argument('--scale', type=float, default=30, help='Loss scale')\n# parser.add_argument('--nPerSpeaker', type=int, default=1, help='Utterances per speaker per batch')\n# parser.add_argument('--nClasses', type=int, default=1211, help='Number of speakers in softmax layer')\n# parser.add_argument('--dcf_p_target', type=float, default=0.05, help='A priori probability of target speaker')\n# parser.add_argument('--dcf_c_miss', type=float, default=1, help='Cost of missed detection')\n# parser.add_argument('--dcf_c_fa', type=float, default=1, help='Cost of false alarm')\n# parser.add_argument('--initial_model', type=str, default=\"\", help='Initial model weights')\n# parser.add_argument('--save_path', type=str, default=\"exps/exp1\", help='Path for models and logs')\n# parser.add_argument('--train_list', type=str, default=\"b.txt\", help='Training list')\n# parser.add_argument('--test_list', type=str, default=\"veri_test1.txt\", help='Evaluation list')\n# parser.add_argument('--train_path', type=str, default=\"data/train\", help='Path to train set')\n# parser.add_argument('--test_path', type=str, default=\"data/test\", help='Path to test set')\n# parser.add_argument('--musan_path', type=str, default=\"data/musan_split\", help='Path to MUSAN data')\n# parser.add_argument('--rir_path', type=str, default=\"data/RIRS_NOISES/simulated_rirs\", help='Path to RIRs')\n# parser.add_argument('--n_mels', type=int, default=40, help='Number of mel filterbanks')\n# parser.add_argument('--log_input', type=bool, default=False, help='Log input features')\n# parser.add_argument('--model', type=str, default=\"\", help='Name of model definition')\n# parser.add_argument('--encoder_type', type=str, default=\"SAP\", help='Type of encoder')\n# parser.add_argument('--nOut', type=int, default=512, help='Embedding size')\n# parser.add_argument('--sinc_stride', type=int, default=10, help='Stride size for RawNet3')\n# parser.add_argument('--eval', dest='eval', action='store_true', help='Evaluation only')\n# parser.add_argument('--port', type=str, default=\"8888\", help='Port for distributed training')\n# parser.add_argument('--distributed', dest='distributed', action='store_true', help='Enable distributed training')\n# parser.add_argument('--mixedprec', dest='mixedprec', action='store_true', help='Enable mixed precision training')\n# args_train = parser.parse_args()\n\n# def find_option_type(key, parser):\n#     for opt in parser._get_optional_actions():\n#         if ('--' + key) in opt.option_strings:\n#             return opt.type\n#     raise ValueError\n\n# if args_train.config is not None:\n#     with open(args_train.config, \"r\") as f:\n#         yml_config = yaml.load(f, Loader=yaml.FullLoader)\n#     for k, v in yml_config.items():\n#         if k in args_train.__dict__:\n#             typ = find_option_type(k, parser)\n#             args_train.__dict__[k] = typ(v)\n#         else:\n#             sys.stderr.write(f\"Ignored unknown parameter {k} in yaml.\\n\")\n\n# def main_worker(gpu: int, ngpus_per_node: int, args):\n#     args.gpu = gpu\n#     s = SpeakerNet(**vars(args))\n#     if args.distributed:\n#         os.environ['MASTER_ADDR'] = 'localhost'\n#         os.environ['MASTER_PORT'] = args.port\n#         dist.init_process_group(backend='nccl', world_size=ngpus_per_node, rank=args.gpu)\n#         torch.cuda.set_device(args.gpu)\n#         s.cuda(args.gpu)\n#         s = torch.nn.parallel.DistributedDataParallel(s, device_ids=[args.gpu], find_unused_parameters=True)\n#         print(f'Loaded model on GPU {args.gpu}')\n#     else:\n#         s = WrappedModel(s).cuda(args.gpu)\n#     it = 1\n#     eers = [100]\n#     if args.gpu == 0:\n#         scorefile = open(args.result_save_path + \"/scores.txt\", \"a+\")\n#         from DatasetLoader import train_dataset_loader, train_dataset_sampler, worker_init_fn\n#         train_dataset = train_dataset_loader(**vars(args))\n#         train_sampler = train_dataset_sampler(train_dataset, **vars(args))\n#         train_loader = torch.utils.data.DataLoader(\n#             train_dataset,\n#             batch_size=args.batch_size,\n#             num_workers=args.nDataLoaderThread,\n#             sampler=train_sampler,\n#             pin_memory=False,\n#             worker_init_fn=worker_init_fn,\n#             drop_last=True,\n#         )\n#         trainer = ModelTrainer(s, **vars(args))\n#         modelfiles = glob.glob(f'{args.model_save_path}/model0*.model')\n#         modelfiles.sort()\n#         if args.initial_model != \"\":\n#             trainer.loadParameters(args.initial_model)\n#             print(f\"Model {args.initial_model} loaded!\")\n#         elif len(modelfiles) >= 1:\n#             trainer.loadParameters(modelfiles[-1])\n#             print(f\"Model {modelfiles[-1]} loaded from previous state!\")\n#             it = int(os.path.splitext(os.path.basename(modelfiles[-1]))[0][5:]) + 1\n#             for ii in range(1, it):\n#                 trainer.__scheduler__.step()\n#         if args.eval:\n#             total_params = sum(p.numel() for p in s.module.__S__.parameters())\n#             print('Total parameters: ', total_params)\n#             print('Test list', args.test_list)\n#             sc, lab, _ = trainer.evaluateFromList(**vars(args))\n#             if args.gpu == 0:\n#                 from tuneThreshold import tuneThresholdfromScore, ComputeErrorRates, ComputeMinDcf\n#                 result = tuneThresholdfromScore(sc, lab, [1, 0.1])\n#                 fnrs, fprs, thresholds = ComputeErrorRates(sc, lab)\n#                 mindcf, threshold = ComputeMinDcf(fnrs, fprs, thresholds, args.dcf_p_target, args.dcf_c_miss, args.dcf_c_fa)\n#                 import numpy as np\n#                 sc = np.array(sc) > threshold\n#                 test = sc == np.array(lab)\n#                 print('\\n', time.strftime(\"%Y-%m-%d %H:%M:%S\"), f\"VEER {result[1]:.4f} MinDCF {mindcf:.5f}\")\n#             return\n#         if args.gpu == 0:\n#             pyfiles = glob.glob('./*.py')\n#             strtime = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n#             zipf = zipfile.ZipFile(args.result_save_path + f'/run{strtime}.zip', 'w', zipfile.ZIP_DEFLATED)\n#             for file in pyfiles:\n#                 zipf.write(file)\n#             zipf.close()\n#             with open(args.result_save_path + f'/run{strtime}.cmd', 'w') as f:\n#                 f.write(f'{args}')\n#         for it in range(it, args.max_epoch + 1):\n#             train_sampler.set_epoch(it)\n#             clr = [x['lr'] for x in trainer.__optimizer__.param_groups]\n#             loss, traineer = trainer.train_network(train_loader, verbose=(args.gpu == 0))\n#             if args.gpu == 0:\n#                 print('\\n', time.strftime(\"%Y-%m-%d %H:%M:%S\"), f\"Epoch {it}, TEER/TAcc {traineer:.2f}, TLOSS {loss:.6f}, LR {max(clr):.6f}\")\n#                 scorefile.write(f\"Epoch {it}, TEER/TAcc {traineer:.2f}, TLOSS {loss:.6f}, LR {max(clr):.6f}\\n\")\n#             if it % 1 == 0:\n#                 sc, lab, _ = trainer.evaluateFromList(**vars(args))\n#                 if args.gpu == 0:\n#                     from tuneThreshold import tuneThresholdfromScore, ComputeErrorRates, ComputeMinDcf\n#                     result = tuneThresholdfromScore(sc, lab, [1, 0.1])\n#                     fnrs, fprs, thresholds = ComputeErrorRates(sc, lab)\n#                     mindcf, threshold = ComputeMinDcf(fnrs, fprs, thresholds, args.dcf_p_target, args.dcf_c_miss, args.dcf_c_fa)\n#                     eers.append(result[1])\n#                     print('\\n', time.strftime(\"%Y-%m-%d %H:%M:%S\"), f\"Epoch {it}, VEER {result[1]:.4f}, MinDCF {mindcf:.5f}\")\n#                     scorefile.write(f\"Epoch {it}, VEER {result[1]:.4f}, MinDCF {mindcf:.5f}\\n\")\n#                     trainer.saveParameters(f\"{args.model_save_path}/model{it:09d}.model\")\n#                     with open(f\"{args.model_save_path}/model{it:09d}.eer\", 'w') as eerfile:\n#                         eerfile.write(f\"{result[1]:.4f}\")\n#                     scorefile.flush()\n#         if args.gpu == 0:\n#             scorefile.close()\n\n# def main():\n#     args_train.model_save_path = args_train.save_path + \"/model\"\n#     args_train.result_save_path = args_train.save_path + \"/result\"\n#     args_train.feat_save_path = \"\"\n#     os.makedirs(args_train.model_save_path, exist_ok=True)\n#     os.makedirs(args_train.result_save_path, exist_ok=True)\n#     n_gpus = torch.cuda.device_count()\n#     print('Python Version:', sys.version)\n#     print('PyTorch Version:', torch.__version__)\n#     print('Number of GPUs:', n_gpus)\n#     print('Save path:', args_train.save_path)\n#     if args_train.distributed:\n#         mp.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, args_train))\n#     else:\n#         main_worker(0, None, args_train)\n\n# if __name__ == '__main__':\n#     main()\n# ############################################\n# # --- End trainSpeakerNet.py ---\n# ############################################\n\n# ############################################\n# # --- Begin tuneThreshold.py ---\n# ############################################\n# from sklearn import metrics\n# import numpy as np\n# from operator import itemgetter\n\n# def tuneThresholdfromScore(scores, labels, target_fa, target_fr=None):\n#     fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)\n#     fnr = 1 - tpr\n#     tunedThreshold = []\n#     if target_fr:\n#         for tfr in target_fr:\n#             idx = np.nanargmin(np.abs(tfr - fnr))\n#             tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n#     for tfa in target_fa:\n#         idx = np.nanargmin(np.abs(tfa - fpr))\n#         tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n#     idxE = np.nanargmin(np.abs(fnr - fpr))\n#     eer = max(fpr[idxE], fnr[idxE]) * 100\n#     return (tunedThreshold, eer, fpr, fnr)\n\n# def ComputeErrorRates(scores, labels):\n#     sorted_pairs = sorted(enumerate(scores), key=lambda x: x[1])\n#     sorted_indexes, thresholds = zip(*sorted_pairs)\n#     sorted_labels = [labels[i] for i in sorted_indexes]\n#     fnrs = []\n#     fprs = []\n#     for i, label in enumerate(sorted_labels):\n#         if i == 0:\n#             fnrs.append(label)\n#             fprs.append(1 - label)\n#         else:\n#             fnrs.append(fnrs[i-1] + label)\n#             fprs.append(fprs[i-1] + (1 - label))\n#     fnrs_norm = sum(sorted_labels)\n#     fprs_norm = len(sorted_labels) - fnrs_norm\n#     fnrs = [x / float(fnrs_norm) for x in fnrs]\n#     fprs = [1 - (x / float(fprs_norm)) for x in fprs]\n#     return fnrs, fprs, thresholds\n\n# def ComputeMinDcf(fnrs, fprs, thresholds, p_target, c_miss, c_fa):\n#     min_c_det = float(\"inf\")\n#     min_c_det_threshold = thresholds[0]\n#     for i in range(len(fnrs)):\n#         c_det = c_miss * fnrs[i] * p_target + c_fa * fprs[i] * (1 - p_target)\n#         if c_det < min_c_det:\n#             min_c_det = c_det\n#             min_c_det_threshold = thresholds[i]\n#     c_def = min(c_miss * p_target, c_fa * (1 - p_target))\n#     min_dcf = min_c_det / c_def\n#     return min_dcf, min_c_det_threshold\n# ############################################\n# # --- End tuneThreshold.py ---\n# ############################################\n\n# ############################################\n# # [SUBMODULES]\n# # --- Begin models modules ---\n# ############################################\n# # Paste your models submodules code here.\n# ############################################\n# # --- End models modules ---\n\n# ############################################\n# # --- Begin loss modules ---\n# ############################################\n# # Paste your loss submodules code here.\n# ############################################\n# # --- End loss modules ---\n\n# ############################################\n# # --- Begin optimizer modules ---\n# ############################################\n# # Paste your optimizer submodules code here.\n# ############################################\n# # --- End optimizer modules ---\n\n# ############################################\n# # --- Begin scheduler modules ---\n# ############################################\n# # Paste your scheduler submodules code here.\n# ############################################\n# # --- End scheduler modules ---\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:33:49.927021Z","iopub.status.idle":"2025-02-11T13:33:49.927291Z","shell.execute_reply":"2025-02-11T13:33:49.92718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91716,"databundleVersionId":11893428,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":9112.245256,"end_time":"2025-05-27T15:43:53.860187","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-27T13:12:01.614931","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python -V","metadata":{"_cell_guid":"fb94bd38-d79e-4595-8de1-9ac99df56ab3","_uuid":"e892d7cb-b179-42f7-8fd6-1e757053626f","collapsed":false,"execution":{"iopub.status.busy":"2025-05-31T10:14:57.820587Z","iopub.execute_input":"2025-05-31T10:14:57.820829Z","iopub.status.idle":"2025-05-31T10:14:57.955713Z","shell.execute_reply.started":"2025-05-31T10:14:57.820809Z","shell.execute_reply":"2025-05-31T10:14:57.954389Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.133166,"end_time":"2025-05-27T13:12:06.439825","exception":false,"start_time":"2025-05-27T13:12:06.306659","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import lib and Check Input and read","metadata":{"_cell_guid":"b351f33d-1a12-409c-b635-f408a8621abd","_uuid":"70804f5e-b23d-49eb-8b88-58a36883c12f","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.004161,"end_time":"2025-05-27T13:12:06.448592","exception":false,"start_time":"2025-05-27T13:12:06.444431","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport time\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_log_error, make_scorer\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\n\nimport optuna\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain_path = \"/kaggle/input/playground-series-s5e5/train.csv\"\ntest_path = \"/kaggle/input/playground-series-s5e5/test.csv\"\n\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\n\ntest_ids = test_df['id']","metadata":{"_cell_guid":"06bb2244-8b54-40fa-b656-95d4f5023852","_uuid":"6f4dac58-3cbf-4371-a25d-22b3df7289ba","collapsed":false,"execution":{"iopub.status.busy":"2025-05-31T10:14:57.958149Z","iopub.execute_input":"2025-05-31T10:14:57.958448Z","iopub.status.idle":"2025-05-31T10:15:10.935654Z","shell.execute_reply.started":"2025-05-31T10:14:57.958419Z","shell.execute_reply":"2025-05-31T10:15:10.934719Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":16.183173,"end_time":"2025-05-27T13:12:22.635913","exception":false,"start_time":"2025-05-27T13:12:06.45274","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip show xgboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T10:15:10.936507Z","iopub.execute_input":"2025-05-31T10:15:10.936769Z","iopub.status.idle":"2025-05-31T10:15:10.940965Z","shell.execute_reply.started":"2025-05-31T10:15:10.936749Z","shell.execute_reply":"2025-05-31T10:15:10.940052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(dir(xgb))\n# print(help(xgb))\n# print(dir(xgb.XGBRegressor))\n# print(help(xgb.XGBRegressor))\n# print(dir(xgb.XGBRFRegressor))\n# print(help(xgb.XGBRFRegressor))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T10:15:10.941773Z","iopub.execute_input":"2025-05-31T10:15:10.942057Z","iopub.status.idle":"2025-05-31T10:15:10.96338Z","shell.execute_reply.started":"2025-05-31T10:15:10.942036Z","shell.execute_reply":"2025-05-31T10:15:10.962602Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Pre-Processing","metadata":{"_cell_guid":"6ee4426f-d021-4f6b-a8a3-2ec7d2a8bb35","_uuid":"7b99a93c-2a7e-48ba-96da-3c93e703902d","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.004021,"end_time":"2025-05-27T13:12:22.644498","exception":false,"start_time":"2025-05-27T13:12:22.640477","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df['Sex_Reversed'] = train_df['Sex'].map({'male': 1, 'female': 0})\ntest_df['Sex_Reversed'] = test_df['Sex'].map({'male': 1, 'female': 0})\n\ntrain_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\ntest_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})\n\ntrain_df['Heart_Rate_pct'] = train_df['Heart_Rate'] / (220 - train_df['Age'])\ntest_df['Heart_Rate_pct'] = test_df['Heart_Rate'] / (220 - test_df['Age'])\n\ntrain_df['BMI'] = train_df['Weight'] / (train_df['Height']/100)**2\ntest_df['BMI'] = test_df['Weight'] / (test_df['Height']/100)**2\n\ntrain_df['BMR'] = np.where(\n    train_df['Sex'] == 'female',\n    10 * train_df['Weight'] + 6.25 * train_df['Height'] - 5 * train_df['Age'] - 161,\n    10 * train_df['Weight'] + 6.25 * train_df['Height'] - 5 * train_df['Age'] + 5\n)\ntest_df['BMR'] = np.where(\n    test_df['Sex'] == 'female',\n    10 * test_df['Weight'] + 6.25 * test_df['Height'] - 5 * test_df['Age'] - 161,\n    10 * test_df['Weight'] + 6.25 * test_df['Height'] - 5 * test_df['Age'] + 5\n)\n\ntrain_df['TSI'] = 5 * ((train_df['Body_Temp'] - 36.5) / (41.5 - 36.5)) + 5 * ((train_df['Heart_Rate'] - 60) / ((220 - train_df['Age']) - 60))\ntrain_df['RPE'] = train_df['Heart_Rate_pct'] + 0.1 * (train_df['Body_Temp'] - 37)\ntrain_df['FI'] = (train_df['Heart_Rate_pct'] ** 2) / train_df['Duration']\ntrain_df['CLI'] = (train_df['Heart_Rate'] * train_df['Duration']) / train_df['Weight']\ntrain_df['TLI'] = ((train_df['Body_Temp'] - 36.6) ** 2) * train_df['Duration']\ntrain_df['AMI'] = (train_df['BMR'] * train_df['Heart_Rate_pct']) / train_df['Duration']\ntrain_df['AWI'] = (train_df['Duration'] * train_df['Heart_Rate_pct']) / train_df['Age']\ntrain_df['WLI'] = train_df['Heart_Rate'] * train_df['Duration'] * train_df['Weight']\ntrain_df['VO2_Proxy'] = np.where(\n    train_df['Sex'] == 'female',\n    (0.85 * train_df['Duration']) / (train_df['Heart_Rate_pct'] * train_df['Age']),\n    (1.00 * train_df['Duration']) / (train_df['Heart_Rate_pct'] * train_df['Age']),\n)\ntest_df['TSI'] = 5 * ((test_df['Body_Temp'] - 36.5) / (41.5 - 36.5)) + 5 * ((test_df['Heart_Rate'] - 60) / ((220 - test_df['Age']) - 60))\ntest_df['RPE'] = test_df['Heart_Rate_pct'] + 0.1 * (test_df['Body_Temp'] - 37)\ntest_df['FI'] = (test_df['Heart_Rate_pct'] ** 2) / test_df['Duration']\ntest_df['CLI'] = (test_df['Heart_Rate'] * test_df['Duration']) / test_df['Weight']\ntest_df['TLI'] = ((test_df['Body_Temp'] - 36.6) ** 2) * test_df['Duration']\ntest_df['AMI'] = (test_df['BMR'] * test_df['Heart_Rate_pct']) / test_df['Duration']\ntest_df['AWI'] = (test_df['Duration'] * test_df['Heart_Rate_pct']) / test_df['Age']\ntest_df['WLI'] = test_df['Heart_Rate'] * test_df['Duration'] * test_df['Weight']\ntest_df['VO2_Proxy'] = np.where(\n    test_df['Sex'] == 'female',\n    (0.85 * test_df['Duration']) / (test_df['Heart_Rate_pct'] * test_df['Age']),\n    (1.00 * test_df['Duration']) / (test_df['Heart_Rate_pct'] * test_df['Age']),\n)\n\ntrain_df['Duration_HR'] = train_df['Duration'] * train_df['Heart_Rate']\ntest_df['Duration_HR'] = test_df['Duration'] * test_df['Heart_Rate']\n\ntrain_df['Duration2_HR'] = (train_df['Duration'])**2 * train_df['Heart_Rate']\ntest_df['Duration2_HR'] = (test_df['Duration'])**2 * test_df['Heart_Rate']\n\ntrain_df['Intensity'] = train_df['Heart_Rate'] / train_df['Duration']\ntest_df['Intensity'] = test_df['Heart_Rate'] / test_df['Duration']\n\nfor f1 in ['Duration', 'Heart_Rate', 'Body_Temp']:\n        for f2 in ['Sex', 'Sex_Reversed']:\n            train_df[f'{f1}_x_{f2}'] = train_df[f1] * train_df[f2]\nfor f1 in ['Duration', 'Heart_Rate', 'Body_Temp']:\n        for f2 in ['Sex', 'Sex_Reversed']:\n            test_df[f'{f1}_x_{f2}'] = test_df[f1] * test_df[f2]\n\ntrain_df['Body_Temp'] = train_df['Body_Temp'] - 37.0\ntest_df['Body_Temp'] = test_df['Body_Temp'] - 37.0\n\n# for col in ['Height', 'Weight', 'Heart_Rate', 'Body_Temp']:\n#         for agg in ['min', 'max']:\n#             agg_val = train_df.groupby('Sex')[col].agg(agg).rename(f'Sex_{col}_{agg}')\n#             train_df = train_df.merge(agg_val, on='Sex', how='left')\n# for col in ['Height', 'Weight', 'Heart_Rate', 'Body_Temp']:\n#         for agg in ['min', 'max']:\n#             agg_val = test_df.groupby('Sex')[col].agg(agg).rename(f'Sex_{col}_{agg}')\n#             test_df = test_df.merge(agg_val, on='Sex', how='left')\n\n# Calculate 'Heart_Rate_Ratio' for the training data\ntrain_df['Heart_Rate_Ratio'] = train_df['Heart_Rate'] / train_df['Age']\n# Calculate 'Heart_Rate_Ratio' for the testing data\ntest_df['Heart_Rate_Ratio'] = test_df['Heart_Rate'] / test_df['Age']\n\n# Calculate 'Weight_x_Duration' for the training data\ntrain_df['Weight_x_Duration'] = train_df['Weight'] * train_df['Duration']\n# Calculate 'Weight_x_Duration' for the testing data\ntest_df['Weight_x_Duration'] = test_df['Weight'] * test_df['Duration']\n\n# Calculate 'Height_x_Duration' for the training data\ntrain_df['Height_x_Duration'] = train_df['Height'] * train_df['Duration']\n# Calculate 'Height_x_Duration' for the testing data\ntest_df['Height_x_Duration'] = test_df['Height'] * test_df['Duration']\n\n# Calculate 'Weight_x_Height' for the training data\ntrain_df['Weight_x_Height'] = train_df['Weight'] * train_df['Height']\n# Calculate 'Weight_x_Height' for the testing data\ntest_df['Weight_x_Height'] = test_df['Weight'] * test_df['Height']\n\n# Calculate 'Weight_x_Intensity' for the training data\ntrain_df['Weight_x_Intensity'] = train_df['Weight'] * train_df['Intensity']\n# Calculate 'Weight_x_Intensity' for the testing data\ntest_df['Weight_x_Intensity'] = test_df['Weight'] * test_df['Intensity']\n\n# Calculate 'Height_x_Intensity' for the training data\ntrain_df['Height_x_Intensity'] = train_df['Height'] * train_df['Intensity']\n# Calculate 'Height_x_Intensity' for the testing data\ntest_df['Height_x_Intensity'] = test_df['Height'] * test_df['Intensity']\n\ntrain_df.drop(columns=['Sex_Reversed'], inplace=True)\ntest_df.drop(columns=['Sex_Reversed'], inplace=True)\n\ntrain_df.drop(columns=['id'], inplace=True)\ntest_df.drop(columns=['id'], inplace=True)","metadata":{"_cell_guid":"5ef17f18-54ca-44b6-b9de-764da5b0355d","_uuid":"bc82cfea-218a-4604-a6b7-27319197abd7","collapsed":false,"execution":{"iopub.status.busy":"2025-05-31T10:15:10.965363Z","iopub.execute_input":"2025-05-31T10:15:10.965665Z","iopub.status.idle":"2025-05-31T10:15:11.643961Z","shell.execute_reply.started":"2025-05-31T10:15:10.965643Z","shell.execute_reply":"2025-05-31T10:15:11.643093Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":2.329869,"end_time":"2025-05-27T13:12:24.978548","exception":false,"start_time":"2025-05-27T13:12:22.648679","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Scaling Numeric Features","metadata":{"_cell_guid":"b124b505-6d4b-417e-b3b8-ca82460fe740","_uuid":"1bbb0f62-0b1d-4d4f-8357-5b0c41781d60","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.003919,"end_time":"2025-05-27T13:12:24.987073","exception":false,"start_time":"2025-05-27T13:12:24.983154","status":"completed"},"tags":[]}},{"cell_type":"code","source":"features = train_df.columns.tolist()\nfeatures.remove('Calories')\nprint(features)","metadata":{"execution":{"iopub.status.busy":"2025-05-31T10:15:11.644988Z","iopub.execute_input":"2025-05-31T10:15:11.645299Z","iopub.status.idle":"2025-05-31T10:15:11.650043Z","shell.execute_reply.started":"2025-05-31T10:15:11.645275Z","shell.execute_reply":"2025-05-31T10:15:11.649376Z"},"papermill":{"duration":0.012081,"end_time":"2025-05-27T13:12:25.003696","exception":false,"start_time":"2025-05-27T13:12:24.991615","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\ntrain_df[features] = scaler.fit_transform(train_df[features])\ntest_df[features] = scaler.transform(test_df[features])","metadata":{"_cell_guid":"706c6d45-0f72-4538-8f1f-ce4ec0ec6bfc","_uuid":"8f51e5c8-81b8-4fd7-b757-c425ec86b0c1","collapsed":false,"execution":{"iopub.status.busy":"2025-05-31T10:15:11.650847Z","iopub.execute_input":"2025-05-31T10:15:11.651082Z","iopub.status.idle":"2025-05-31T10:15:12.448637Z","shell.execute_reply.started":"2025-05-31T10:15:11.651064Z","shell.execute_reply":"2025-05-31T10:15:12.447939Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.690359,"end_time":"2025-05-27T13:12:25.698614","exception":false,"start_time":"2025-05-27T13:12:25.008255","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df.head())\nprint(train_df.tail())\n\nprint(test_df.head())\nprint(test_df.tail())","metadata":{"_cell_guid":"ff08f9cb-51b9-42ef-8486-9689f8570baa","_uuid":"8e9e1a45-c43c-4cbc-9e4e-6064c6fb022e","collapsed":false,"execution":{"iopub.status.busy":"2025-05-31T10:15:12.449395Z","iopub.execute_input":"2025-05-31T10:15:12.449644Z","iopub.status.idle":"2025-05-31T10:15:12.498781Z","shell.execute_reply.started":"2025-05-31T10:15:12.449624Z","shell.execute_reply":"2025-05-31T10:15:12.497774Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.042118,"end_time":"2025-05-27T13:12:25.745764","exception":false,"start_time":"2025-05-27T13:12:25.703646","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = train_df[features]\ny_train = train_df['Calories']\n\nX_test = test_df[features]\n\nX_train = X_train.fillna(0) # Fill NaNs in training features\nX_test = X_test.fillna(0) # Fill NaNs in test features","metadata":{"execution":{"iopub.status.busy":"2025-05-31T10:15:12.49988Z","iopub.execute_input":"2025-05-31T10:15:12.500151Z","iopub.status.idle":"2025-05-31T10:15:12.871341Z","shell.execute_reply.started":"2025-05-31T10:15:12.50013Z","shell.execute_reply":"2025-05-31T10:15:12.870487Z"},"papermill":{"duration":0.342998,"end_time":"2025-05-27T13:12:26.200283","exception":false,"start_time":"2025-05-27T13:12:25.857285","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Functions","metadata":{"papermill":{"duration":0.006417,"end_time":"2025-05-27T13:12:26.213228","exception":false,"start_time":"2025-05-27T13:12:26.206811","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## RMSLE Scorer","metadata":{"papermill":{"duration":0.005922,"end_time":"2025-05-27T13:12:26.224891","exception":false,"start_time":"2025-05-27T13:12:26.218969","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def rmsle_scorer(y_true, y_pred):\n    y_pred_positive = np.maximum(y_pred, 0.001) \n    return np.sqrt(mean_squared_log_error(y_true, y_pred_positive))","metadata":{"execution":{"iopub.status.busy":"2025-05-31T10:15:12.872468Z","iopub.execute_input":"2025-05-31T10:15:12.872751Z","iopub.status.idle":"2025-05-31T10:15:12.877398Z","shell.execute_reply.started":"2025-05-31T10:15:12.87273Z","shell.execute_reply":"2025-05-31T10:15:12.876389Z"},"papermill":{"duration":0.012205,"end_time":"2025-05-27T13:12:26.243479","exception":false,"start_time":"2025-05-27T13:12:26.231274","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## KFold CV","metadata":{"papermill":{"duration":0.005403,"end_time":"2025-05-27T13:12:26.25472","exception":false,"start_time":"2025-05-27T13:12:26.249317","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def run_kfold_cv(X, y, model, model_name, n_splits, random_state=42):\n    print(f\"\\n--- Starting {n_splits}-Fold Cross-Validation for {model_name} ---\")\n    start_cv_time = time.time() # Start timing for the entire CV process\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    \n    fold_rmsle_scores = []\n    oof_predictions = np.zeros(X.shape[0]) \n    \n    fold_times = [] \n\n    # Iterate through each fold\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n        fold_start_time = time.time() # Start timing for the current fold\n\n        # Split data for the current fold\n        X_train_fold, y_train_fold = X.iloc[train_idx], y.iloc[train_idx]\n        X_val_fold, y_val_fold = X.iloc[val_idx], y.iloc[val_idx]\n        \n        model_fold = model.__class__(**model.get_params()) \n        model_fold.fit(X_train_fold, y_train_fold) # Train the model on the training fold\n        \n        val_preds = model_fold.predict(X_val_fold) # Make predictions on the validation fold\n        \n        val_preds[val_preds < 0] = 0.001 \n        \n        # Store out-of-fold predictions\n        oof_predictions[val_idx] = val_preds\n\n        # Evaluate the model's performance on the validation set for this fold using RMSLE\n        try:\n            fold_rmsle = np.sqrt(mean_squared_log_error(y_val_fold, val_preds))\n            fold_rmsle_scores.append(fold_rmsle)\n        except ValueError as e:\n            print(f\"  Warning: Error calculating RMSLE for Fold {fold + 1} ({model_name}): {e}. Setting RMSLE to NaN.\")\n            fold_rmsle_scores.append(np.nan)\n\n        fold_end_time = time.time() # End timing for the current fold\n        fold_duration = fold_end_time - fold_start_time\n        fold_times.append(fold_duration)\n\n    end_cv_time = time.time() # End timing for the entire CV process\n    total_cv_time = end_cv_time - start_cv_time\n\n    # Summarize results\n    valid_fold_rmsle_scores = [s for s in fold_rmsle_scores if not np.isnan(s)]\n    \n    mean_cv_rmsle = np.nan\n    std_cv_rmsle = np.nan\n    overall_oof_rmsle = np.nan\n\n    if valid_fold_rmsle_scores:\n        mean_cv_rmsle = np.mean(valid_fold_rmsle_scores)\n        std_cv_rmsle = np.std(valid_fold_rmsle_scores)\n        print(f\"\\n--- {n_splits}-Fold CV Summary for {model_name} ---\")\n        print(f\"Average RMSLE: {mean_cv_rmsle:.4f} +/- {std_cv_rmsle:.4f}\")\n    else:\n        print(f\"\\n--- {n_splits}-Fold CV Summary for {model_name} ---\")\n        print(f\"RMSLE calculation failed for all folds.\")\n\n    # Calculate overall OOF RMSLE if possible\n    if y.min() >= 0 and oof_predictions.min() >= 0 and valid_fold_rmsle_scores:\n        try:\n            overall_oof_rmsle = np.sqrt(mean_squared_log_error(y, oof_predictions))\n            print(f\"Overall OOF RMSLE: {overall_oof_rmsle:.4f}\")\n        except ValueError as e:\n            print(f\"Error calculating Overall OOF RMSLE for {model_name}: {e}. Ensure target and predictions are non-negative.\")\n    \n    return {\n        'Model': model_name,\n        'N_Splits': n_splits,\n        'Average RMSLE': mean_cv_rmsle,\n        'Std RMSLE': std_cv_rmsle,\n        'Overall OOF RMSLE': overall_oof_rmsle,\n        'Total CV Time (s)': total_cv_time,\n        'Avg Fold Time (s)': np.mean(fold_times) if fold_times else np.nan\n    }","metadata":{"execution":{"iopub.status.busy":"2025-05-31T10:15:12.878315Z","iopub.execute_input":"2025-05-31T10:15:12.878624Z","iopub.status.idle":"2025-05-31T10:15:12.894086Z","shell.execute_reply.started":"2025-05-31T10:15:12.878603Z","shell.execute_reply":"2025-05-31T10:15:12.89332Z"},"papermill":{"duration":0.018776,"end_time":"2025-05-27T13:12:26.279239","exception":false,"start_time":"2025-05-27T13:12:26.260463","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{"papermill":{"duration":0.005495,"end_time":"2025-05-27T13:12:26.290712","exception":false,"start_time":"2025-05-27T13:12:26.285217","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# n_splits_list = [5, 10, 15, 30, 50]\nn_splits_list = [5,]# 10, 15, 30, 50]\n\nRANDOM_STATE = 42","metadata":{"execution":{"iopub.status.busy":"2025-05-31T10:15:12.894899Z","iopub.execute_input":"2025-05-31T10:15:12.89519Z","iopub.status.idle":"2025-05-31T10:15:12.916471Z","shell.execute_reply.started":"2025-05-31T10:15:12.89517Z","shell.execute_reply":"2025-05-31T10:15:12.915608Z"},"papermill":{"duration":0.011489,"end_time":"2025-05-27T13:12:26.307875","exception":false,"start_time":"2025-05-27T13:12:26.296386","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optuna","metadata":{}},{"cell_type":"code","source":"# --- General Optuna Objective Function ---\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\ndef general_objective(trial, model_class, model_name_prefix):\n    \"\"\"\n    General objective function for Optuna to optimize various booster models.\n    Minimizes the average RMSLE from 5-fold cross-validation.\n    \"\"\"\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'random_state': RANDOM_STATE,\n        'n_jobs': -1,\n    }\n\n    if model_class == lgb.LGBMRegressor:\n        params['objective'] = 'regression_l1'\n        params['metric'] = 'rmsle'\n        params['num_leaves'] = trial.suggest_int('num_leaves', 20, 100)\n        params['min_child_samples'] = trial.suggest_int('min_child_samples', 5, 50)\n        params['colsample_bytree'] = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n        params['reg_alpha'] = trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True)\n        params['reg_lambda'] = trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True)\n        # params['device'] = 'gpu'\n        params['verbose'] = -1\n\n    elif model_class == xgb.XGBRegressor:\n        params['objective'] = 'reg:squarederror'\n        params['eval_metric'] = 'rmse'\n        params['min_child_weight'] = trial.suggest_int('min_child_weight', 1, 10)\n        params['colsample_bytree'] = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n        params['gamma'] = trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n        params['lambda'] = trial.suggest_float('lambda', 1e-8, 1.0, log=True) # reg_lambda\n        params['alpha'] = trial.suggest_float('alpha', 1e-8, 1.0, log=True)   # reg_alpha\n        # params['tree_method'] = 'gpu_hist'\n        # params['predictor'] = 'gpu_predictor'\n\n    # elif model_class == CatBoostRegressor:\n    #     params['iterations'] = params.pop('n_estimators') # CatBoost uses 'iterations'\n    #     params['loss_function'] = 'RMSE'\n    #     params['eval_metric'] = 'RMSE'\n    #     params['depth'] = params.pop('max_depth') # CatBoost uses 'depth'\n    #     params['l2_leaf_reg'] = trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True)\n    #     params['colsample_bylevel'] = trial.suggest_float('colsample_bylevel', 0.6, 1.0)\n    #     params['min_data_in_leaf'] = trial.suggest_int('min_data_in_leaf', 1, 30)\n    #     params['random_seed'] = RANDOM_STATE # CatBoost uses random_seed\n    #     params.pop('n_jobs') # CatBoost uses thread_count\n    #     params['thread_count'] = -1\n    #     params['task_type'] = 'GPU'\n    #     params['verbose'] = 0 # Suppress verbose output\n        \n        # # Ensure bootstrap_type for subsampling\n        # if params['subsample'] < 1.0:\n        #     params['bootstrap_type'] = 'Bernoulli'\n        # else:\n        #     params['bootstrap_type'] = 'MVS' # Or 'Bayesian', 'No'\n\n    model = model_class(**params)\n    \n    # cv_results = run_kfold_cv(X_train, y_train, model, model_name_prefix, n_splits=1, random_state=RANDOM_STATE)\n    # Perform train-validation split\n    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n    \n    # Train the model\n    model.fit(X_tr, y_tr)\n    \n    # Predict on validation set\n    y_pred = model.predict(X_val)\n    \n    # Ensure non-negative predictions for RMSLE\n    y_pred = np.clip(y_pred, 0, None)\n    \n    # Calculate RMSLE\n    rmsle = np.sqrt(mean_squared_log_error(y_val, y_pred))\n    \n    return rmsle #cv_results['Average RMSLE']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T10:15:12.919622Z","iopub.execute_input":"2025-05-31T10:15:12.919892Z","iopub.status.idle":"2025-05-31T10:15:12.936919Z","shell.execute_reply.started":"2025-05-31T10:15:12.919872Z","shell.execute_reply":"2025-05-31T10:15:12.935964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- Optuna Optimization for LightGBM ---\n# print(\"\\n\" + \"=\"*80)\n# print(\"Starting Optuna Hyperparameter Optimization for LightGBM\")\n# print(\"=\"*80)\n\n# study_lgbm = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n# study_lgbm.optimize(lambda trial: general_objective(trial, lgb.LGBMRegressor, \"LightGBM_Optuna\"), n_trials=100, show_progress_bar=True)\n\n# print(\"\\nOptuna LightGBM optimization finished.\")\n# print(\"Number of finished trials: \", len(study_lgbm.trials))\n# print(\"Best trial (LightGBM):\")\n# trial_lgbm = study_lgbm.best_trial\n\n# print(\"  Value (Avg RMSLE): \", trial_lgbm.value)\n# print(\"  Params: \")\n# for key, value in trial_lgbm.params.items():\n#     print(f\"    {key}: {value}\")\n\n# best_lgbm_params = trial_lgbm.params\n# best_lgbm_params['objective'] = 'regression_l1'\n# best_lgbm_params['metric'] = 'rmse'\n# best_lgbm_params['random_state'] = RANDOM_STATE\n# best_lgbm_params['n_jobs'] = -1\n# # best_lgbm_params['device'] = 'gpu'\n# best_lgbm_params['verbose'] = -1\n\n\n# # --- Optuna Optimization for XGBoost ---\n# print(\"\\n\" + \"=\"*80)\n# print(\"Starting Optuna Hyperparameter Optimization for XGBoost\")\n# print(\"=\"*80)\n\n# study_xgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE + 1))\n# study_xgb.optimize(lambda trial: general_objective(trial, xgb.XGBRegressor, \"XGBoost_Optuna\"), n_trials=50, show_progress_bar=True)\n\n# print(\"\\nOptuna XGBoost optimization finished.\")\n# print(\"Number of finished trials: \", len(study_xgb.trials))\n# print(\"Best trial (XGBoost):\")\n# trial_xgb = study_xgb.best_trial\n\n# print(\"  Value (Avg RMSLE): \", trial_xgb.value)\n# print(\"  Params: \")\n# for key, value in trial_xgb.params.items():\n#     print(f\"    {key}: {value}\")\n\n# best_xgb_params = trial_xgb.params\n# best_xgb_params['objective'] = 'reg:squarederror'\n# best_xgb_params['eval_metric'] = 'rmse'\n# best_xgb_params['random_state'] = RANDOM_STATE\n# # best_xgb_params['tree_method'] = 'gpu_hist' # Ensure GPU is set for the final model if available\n# # best_xgb_params['predictor'] = 'gpu_predictor' # Ensure GPU is set for the final model if available\n# best_xgb_params['n_jobs'] = -1\n\n\n# # # --- Optuna Optimization for CatBoost ---\n# # print(\"\\n\" + \"=\"*80)\n# # print(\"Starting Optuna Hyperparameter Optimization for CatBoost\")\n# # print(\"=\"*80)\n\n# # study_cat = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE + 2))\n# # study_cat.optimize(lambda trial: general_objective(trial, CatBoostRegressor, \"CatBoost_Optuna\"), n_trials=50, show_progress_bar=True)\n\n# # print(\"\\nOptuna CatBoost optimization finished.\")\n# # print(\"Number of finished trials: \", len(study_cat.trials))\n# # print(\"Best trial (CatBoost):\")\n# # trial_cat = study_cat.best_trial\n\n# # print(\"  Value (Avg RMSLE): \", trial_cat.value)\n# # print(\"  Params: \")\n# # for key, value in trial_cat.params.items():\n# #     print(f\"    {key}: {value}\")\n\n# # best_cat_params = trial_cat.params\n# # best_cat_params['random_seed'] = RANDOM_STATE\n# # best_cat_params['verbose'] = 0\n# # best_cat_params['thread_count'] = -1\n# # best_cat_params['eval_metric'] = 'RMSE'\n# # best_cat_params['eval_metric'] = 'RMSE' \n# # best_cat_params['task_type'] = 'GPU' # Ensure GPU is set for the final model if available\n# # best_cat_params['loss_function'] = 'RMSE'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T10:15:12.937899Z","iopub.execute_input":"2025-05-31T10:15:12.938169Z","iopub.status.idle":"2025-05-31T10:15:12.959844Z","shell.execute_reply.started":"2025-05-31T10:15:12.938144Z","shell.execute_reply":"2025-05-31T10:15:12.958825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Models\n","metadata":{"papermill":{"duration":0.005511,"end_time":"2025-05-27T13:12:26.319357","exception":false,"start_time":"2025-05-27T13:12:26.313846","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# models = {\n#     # \"Linear Regression\": LinearRegression(),\n#     # \"Decision Tree Regressor\": DecisionTreeRegressor(random_state=RANDOM_STATE),\n#     # \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n#     # \"Gradient Boosting Regressor\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=RANDOM_STATE),\n#     \"XGBoost Regressor\": xgb.XGBRegressor(\n#     n_estimators=1000,  # Start with a large number and use early stopping during training\n#     learning_rate=0.05,  # Start with a value in the suggested range (0.01 to 0.05)\n#     max_depth=6,         # Start in the suggested range (4 to 10)\n#     colsample_bytree=0.8, # Start in the suggested range (0.6 to 1.0). Lower if many features.\n#     subsample=0.8,       # Start in the suggested range (0.6 to 1.0)\n#     reg_alpha=0,         # You might want to tune this (e.g., 0 to 5)\n#     reg_lambda=1,        # You might want to tune this (e.g., 0 to 5)\n#     random_state=RANDOM_STATE,\n#     n_jobs=-1\n# ),\n#     \"LightGBM Regressor\": lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=31, random_state=RANDOM_STATE, n_jobs=-1),\n#     \"CatBoost Regressor\": CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=3, random_state=RANDOM_STATE, verbose=0, thread_count=-1)\n# }\n\nmodels = {\n    # \"XGBoost Regressor\": xgb.XGBRegressor(**study_xgb.best_params),\n    \"XGBoost Regressor\": xgb.XGBRegressor(\n    n_estimators=633,\n    learning_rate=0.014465037740657534,\n    max_depth=10,\n    subsample=0.834441652086676,\n    min_child_weight=2,\n    colsample_bytree=0.7126992343210137,\n    # reg_gamma=6.358507422599293e-06,\n    reg_lambda=0.12835242746436062,\n    reg_alpha=1.9027210521909599e-07,\n    random_state=42\n),\n    # \"LightGBM Regressor\": lgb.LGBMRegressor(**study_lgbm.best_params),\n    \"LightGBM Regressor\": lgb.LGBMRegressor(\n    n_estimators=986,\n    learning_rate=0.057420007485704215,\n    max_depth=10,\n    subsample=0.6325012157391713,\n    num_leaves=95,\n    min_child_samples=10,\n    colsample_bytree=0.6483999638619974,\n    reg_alpha=1.653205298761583e-05,\n    reg_lambda=2.1766070473179248e-05,\n    random_state=42 # It's good practice to set a random state for reproducibility\n),\n    # \"CatBoost Regressor\": CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=3, random_state=RANDOM_STATE, verbose=0, thread_count=-1)\n}","metadata":{"execution":{"iopub.status.busy":"2025-05-31T10:15:12.960963Z","iopub.execute_input":"2025-05-31T10:15:12.961226Z","iopub.status.idle":"2025-05-31T10:15:12.980387Z","shell.execute_reply.started":"2025-05-31T10:15:12.961207Z","shell.execute_reply":"2025-05-31T10:15:12.979719Z"},"papermill":{"duration":0.015086,"end_time":"2025-05-27T13:12:26.3402","exception":false,"start_time":"2025-05-27T13:12:26.325114","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# KFold and Train and Save CSV","metadata":{"papermill":{"duration":0.005969,"end_time":"2025-05-27T13:12:26.352206","exception":false,"start_time":"2025-05-27T13:12:26.346237","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# List to store all CV results for comparison table\nall_cv_results = []\n\n# --- Run K-Fold CV for Each Model ---\nfor model_name, model_instance in models.items():\n    print(f\"\\n{'='*80}\\nRunning K-Fold Cross-Validation for: {model_name}\\n{'='*80}\")\n    for n_splits_val in n_splits_list:\n        results = run_kfold_cv(X_train, y_train, model_instance, model_name, n_splits_val, RANDOM_STATE)\n        all_cv_results.append(results)\n    \n    # --- Train on Full X_train and Predict on X_test (after CV for this model) ---\n    print(f\"\\n--- Training {model_name} on full X_train and predicting on X_test ---\")\n    final_model = model_instance.__class__(**model_instance.get_params()) # Create a fresh instance for final training\n    \n    start_time_full_train = time.time()\n    final_model.fit(X_train, y_train)\n    end_time_full_train = time.time()\n    print(f\"Full training complete in {(end_time_full_train - start_time_full_train):.4f} seconds.\")\n\n    start_time_predict = time.time()\n    predictions_test = final_model.predict(X_test)\n    end_time_predict = time.time()\n    print(f\"Predictions made in {(end_time_predict - start_time_predict):.4f} seconds.\")\n\n    # Handle negative predictions for submission file\n    predictions_test[predictions_test < 0] = np.abs(predictions_test[predictions_test < 0]) #0.001 #np.abs(predictions_test)\n    print(predictions_test)\n\n    # Save predictions to CSV\n    submission_df = pd.DataFrame({'id': test_ids, 'Predictions': predictions_test})\n    csv_filename = f'{model_name.replace(\" \", \"_\")}_predictions.csv'\n    submission_df.to_csv(csv_filename, index=False)\n    print(f\"Submission file '{csv_filename}' created successfully.\")\n\n    # Print feature importances if available (for tree-based models)\n    if hasattr(final_model, 'feature_importances_'):\n        print(f\"Feature Importances for {model_name}:\")\n        # Map feature importances to original feature names\n        feature_importances_df = pd.DataFrame({\n            'Feature': X_train.columns,\n            'Importance': final_model.feature_importances_\n        }).sort_values(by='Importance', ascending=False)\n        print(feature_importances_df.to_string(index=False))\n    print(f\"{'-'*80}\") # Separator after each model's full training/prediction","metadata":{"execution":{"iopub.status.busy":"2025-05-31T10:33:30.466713Z","iopub.execute_input":"2025-05-31T10:33:30.46762Z"},"papermill":{"duration":9085.265923,"end_time":"2025-05-27T15:43:51.624138","exception":false,"start_time":"2025-05-27T13:12:26.358215","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Comparison Table","metadata":{"papermill":{"duration":0.1815,"end_time":"2025-05-27T15:43:51.987911","exception":false,"start_time":"2025-05-27T15:43:51.806411","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Final Comparison Table\nprint(\"\\n\" + \"=\"*120) # Adjusted width for new columns and more models\nprint(\"                                Cross-Validation Summary Across Different Models and Folds                                \")\nprint(\"=\"*120)\n\n# Create a DataFrame from the results for a nice tabular output\nresults_df = pd.DataFrame(all_cv_results)\n\n# Sort for better comparison: by Model, then by N_Splits\nresults_df = results_df.sort_values(by=['Model', 'N_Splits']).reset_index(drop=True)\n\n# Format the numerical columns for better readability\nresults_df['Average RMSLE'] = results_df['Average RMSLE'].map('{:.4f}'.format)\nresults_df['Std RMSLE'] = results_df['Std RMSLE'].map('{:.4f}'.format)\nresults_df['Overall OOF RMSLE'] = results_df['Overall OOF RMSLE'].map('{:.4f}'.format)\nresults_df['Total CV Time (s)'] = results_df['Total CV Time (s)'].map('{:.4f}'.format)\nresults_df['Avg Fold Time (s)'] = results_df['Avg Fold Time (s)'].map('{:.4f}'.format)\n\n# Print the DataFrame\nprint(results_df.to_string(index=False))\nprint(\"=\"*120)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-31T10:28:01.424328Z","iopub.execute_input":"2025-05-31T10:28:01.424595Z","iopub.status.idle":"2025-05-31T10:28:01.44606Z","shell.execute_reply.started":"2025-05-31T10:28:01.424577Z","shell.execute_reply":"2025-05-31T10:28:01.445299Z"},"papermill":{"duration":0.198158,"end_time":"2025-05-27T15:43:52.447521","exception":false,"start_time":"2025-05-27T15:43:52.249363","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef ensemble_submission_csvs(file_paths, id_col='id', prediction_col='Calories', output_filename='ensemble_submission.csv'):\n    \"\"\"\n    Ensembles predictions from multiple submission CSV files by averaging.\n\n    Args:\n        file_paths (list): A list of paths to the submission CSV files.\n        id_col (str): The name of the ID column in the CSV files.\n        prediction_col (str): The name of the column containing the predictions.\n        output_filename (str): The name of the CSV file to save the ensembled predictions.\n    \"\"\"\n    if not file_paths:\n        print(\"Error: No file paths provided for ensembling.\")\n        return\n\n    # Read the first submission file\n    try:\n        ensemble_df = pd.read_csv(file_paths[0])\n        ensemble_df = ensemble_df[[id_col, prediction_col]].copy()\n        ensemble_df.rename(columns={prediction_col: f'{prediction_col}_1'}, inplace=True)\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_paths[0]}\")\n        return\n    except KeyError:\n        print(f\"Error: '{id_col}' or '{prediction_col}' not found in {file_paths[0]}\")\n        return\n\n    # Read and merge subsequent submission files\n    for i, file_path in enumerate(file_paths[1:], start=2):\n        try:\n            current_df = pd.read_csv(file_path)\n            current_df = current_df[[id_col, prediction_col]].copy()\n            current_df.rename(columns={prediction_col: f'{prediction_col}_{i}'}, inplace=True)\n            ensemble_df = pd.merge(ensemble_df, current_df, on=id_col, how='inner') # Use inner to ensure common IDs\n        except FileNotFoundError:\n            print(f\"Warning: File not found at {file_path}. Skipping this file.\")\n            continue\n        except KeyError:\n            print(f\"Warning: '{id_col}' or '{prediction_col}' not found in {file_path}. Skipping this file.\")\n            continue\n\n    # Identify prediction columns to average\n    pred_cols_to_average = [col for col in ensemble_df.columns if col.startswith(prediction_col + '_')]\n\n    if not pred_cols_to_average:\n        print(\"Error: No prediction columns found to average after merging.\")\n        return\n\n    # Calculate the average of the predictions\n    ensemble_df[prediction_col] = ensemble_df[pred_cols_to_average].mean(axis=1)\n\n    # Prepare the final submission DataFrame\n    final_submission_df = ensemble_df[[id_col, prediction_col]]\n\n    # Save the ensembled predictions to a new CSV file\n    try:\n        final_submission_df.to_csv(output_filename, index=False)\n        print(f\"\\nEnsembled predictions saved to {output_filename}\")\n        print(\"First 5 rows of the ensembled submission:\")\n        print(final_submission_df.head())\n    except Exception as e:\n        print(f\"Error saving the ensembled submission file: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T10:28:01.447257Z","iopub.execute_input":"2025-05-31T10:28:01.447605Z","iopub.status.idle":"2025-05-31T10:28:01.475641Z","shell.execute_reply.started":"2025-05-31T10:28:01.447578Z","shell.execute_reply":"2025-05-31T10:28:01.47483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace these with the actual paths to your submission CSV files\nsubmission_files = [\n    '/kaggle/working/XGBoost_Regressor_predictions.csv',   # Output from your Random Forest model\n    '/kaggle/working/LightGBM_Regressor_predictions.csv', # Output from your LightGBM model\n    # '/kaggle/input/ensemble-for-pce-s5e5/XGBoost_Regressor_predictions.csv',# Add more files here if you have them, e.g., 'submission_xgb.csv'\n]\n\n# Call the function to ensemble\nensemble_submission_csvs(\n    file_paths=submission_files,\n    id_col='id',            # Make sure this matches your ID column name\n    prediction_col='Predictions', # Make sure this matches your prediction column name\n    output_filename='averaged_ensemble_submission.csv'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T10:28:01.476375Z","iopub.execute_input":"2025-05-31T10:28:01.47662Z","iopub.status.idle":"2025-05-31T10:28:02.198142Z","shell.execute_reply.started":"2025-05-31T10:28:01.4766Z","shell.execute_reply":"2025-05-31T10:28:02.197178Z"}},"outputs":[],"execution_count":null}]}
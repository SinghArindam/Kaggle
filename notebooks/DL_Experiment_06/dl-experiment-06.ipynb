{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":636393,"sourceType":"datasetVersion","datasetId":312121}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:38:40.036433Z","iopub.execute_input":"2025-04-20T11:38:40.03661Z","iopub.status.idle":"2025-04-20T11:38:41.840715Z","shell.execute_reply.started":"2025-04-20T11:38:40.036592Z","shell.execute_reply":"2025-04-20T11:38:41.839977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Generate synthetic sine wave dataset\nclass SineWaveDataset(Dataset):\n    def __init__(self, seq_length, num_samples):\n        self.seq_length = seq_length\n        self.num_samples = num_samples\n        self.data = []\n        self.labels = []\n        \n        # Generate sine wave data\n        t = np.linspace(0, 100, num_samples + seq_length + 1)\n        sine_wave = np.sin(t)\n        \n        for i in range(num_samples):\n            x = sine_wave[i:i+seq_length]\n            y = sine_wave[i+seq_length]\n            self.data.append(x)\n            self.labels.append(y)\n            \n        self.data = torch.FloatTensor(self.data).reshape(-1, seq_length, 1)\n        self.labels = torch.FloatTensor(self.labels).reshape(-1, 1)\n    \n    def __len__(self):\n        return self.num_samples\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# Define RNN model\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        # Initialize hidden state\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n        \n        # Forward propagate RNN\n        out, _ = self.rnn(x, h0)\n        \n        # Get output from the last time step\n        out = self.fc(out[:, -1, :])\n        return out\n\n# Hyperparameters\nseq_length = 10\nhidden_size = 64\nnum_layers = 2\nnum_epochs = 100\nbatch_size = 32\nlearning_rate = 0.001\nnum_samples = 1000\n\n# Create datasets and dataloaders\ntrain_dataset = SineWaveDataset(seq_length, num_samples)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize model, loss function, and optimizer\nmodel = RNN(input_size=1, hidden_size=hidden_size, num_layers=num_layers, output_size=1).to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\ntrain_losses = []\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    for batch_x, batch_y in train_loader:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        \n        # Forward pass\n        outputs = model(batch_x)\n        loss = criterion(outputs, batch_y)\n        \n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n    \n    avg_loss = epoch_loss / len(train_loader)\n    train_losses.append(avg_loss)\n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}')\n\n# Plot training loss\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.title('Training Loss over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.legend()\nplt.grid(True)\nplt.savefig('training_loss.png')\nplt.show()  # Added to display the plot\nplt.close()\n\n# Generate predictions for visualization\nmodel.eval()\ntest_dataset = SineWaveDataset(seq_length, 100)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\npredictions = []\nactuals = []\nwith torch.no_grad():\n    for batch_x, batch_y in test_loader:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        outputs = model(batch_x)\n        predictions.extend(outputs.cpu().numpy().flatten())\n        actuals.extend(batch_y.cpu().numpy().flatten())\n\n# Plot predictions vs actual\nplt.figure(figsize=(12, 6))\nplt.plot(actuals, label='Actual', alpha=0.7)\nplt.plot(predictions, label='Predicted', alpha=0.7)\nplt.title('RNN Predictions vs Actual Values')\nplt.xlabel('Sample')\nplt.ylabel('Value')\nplt.legend()\nplt.grid(True)\nplt.savefig('predictions.png')\nplt.show()  # Added to display the plot\nplt.close()\n\n# Print final loss\nprint(f'Final Training Loss: {train_losses[-1]:.6f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:42:33.067262Z","iopub.execute_input":"2025-04-20T11:42:33.067896Z","iopub.status.idle":"2025-04-20T11:42:40.800605Z","shell.execute_reply.started":"2025-04-20T11:42:33.067867Z","shell.execute_reply":"2025-04-20T11:42:40.799812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load and preprocess the dataset\nclass ClimateDataset(Dataset):\n    def __init__(self, data, seq_length, target_col='meantemp'):\n        self.seq_length = seq_length\n        self.target_col = target_col\n        \n        # Normalize the data\n        self.scaler = MinMaxScaler()\n        data_normalized = self.scaler.fit_transform(data[[target_col]])\n        \n        self.data = []\n        self.labels = []\n        \n        # Create sequences\n        for i in range(len(data_normalized) - seq_length):\n            x = data_normalized[i:i+seq_length]\n            y = data_normalized[i+seq_length]\n            self.data.append(x)\n            self.labels.append(y)\n            \n        self.data = torch.FloatTensor(self.data)\n        self.labels = torch.FloatTensor(self.labels)\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# Define RNN model\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        # Initialize hidden state\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n        \n        # Forward propagate RNN\n        out, _ = self.rnn(x, h0)\n        \n        # Get output from the last time step\n        out = self.fc(out[:, -1, :])\n        return out\n\n# Load the dataset (assuming it's in Kaggle input directory)\ntry:\n    df = pd.read_csv('/kaggle/input/daily-climate-time-series-data/DailyDelhiClimateTrain.csv')\nexcept FileNotFoundError:\n    print(\"Please ensure the Daily Delhi Climate dataset is available in the Kaggle input directory.\")\n    exit()\n\n# Convert date to datetime and sort\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\n\n# Hyperparameters\nseq_length = 10\nhidden_size = 64\nnum_layers = 2\nnum_epochs = 100\nbatch_size = 32\nlearning_rate = 0.001\n\n# Create dataset and dataloader\ndataset = ClimateDataset(df, seq_length, target_col='meantemp')\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Initialize model, loss function, and optimizer\nmodel = RNN(input_size=1, hidden_size=hidden_size, num_layers=num_layers, output_size=1).to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\ntrain_losses = []\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    for batch_x, batch_y in train_loader:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        \n        # Forward pass\n        outputs = model(batch_x)\n        loss = criterion(outputs, batch_y)\n        \n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n    \n    avg_loss = epoch_loss / len(train_loader)\n    train_losses.append(avg_loss)\n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}')\n\n# Plot training loss\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.title('Training Loss over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.legend()\nplt.grid(True)\nplt.savefig('training_loss.png')\nplt.show()  # Added to display the plot\nplt.close()\n\n# Generate predictions for visualization\nmodel.eval()\npredictions = []\nactuals = []\nwith torch.no_grad():\n    for batch_x, batch_y in test_loader:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        outputs = model(batch_x)\n        \n        # Inverse transform to original scale\n        outputs = dataset.scaler.inverse_transform(outputs.cpu().numpy())\n        batch_y = dataset.scaler.inverse_transform(batch_y.cpu().numpy())\n        \n        predictions.extend(outputs.flatten())\n        actuals.extend(batch_y.flatten())\n\n# Plot predictions vs actual\nplt.figure(figsize=(12, 6))\nplt.plot(actuals, label='Actual Temperature', alpha=0.7)\nplt.plot(predictions, label='Predicted Temperature', alpha=0.7)\nplt.title('RNN Temperature Predictions vs Actual Values')\nplt.xlabel('Sample')\nplt.ylabel('Mean Temperature (Â°C)')\nplt.legend()\nplt.grid(True)\nplt.savefig('predictions.png')\nplt.show()  # Added to display the plot\nplt.close()\n\n# Print final loss\nprint(f'Final Training Loss: {train_losses[-1]:.6f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:42:48.378969Z","iopub.execute_input":"2025-04-20T11:42:48.379243Z","iopub.status.idle":"2025-04-20T11:42:58.012183Z","shell.execute_reply.started":"2025-04-20T11:42:48.379221Z","shell.execute_reply":"2025-04-20T11:42:58.011425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
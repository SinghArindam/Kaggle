{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torch import nn\nfrom torch import optim\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\n\n# Define hyperparameters and variables\nLEARNING_RATE = 0.0005\nBATCH_SIZE = 1024\nIMAGE_SIZE = 64\nEPOCHS = 50\nimage_channels = 1\nnoise_channels = 256\ngen_features = 64\ndisc_features = 64\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the transform\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,)),\n])\n\n# Load the dataset\ndataset = FashionMNIST(root=\"dataset/\", train=True, transform=data_transforms, download=True)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n\n# Generator\nclass Generator(nn.Module):\n    def __init__(self, noise_channels, image_channels, features):\n        super(Generator, self).__init__()\n        \"\"\"\n        The generator model uses 4 ConvTranspose blocks. Each block contains \n        a ConvTranspose2d, BatchNorm2d, and ReLU activation.\n        \"\"\"\n        self.model = nn.Sequential(\n            # Transpose block 1\n            nn.ConvTranspose2d(noise_channels, features*16, kernel_size=4, stride=1, padding=0),\n            nn.ReLU(),\n            # Transpose block 2\n            nn.ConvTranspose2d(features*16, features*8, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(features*8),\n            nn.ReLU(),\n            # Transpose block 3\n            nn.ConvTranspose2d(features*8, features*4, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(features*4),\n            nn.ReLU(),\n            # Transpose block 4\n            nn.ConvTranspose2d(features*4, features*2, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(features*2),\n            nn.ReLU(),\n            # Last transpose block\n            nn.ConvTranspose2d(features*2, image_channels, kernel_size=4, stride=2, padding=1),\n            nn.Tanh(),\n        )\n    \n    def forward(self, x):\n        return self.model(x)\n\n# Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self, image_channels, features):\n        super(Discriminator, self).__init__()\n        \"\"\"\n        The discriminator model has 5 Conv blocks with Conv2d, BatchNorm, and LeakyReLU activation.\n        \"\"\"\n        self.model = nn.Sequential(\n            # Conv block 1\n            nn.Conv2d(image_channels, features, kernel_size=4, stride=2, padding=1),  # 64x64 -> 32x32\n            nn.LeakyReLU(0.2),\n            # Conv block 2\n            nn.Conv2d(features, features*2, kernel_size=4, stride=2, padding=1),  # 32x32 -> 16x16\n            nn.BatchNorm2d(features*2),\n            nn.LeakyReLU(0.2),\n            # Conv block 3\n            nn.Conv2d(features*2, features*4, kernel_size=4, stride=2, padding=1),  # 16x16 -> 8x8\n            nn.BatchNorm2d(features*4),\n            nn.LeakyReLU(0.2),\n            # Conv block 4\n            nn.Conv2d(features*4, features*8, kernel_size=4, stride=2, padding=1),  # 8x8 -> 4x4\n            nn.BatchNorm2d(features*8),\n            nn.LeakyReLU(0.2),\n            # Conv block 5\n            nn.Conv2d(features*8, 1, kernel_size=4, stride=1, padding=0),  # 4x4 -> 1x1\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        return output.view(-1)  # Flatten to [batch_size]\n\n# Load models\ngen_model = Generator(noise_channels, image_channels, gen_features).to(device)\ndisc_model = Discriminator(image_channels, disc_features).to(device)\n\n# Setup optimizers\ngen_optimizer = optim.Adam(gen_model.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\ndisc_optimizer = optim.Adam(disc_model.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n\n# Define loss function\ncriterion = nn.BCELoss()\n\n# Set models to training mode\ngen_model.train()\ndisc_model.train()\n\n# Define labels with smoothing\nfake_label = 0.1\nreal_label = 0.9\n\n# Define fixed noise for consistent image generation\nfixed_noise = torch.randn(64, noise_channels, 1, 1).to(device)\n\n# TensorBoard writers\nwriter_real = SummaryWriter(f\"runs/fashion/test_real\")\nwriter_fake = SummaryWriter(f\"runs/fashion/test_fake\")\n\n# Loss tracking\ngen_losses = []\ndisc_losses = []\n\n# Training loop\nprint(\"Start training...\")\nstep = 0\n\nfor epoch in range(EPOCHS):\n    for batch_idx, (data, target) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch}\")):\n        data = data.to(device)\n        batch_size = data.shape[0]\n\n        # Train discriminator on real data\n        disc_model.zero_grad()\n        label = (torch.ones(batch_size) * real_label).to(device)\n        output = disc_model(data).reshape(-1)\n        real_disc_loss = criterion(output, label)\n        d_x = output.mean().item()\n\n        # Train discriminator on fake data\n        noise = torch.randn(batch_size, noise_channels, 1, 1, device=device)\n        fake = gen_model(noise)\n        label = (torch.ones(batch_size) * fake_label).to(device)\n        output = disc_model(fake.detach()).reshape(-1)\n        fake_disc_loss = criterion(output, label)\n\n        # Calculate discriminator loss\n        disc_loss = real_disc_loss + fake_disc_loss\n        disc_loss.backward()\n        disc_optimizer.step()\n\n        # Train generator\n        gen_model.zero_grad()\n        label = torch.ones(batch_size).to(device)\n        output = disc_model(fake).reshape(-1)\n        gen_loss = criterion(output, label)\n        gen_loss.backward()\n        gen_optimizer.step()\n\n        # Store losses\n        gen_losses.append(gen_loss.item())\n        disc_losses.append(disc_loss.item())\n\n        # Log losses and images\n        if batch_idx % 50 == 0:\n            step += 1\n            print(\n                f\"Epoch: {epoch} ===== Batch: {batch_idx}/{len(dataloader)} ===== \"\n                f\"Disc loss: {disc_loss:.4f} ===== Gen loss: {gen_loss:.4f}\"\n            )\n            with torch.no_grad():\n                fake_images = gen_model(fixed_noise)\n                img_grid_real = torchvision.utils.make_grid(data[:40], normalize=True)\n                img_grid_fake = torchvision.utils.make_grid(fake_images[:40], normalize=True)\n                writer_real.add_image(\"Real images\", img_grid_real, global_step=step)\n                writer_fake.add_image(\"Generated images\", img_grid_fake, global_step=step)\n\n    # Save generated images every 10 epochs or at epoch 1\n    if (epoch + 1) % 10 == 0 or epoch == 0:\n        with torch.no_grad():\n            fake_images = gen_model(fixed_noise).detach().cpu()\n        plt.figure(figsize=(10, 10))\n        for j in range(64):\n            plt.subplot(8, 8, j+1)\n            plt.imshow(fake_images[j].squeeze(), cmap='gray')\n            plt.axis('off')\n        plt.savefig(f'generated_images_epoch_{epoch+1}.png')\n        plt.close()\n\n# Plot losses\nplt.figure(figsize=(10, 5))\nplt.plot(gen_losses, label='Generator Loss')\nplt.plot(disc_losses, label='Discriminator Loss')\nplt.title('Generator and Discriminator Losses During Training')\nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.savefig('loss_plot.png')\nplt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:44:46.202931Z","iopub.execute_input":"2025-04-23T12:44:46.203378Z","execution_failed":"2025-04-23T12:59:20.835Z"}},"outputs":[],"execution_count":null}]}
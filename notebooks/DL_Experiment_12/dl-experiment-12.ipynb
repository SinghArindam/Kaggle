{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport random\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\n# Set random seed for reproducibility\nSEED = 1234\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load IWSLT16 English-French dataset\n# dataset = load_dataset(\"iwslt2017\", \"iwslt2017-en-fr\")\ndataset = load_dataset(\"iwslt2017\", \"iwslt2017-en-fr\", trust_remote_code=True)\nprint(f\"Dataset loaded with {len(dataset['train'])} training examples\")\n\n# Limit to 10K examples\nMAX_EXAMPLES = 10000\ntrain_data = dataset['train'].select(range(min(MAX_EXAMPLES, len(dataset['train']))))\nval_data = dataset['validation'].select(range(min(1000, len(dataset['validation']))))\n\n# Simple tokenization function (in a real scenario, use proper tokenizers)\ndef tokenize(text, lang):\n    return text.lower().split()\n\n# Create vocabularies\nclass Vocab:\n    def __init__(self, name):\n        self.name = name\n        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n        self.idx2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n        self.count = 4\n        self.max_size = 10000\n    \n    def add_sentence(self, sentence):\n        for word in sentence:\n            if word not in self.word2idx and self.count < self.max_size:\n                self.word2idx[word] = self.count\n                self.idx2word[self.count] = word\n                self.count += 1\n    \n    def __len__(self):\n        return self.count\n\n# Build vocabularies\nsrc_vocab = Vocab(\"English\")\ntgt_vocab = Vocab(\"French\")\n\nprint(\"Building vocabularies...\")\nfor item in tqdm(train_data):\n    en_tokens = tokenize(item['translation']['en'], 'en')\n    fr_tokens = tokenize(item['translation']['fr'], 'fr')\n    src_vocab.add_sentence(en_tokens)\n    tgt_vocab.add_sentence(fr_tokens)\n\nprint(f\"Source vocabulary size: {len(src_vocab)}\")\nprint(f\"Target vocabulary size: {len(tgt_vocab)}\")\n\n# Dataset class\nclass TranslationDataset(Dataset):\n    def __init__(self, data, src_vocab, tgt_vocab):\n        self.data = data\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        src_text = item['translation']['en']\n        tgt_text = item['translation']['fr']\n        \n        src_tokens = tokenize(src_text, 'en')\n        tgt_tokens = tokenize(tgt_text, 'fr')\n        \n        # Limit sequence length to 50\n        src_tokens = src_tokens[:50]\n        tgt_tokens = tgt_tokens[:50]\n        \n        # Convert tokens to indices\n        src_indices = [self.src_vocab.word2idx.get(token, 3) for token in src_tokens] \n        tgt_indices = [self.tgt_vocab.word2idx.get(token, 3) for token in tgt_tokens]\n        \n        # Add SOS and EOS tokens\n        src_indices = [1] + src_indices + [2]\n        tgt_indices = [1] + tgt_indices + [2]\n        \n        return torch.LongTensor(src_indices), torch.LongTensor(tgt_indices)\n\n# Custom collate function to handle padding\ndef collate_fn(batch):\n    src_batch, tgt_batch = [], []\n    for src, tgt in batch:\n        src_batch.append(src)\n        tgt_batch.append(tgt)\n    \n    # Pad sequences\n    src_batch = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n    tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n    \n    return src_batch, tgt_batch\n\n# Create data loaders\nBATCH_SIZE = 64\ntrain_dataset = TranslationDataset(train_data, src_vocab, tgt_vocab)\nval_dataset = TranslationDataset(val_data, src_vocab, tgt_vocab)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n                          shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, \n                        collate_fn=collate_fn)\n\n# Basic Encoder (shared for both models)\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, \n                         dropout=dropout if n_layers > 1 else 0, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src):\n        # src: [batch_size, src_len]\n        embedded = self.dropout(self.embedding(src))\n        outputs, hidden = self.rnn(embedded)\n        return outputs, hidden\n\n# Decoder without attention\nclass DecoderNoAttention(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.output_dim = output_dim\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, \n                         dropout=dropout if n_layers > 1 else 0, batch_first=True)\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden):\n        # input: [batch_size]\n        input = input.unsqueeze(1)  # Add seq dimension\n        embedded = self.dropout(self.embedding(input))\n        output, hidden = self.rnn(embedded, hidden)\n        prediction = self.fc_out(output.squeeze(1))\n        return prediction, hidden\n\n# Attention mechanism\nclass Attention(nn.Module):\n    def __init__(self, hid_dim):\n        super().__init__()\n        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n        self.v = nn.Linear(hid_dim, 1, bias=False)\n        \n    def forward(self, hidden, encoder_outputs, mask=None):\n        # hidden: [batch_size, hid_dim]\n        # encoder_outputs: [batch_size, src_len, hid_dim]\n        \n        src_len = encoder_outputs.shape[1]\n        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n        \n        # Calculate attention scores\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        attention = self.v(energy).squeeze(2)\n        \n        # Apply mask\n        if mask is not None:\n            attention = attention.masked_fill(mask == 0, -1e10)\n        \n        # Softmax to get weights\n        attention_weights = F.softmax(attention, dim=1)\n        \n        # Weighted sum of encoder outputs\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n        \n        return context, attention_weights\n\n# Decoder with attention\nclass DecoderWithAttention(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, attention, dropout):\n        super().__init__()\n        self.output_dim = output_dim\n        self.attention = attention\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim, n_layers, \n                          dropout=dropout if n_layers > 1 else 0, batch_first=True)\n        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, encoder_outputs, mask=None):\n        input = input.unsqueeze(1)  # Add seq dimension\n        embedded = self.dropout(self.embedding(input))\n        \n        # Calculate attention\n        context, attention = self.attention(hidden[-1], encoder_outputs, mask)\n        \n        # Combine embedding and context for RNN input\n        rnn_input = torch.cat((embedded, context.unsqueeze(1)), dim=2)\n        output, hidden = self.rnn(rnn_input, hidden)\n        \n        # Prepare for output projection\n        embedded = embedded.squeeze(1)\n        output = output.squeeze(1)\n        \n        # Concatenate for prediction\n        prediction = self.fc_out(torch.cat((output, context, embedded), dim=1))\n        \n        return prediction, hidden, attention\n\n# Seq2Seq model without attention\nclass Seq2SeqNoAttention(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n        batch_size = src.shape[0]\n        tgt_len = tgt.shape[1]\n        tgt_vocab_size = self.decoder.output_dim\n        \n        # Tensor to store outputs\n        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n        \n        # Get encoder outputs\n        encoder_outputs, hidden = self.encoder(src)\n        \n        # First decoder input is <sos> token\n        input = tgt[:, 0]\n        \n        for t in range(1, tgt_len):\n            output, hidden = self.decoder(input, hidden)\n            outputs[:, t] = output\n            \n            # Teacher forcing\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = tgt[:, t] if teacher_force else top1\n            \n        return outputs\n\n# Seq2Seq model with attention\nclass Seq2SeqWithAttention(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n    def create_mask(self, src):\n        mask = (src != 0).to(self.device)\n        return mask\n    \n    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n        batch_size = src.shape[0]\n        tgt_len = tgt.shape[1]\n        tgt_vocab_size = self.decoder.output_dim\n        src_len = src.shape[1]\n        \n        # Store outputs and attentions\n        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n        attentions = torch.zeros(batch_size, tgt_len, src_len).to(self.device)\n        \n        # Create mask for attention\n        mask = self.create_mask(src)\n        \n        # Encode source sequence\n        encoder_outputs, hidden = self.encoder(src)\n        \n        # Start with <sos> token\n        input = tgt[:, 0]\n        \n        for t in range(1, tgt_len):\n            output, hidden, attention = self.decoder(input, hidden, encoder_outputs, mask)\n            \n            outputs[:, t] = output\n            attentions[:, t] = attention\n            \n            # Teacher forcing\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = tgt[:, t] if teacher_force else top1\n            \n        return outputs, attentions\n\n# Initialize models\nINPUT_DIM = len(src_vocab)\nOUTPUT_DIM = len(tgt_vocab)\nEMB_DIM = 256\nHID_DIM = 512\nN_LAYERS = 2\nDROPOUT = 0.5\n\n# Create models\nencoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\ndecoder_no_attn = DecoderNoAttention(OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\nmodel_no_attn = Seq2SeqNoAttention(encoder, decoder_no_attn, device).to(device)\n\nencoder_attn = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\nattention = Attention(HID_DIM)\ndecoder_attn = DecoderWithAttention(OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, attention, DROPOUT)\nmodel_attn = Seq2SeqWithAttention(encoder_attn, decoder_attn, device).to(device)\n\n# Optimizers\noptimizer_no_attn = optim.Adam(model_no_attn.parameters())\noptimizer_attn = optim.Adam(model_attn.parameters())\n\n# Loss function (ignores padding)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\n# Training function\ndef train(model, data_loader, optimizer, criterion, clip=1.0):\n    model.train()\n    epoch_loss = 0\n    \n    for src, tgt in tqdm(data_loader):\n        src, tgt = src.to(device), tgt.to(device)\n        \n        optimizer.zero_grad()\n        \n        if isinstance(model, Seq2SeqNoAttention):\n            output = model(src, tgt)\n        else:\n            output, _ = model(src, tgt)\n        \n        # Calculate loss excluding the <sos> token\n        output_dim = output.shape[-1]\n        output = output[:, 1:].reshape(-1, output_dim)\n        tgt = tgt[:, 1:].reshape(-1)\n        \n        loss = criterion(output, tgt)\n        loss.backward()\n        \n        # Prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        epoch_loss += loss.item()\n        \n    return epoch_loss / len(data_loader)\n\n# Evaluation function\ndef evaluate(model, data_loader, criterion):\n    model.eval()\n    epoch_loss = 0\n    \n    with torch.no_grad():\n        for src, tgt in data_loader:\n            src, tgt = src.to(device), tgt.to(device)\n            \n            if isinstance(model, Seq2SeqNoAttention):\n                output = model(src, tgt, 0)  # No teacher forcing\n            else:\n                output, _ = model(src, tgt, 0)\n            \n            # Calculate loss\n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim)\n            tgt = tgt[:, 1:].reshape(-1)\n            \n            loss = criterion(output, tgt)\n            epoch_loss += loss.item()\n            \n    return epoch_loss / len(data_loader)\n\n# Translation function\n# def translate_sentence(model, sentence, src_vocab, tgt_vocab, device, max_len=50):\n#     model.eval()\n    \n#     # Tokenize if string\n#     if isinstance(sentence, str):\n#         tokens = tokenize(sentence, 'en')\n#     else:\n#         tokens = sentence\n    \n#     # Convert to indices\n#     src_indices = [src_vocab.word2idx.get(token, 3) for token in tokens]\n#     src_indices = [1] + src_indices + [2]  # Add <sos> and <eos>\n#     src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n    \n#     with torch.no_grad():\n#         if isinstance(model, Seq2SeqNoAttention):\n#             encoder_outputs, hidden = model.encoder(src_tensor)\n            \n#             # Start with <sos>\n#             tgt_idx = [1]\n#             for i in range(max_len):\n#                 tgt_tensor = torch.LongTensor([tgt_idx[-1]]).to(device)\n#                 output, hidden = model.decoder(tgt_tensor, hidden)\n#                 pred_token = output.argmax(1).item()\n#                 tgt_idx.append(pred_token)\n                \n#                 if pred_token == 2:  # <eos>\n#                     break\n                    \n#             translation = [tgt_vocab.idx2word[i] for i in tgt_idx[1:]]  # Skip <sos>\n#             return translation, None\n            \n#         else:  # With attention\n#             encoder_outputs, hidden = model.encoder(src_tensor)\n#             mask = model.create_mask(src_tensor)\n            \n#             # Start with <sos>\n#             tgt_idx = [1]\n#             attentions = []\n            \n#             for i in range(max_len):\n#                 tgt_tensor = torch.LongTensor([tgt_idx[-1]]).to(device)\n#                 output, hidden, attention = model.decoder(tgt_tensor, hidden, encoder_outputs, mask)\n                \n#                 attentions.append(attention.cpu().numpy())\n#                 pred_token = output.argmax(1).item()\n#                 tgt_idx.append(pred_token)\n                \n#                 if pred_token == 2:  # <eos>\n#                     break\n                    \n#             translation = [tgt_vocab.idx2word[i] for i in tgt_idx[1:]]  # Skip <sos>\n#             return translation, np.array(attentions).squeeze(0)\n\ndef translate_sentence(model, sentence, src_vocab, tgt_vocab, device, max_len=50):\n    model.eval()\n    \n    # Tokenize if string\n    if isinstance(sentence, str):\n        tokens = tokenize(sentence, 'en')\n    else:\n        tokens = sentence\n    \n    # Convert to indices\n    src_indices = [src_vocab.word2idx.get(token, 3) for token in tokens]\n    src_indices = [1] + src_indices + [2]  # Add <sos> and <eos>\n    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        if isinstance(model, Seq2SeqNoAttention):\n            encoder_outputs, hidden = model.encoder(src_tensor)\n            \n            # Start with <sos>\n            tgt_idx = [1]\n            for i in range(max_len):\n                tgt_tensor = torch.LongTensor([tgt_idx[-1]]).to(device)\n                output, hidden = model.decoder(tgt_tensor, hidden)\n                pred_token = output.argmax(1).item()\n                tgt_idx.append(pred_token)\n                \n                if pred_token == 2:  # <eos>\n                    break\n                    \n            translation = [tgt_vocab.idx2word[i] for i in tgt_idx[1:]]  # Skip <sos>\n            return translation, None\n            \n        else:  # With attention\n            encoder_outputs, hidden = model.encoder(src_tensor)\n            mask = model.create_mask(src_tensor)\n            \n            # Start with <sos>\n            tgt_idx = [1]\n            attentions = []\n            \n            for i in range(max_len):\n                tgt_tensor = torch.LongTensor([tgt_idx[-1]]).to(device)\n                output, hidden, attention = model.decoder(tgt_tensor, hidden, encoder_outputs, mask)\n                \n                attentions.append(attention.cpu().numpy())\n                pred_token = output.argmax(1).item()\n                tgt_idx.append(pred_token)\n                \n                if pred_token == 2:  # <eos>\n                    break\n                    \n            translation = [tgt_vocab.idx2word[i] for i in tgt_idx[1:]]  # Skip <sos>\n            # Fixed to handle arrays of any shape\n            attention_array = np.array(attentions)\n            return translation, attention_array\n\n\n# Attention visualization\n# def display_attention(sentence, translation, attention):\n#     fig = plt.figure(figsize=(10, 8))\n#     ax = fig.add_subplot(1, 1, 1)\n    \n#     # Remove <eos> token if present\n#     if translation[-1] == '<eos>':\n#         translation = translation[:-1]\n    \n#     # Get attention from first layer\n#     attention = attention[:len(translation), :len(sentence)]\n    \n#     # Plot heatmap\n#     cax = ax.matshow(attention, cmap='viridis')\n#     fig.colorbar(cax)\n    \n#     # Set axes\n#     ax.set_xticklabels([''] + sentence, rotation=90)\n#     ax.set_yticklabels([''] + translation)\n    \n#     # Show label at every tick\n#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    \n#     plt.tight_layout()\n#     return fig\n\n# def display_attention(sentence, translation, attention):\n#     fig = plt.figure(figsize=(10, 8))\n#     ax = fig.add_subplot(1, 1, 1)\n    \n#     # Remove <eos> token if present\n#     if translation[-1] == '<eos>':\n#         translation = translation[:-1]\n    \n#     # Get attention and reshape if needed\n#     if len(attention.shape) > 2:\n#         attention = attention.reshape(len(translation), -1)\n#     attention = attention[:len(translation), :len(sentence)]\n    \n#     # Plot heatmap\n#     cax = ax.matshow(attention, cmap='viridis')\n#     fig.colorbar(cax)\n    \n#     # Set axes\n#     ax.set_xticklabels([''] + sentence, rotation=90)\n#     ax.set_yticklabels([''] + translation)\n    \n#     # Show label at every tick\n#     ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n#     ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    \n#     plt.tight_layout()\n#     return fig\n\ndef display_attention(sentence, translation, attention):\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(1, 1, 1)\n    \n    # Remove <eos> token if present\n    if translation[-1] == '<eos>':\n        translation = translation[:-1]\n    \n    # Debug information\n    print(f\"Attention shape: {attention.shape}\")\n    print(f\"Translation length: {len(translation)}\")\n    print(f\"Sentence length: {len(sentence)}\")\n    \n    # Handle attention arrays of different shapes\n    if len(attention.shape) == 1:\n        # For 1D attention, reshape to 2D with one row\n        attention_2d = attention.reshape(1, -1)\n    elif len(attention.shape) == 2:\n        # Already 2D\n        attention_2d = attention\n    elif len(attention.shape) == 3:\n        # For 3D attention, take the first batch\n        # This assumes batch size is the middle dimension\n        attention_2d = attention[:, 0, :] if attention.shape[1] > 0 else attention.reshape(attention.shape[0], -1)\n    else:\n        # Just flatten to 2D for higher dimensions\n        attention_2d = attention.reshape(attention.shape[0], -1)\n    \n    # Determine how much of the attention to display\n    n_rows = min(len(translation), attention_2d.shape[0])\n    n_cols = min(len(sentence), attention_2d.shape[1])\n    \n    # Extract the part we can display\n    attention_plot = attention_2d[:n_rows, :n_cols]\n    \n    # Plot heatmap\n    cax = ax.matshow(attention_plot, cmap='viridis')\n    fig.colorbar(cax)\n    \n    # Label axes\n    ax.set_xticklabels([''] + sentence[:n_cols], rotation=90)\n    ax.set_yticklabels([''] + translation[:n_rows])\n    \n    # Show label at every tick\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    \n    plt.tight_layout()\n    return fig\n\n\n# Training and evaluation\nN_EPOCHS = 25\ntrain_losses_no_attn = []\nval_losses_no_attn = []\ntrain_losses_attn = []\nval_losses_attn = []\n\nprint(\"Training model without attention...\")\nfor epoch in range(N_EPOCHS):\n    train_loss = train(model_no_attn, train_loader, optimizer_no_attn, criterion)\n    val_loss = evaluate(model_no_attn, val_loader, criterion)\n    train_losses_no_attn.append(train_loss)\n    val_losses_no_attn.append(val_loss)\n    print(f'Epoch {epoch+1}/{N_EPOCHS}, Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}')\n\nprint(\"Training model with attention...\")\nfor epoch in range(N_EPOCHS):\n    train_loss = train(model_attn, train_loader, optimizer_attn, criterion)\n    val_loss = evaluate(model_attn, val_loader, criterion)\n    train_losses_attn.append(train_loss)\n    val_losses_attn.append(val_loss)\n    print(f'Epoch {epoch+1}/{N_EPOCHS}, Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}')\n\n# Plot training curves\nplt.figure(figsize=(12, 6))\nplt.plot(train_losses_no_attn, label='Train Loss (No Attention)')\nplt.plot(val_losses_no_attn, label='Valid Loss (No Attention)')\nplt.plot(train_losses_attn, label='Train Loss (With Attention)')\nplt.plot(val_losses_attn, label='Valid Loss (With Attention)')\nplt.title('Training and Validation Losses')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('loss_comparison.png')\nplt.show()\n\n# Test examples\nexample_sentences = [\n    \"I love learning new languages.\",\n    \"The cat sits on the mat.\",\n    \"She plays the piano very well.\"\n]\n\nfor sentence in example_sentences:\n    print(f\"\\nOriginal: {sentence}\")\n    \n    # Translate with model without attention\n    translation_no_attn, _ = translate_sentence(model_no_attn, sentence, src_vocab, tgt_vocab, device)\n    print(f\"No Attention: {' '.join([t for t in translation_no_attn if t != '<eos>'])}\")\n    \n    # Translate with model with attention\n    translation_attn, attention_weights = translate_sentence(model_attn, sentence, src_vocab, tgt_vocab, device)\n    print(f\"With Attention: {' '.join([t for t in translation_attn if t != '<eos>'])}\")\n    \n    # Visualize attention\n    if attention_weights is not None:\n        fig = display_attention(['<sos>'] + tokenize(sentence, 'en') + ['<eos>'], translation_attn, attention_weights)\n        plt.savefig(f'attention_{sentence[:15].replace(\" \", \"_\")}.png')\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T13:01:07.3535Z","iopub.execute_input":"2025-04-23T13:01:07.353733Z","execution_failed":"2025-04-23T13:05:31.103Z"}},"outputs":[],"execution_count":null}]}
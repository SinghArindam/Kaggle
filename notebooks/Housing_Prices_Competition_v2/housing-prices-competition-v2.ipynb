{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10211,"databundleVersionId":111096,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:15:39.925286Z","iopub.execute_input":"2025-02-01T10:15:39.925691Z","iopub.status.idle":"2025-02-01T10:15:40.956849Z","shell.execute_reply.started":"2025-02-01T10:15:39.925652Z","shell.execute_reply":"2025-02-01T10:15:40.955852Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\npd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\").head","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:18:06.821049Z","iopub.execute_input":"2025-02-01T10:18:06.821379Z","iopub.status.idle":"2025-02-01T10:18:06.856618Z","shell.execute_reply.started":"2025-02-01T10:18:06.821353Z","shell.execute_reply":"2025-02-01T10:18:06.855897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\")\nprint(train.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:19:44.551631Z","iopub.execute_input":"2025-02-01T10:19:44.552029Z","iopub.status.idle":"2025-02-01T10:19:44.575585Z","shell.execute_reply.started":"2025-02-01T10:19:44.551999Z","shell.execute_reply":"2025-02-01T10:19:44.574617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n\n# Load data\ntrain = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv\")\n\n# Save IDs and target\ntrain_ids = train['Id']\ntest_ids = test['Id']\ntarget = np.log1p(train['SalePrice'])  # Log transformation\n\n# Remove IDs and target from features\ntrain.drop(['Id', 'SalePrice'], axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)\n\n# Combine data for preprocessing\nall_data = pd.concat([train, test], axis=0)\n\n# ... (previous imports and data loading remain the same)\n\n# Handle missing values - CORRECTED VERSION\nmissing_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n                'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n                'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n\n# Corrected missing value handling\nfor col in missing_none:\n    all_data[col] = all_data[col].fillna('None')  # Assign back instead of inplace\n\n# Fill with mode for other categoricals (corrected)\ncat_cols = all_data.select_dtypes(include='object').columns\nfor col in cat_cols:\n    all_data[col] = all_data[col].fillna(all_data[col].mode().iloc[0])  # Use iloc[0] for clarity\n\n# Fill numerical missing values (corrected)\nnum_cols = all_data.select_dtypes(exclude='object').columns\nfor col in num_cols:\n    all_data[col] = all_data[col].fillna(all_data[col].median())\n\n# ... (rest of the code remains the same)\n\n# Feature engineering\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\nall_data['TotalBath'] = (all_data['FullBath'] + 0.5*all_data['HalfBath'] +\n                         all_data['BsmtFullBath'] + 0.5*all_data['BsmtHalfBath'])\nall_data['Age'] = all_data['YrSold'] - all_data['YearBuilt']\nall_data['RemodAge'] = all_data['YrSold'] - all_data['YearRemodAdd']\n\n# Handle skewed features\nskewed_feats = all_data[num_cols].apply(lambda x: x.skew()).sort_values(ascending=False)\nhigh_skew = skewed_feats[abs(skewed_feats) > 0.75]\nfor feat in high_skew.index:\n    all_data[feat] = np.log1p(all_data[feat])\n\n# Encode categorical variables\nordinal_mapping = {\n    'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},\n    'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},\n    'BsmtQual': {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'None':0},\n    'KitchenQual': {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1},\n    # Add other ordinal mappings\n}\n\nfor col, mapping in ordinal_mapping.items():\n    all_data[col] = all_data[col].map(mapping)\n\n# One-hot encode remaining categoricals\nall_data = pd.get_dummies(all_data)\n\n# Split back into train and test\nX_train = all_data.iloc[:len(train)]\nX_test = all_data.iloc[len(train):]\n\n# Outlier removal (example for GrLivArea)\noutliers = X_train[(X_train['GrLivArea'] > 4000) & (target < np.log1p(300000))].index\nX_train = X_train.drop(outliers)\ntarget = target.drop(outliers)\n\n# Cross-validation setup\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# XGBoost model\nxgb_preds = np.zeros(X_test.shape[0])\nxgb_rmse = []\n\nfor train_idx, val_idx in kf.split(X_train):\n    X_train_kf, X_val_kf = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_train_kf, y_val_kf = target.iloc[train_idx], target.iloc[val_idx]\n    \n    dtrain = xgb.DMatrix(X_train_kf, y_train_kf)\n    dval = xgb.DMatrix(X_val_kf, y_val_kf)\n    dtest = xgb.DMatrix(X_test)\n    \n    params = {\n        'eta': 0.01,\n        'max_depth': 4,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'objective': 'reg:squarederror',\n        'eval_metric': 'rmse',\n        'seed': 42\n    }\n    \n    model = xgb.train(params, dtrain, num_boost_round=10000,\n                      evals=[(dtrain, 'train'), (dval, 'val')],\n                      early_stopping_rounds=100, verbose_eval=False)\n    \n    xgb_preds += np.expm1(model.predict(dtest)) / kf.n_splits\n    xgb_rmse.append(model.best_score)\n\n# Corrected LightGBM training code\nlgb_preds = np.zeros(X_test.shape[0])\nlgb_rmse = []\n\nfor train_idx, val_idx in kf.split(X_train):\n    X_train_kf, X_val_kf = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_train_kf, y_val_kf = target.iloc[train_idx], target.iloc[val_idx]\n    \n    train_data = lgb.Dataset(X_train_kf, label=y_train_kf)\n    val_data = lgb.Dataset(X_val_kf, label=y_val_kf, reference=train_data)\n    \n    params = {\n        'learning_rate': 0.01,\n        'num_leaves': 31,\n        'max_depth': -1,\n        'min_child_samples': 20,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 5,\n        'metric': 'rmse',\n        'random_state': 42,\n        'early_stopping_round': 100,\n        'verbosity': -1  # Suppresses LightGBM output\n    }\n    \n    # Corrected training call\n    model = lgb.train(\n        params,\n        train_data,\n        valid_sets=[val_data],  # Only validation set needed\n        num_boost_round=10000,\n        callbacks=[lgb.log_evaluation(period=0)]  # Suppresses output\n    )\n    \n    lgb_preds += np.expm1(model.predict(X_test, num_iteration=model.best_iteration)) / kf.n_splits\n    lgb_rmse.append(model.best_score['valid_0']['rmse'])  # Changed to 'valid_0'\n\n# Ensemble predictions\nfinal_preds = 0.5 * xgb_preds + 0.5 * lgb_preds\n\n# Create submission\nsubmission = pd.DataFrame({'Id': test_ids, 'SalePrice': final_preds})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T10:48:36.429449Z","iopub.execute_input":"2025-02-01T10:48:36.429841Z","iopub.status.idle":"2025-02-01T10:49:25.289659Z","shell.execute_reply.started":"2025-02-01T10:48:36.429798Z","shell.execute_reply":"2025-02-01T10:49:25.288764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
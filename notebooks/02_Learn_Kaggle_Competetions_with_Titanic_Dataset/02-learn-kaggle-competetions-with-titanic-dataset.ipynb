{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T12:19:17.977716Z","iopub.execute_input":"2025-04-28T12:19:17.978052Z","iopub.status.idle":"2025-04-28T12:19:20.301754Z","shell.execute_reply.started":"2025-04-28T12:19:17.977989Z","shell.execute_reply":"2025-04-28T12:19:20.300352Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**DISCLAIMER:**\n\nI have used AI to organize topics and what should come after what and also to format the writeup (I am currently learning markdown), the content is written by me alone, I have mentioned the resources i referenced at last. Many thanks to those, for helping me get a better understanding.","metadata":{}},{"cell_type":"markdown","source":"# 0. Pre-Introduction","metadata":{}},{"cell_type":"markdown","source":"In this notebook I explain all different methods typically used in Competetions, by implementing on Titanic Dataset.\n\nI have tried my best to explain things in as detail as i can, also got it formatted and organized by google gemini 2.0 Flash. Got some ideas from grok 3, and researched different sources. \n\nIn my opinion this is probably one of the few notebooks with such detailed explanation, especially for beginners. Or this might be the only one as of the date of Writing.\n\nMaking it public on 28 April 2025 IST. (for more Details see Changelog)","metadata":{}},{"cell_type":"markdown","source":"You are invited to **Upvote** this Notebook if you like it, **Downvote** if you Dislike it.\n\nIn Either case I invite Suggestions, advises, improvements, etc. from everyone.\n\nI aim to make it a complete notebook for basics.\n\nI will try my best to add all the inputs to the best of my abilities.\n\n> #### **And Yes this is going to be long and weary.**","metadata":{}},{"cell_type":"markdown","source":"And I will include some of the good practices, including but not limited to:\n\n* sitting posture\n* eye relaxation and exercise\n\nperiodically in my notebook.","metadata":{}},{"cell_type":"markdown","source":"### >TLDR; Basics for beginners<","metadata":{}},{"cell_type":"markdown","source":"## Pre-Requisites\n* Basics of\n    * Python\n    * Matplorlib\n    * Numpy\n    * Seaborn\n    * scikit-learn\n    * pytorch\n    * jupyter\n    * Kaggle\n    * Windows/Linux/MacOS whichever you are using\n* An alert brain\n* A way to operate your computer with ease. (hands, keyboard, mouse, etc.)\n* Focus/Attention","metadata":{}},{"cell_type":"markdown","source":"### What You Need\n- **Python**: The programming language we’ll use. Download it from [python.org](https://www.python.org/) if you don’t have it.\n- **Jupyter Notebook**: A tool to write and run code in chunks. It’s like a digital notebook.\n- **Libraries**:\n  - `pandas`: For handling data (like Excel but cooler).\n  - `numpy`: For math stuff.\n  - `matplotlib` and `seaborn`: For making pretty charts.\n  - `torch` (PyTorch): For building machine learning models.\n  - `scikit-learn`: For extra tools like splitting data.\n- **VS Code**: I personally use it for writing jupyter notebooks.","metadata":{}},{"cell_type":"markdown","source":"### Installing Everything\nOpen your terminal (or Command Prompt on Windows) and run:\n\n```bash\npip install pandas numpy matplotlib seaborn torch scikit-learn\n```\n\nIf you’re using Jupyter Notebook, install it too:\n\n```bash\npip install jupyter\n```\n\nThen, start Jupyter by typing:\n\n```bash\njupyter notebook\n```\n\nThis opens a browser window. Click “New” → “Python 3” to create a notebook. You’re ready to code!\n\n### **Or Simply Use Kaggle Notebook.**","metadata":{}},{"cell_type":"markdown","source":"# 1. Introduction","metadata":{}},{"cell_type":"markdown","source":"The Titanic dataset is perhaps the most famous starting point for anyone new to Kaggle or machine learning competitions. But before diving into code, let's stop and ask: \n- *Why start with Titanic?*\n- *Why do so many people start with Titanic?*\n- *What are we really trying to learn here?*","metadata":{}},{"cell_type":"markdown","source":"### **Why the Titanic?**\n\nThe story of the Titanic is well known: in 1912, this \"unsinkable\" ship tragically sank after hitting an iceberg, resulting in the loss of over 1,500 lives out of more than 2,200 passengers and crew. \n\nBut beyond the history, the Titanic dataset is perfect for learning the basics of Data Science and ML (and Deep Learning, will be added in later part), because:\n- The dataset is small and easy to understand, so you can quickly try out ideas.\n- The competition is open indefinitely, so there’s no deadline pressure. Competetion, because getting a good rank motivates people.\n- The problem-predicting who survived-is simple, but not trivial. It requires a bit of thinking, experimenting, and learning new techniques.","metadata":{}},{"cell_type":"markdown","source":"### **What’s the Goal?**\n\nOur main task is to predict whether a given passenger survived the disaster, using information like their age, sex, ticket class, and more. Kaggle provides us with two datasets:\n\n- **train.csv**: Contains information about passengers *and* whether they survived (the \"ground truth\").\n- **test.csv**: Contains information about other passengers, but *not* whether they survived. Our job is to predict these outcomes and submit our prediction to Kaggle for scoring.\n- **gender_submission.csv**: A sample submission file to show the format.\n\nOur goal is to build a model that predicts `Survived`(the target) as accurately as possible. The Competetion scores you on **accuracy** (how many predictions you get right).","metadata":{}},{"cell_type":"markdown","source":"### **Why Do We Do Each Step?**\n\nWhen you look at a competition like this, it’s easy to compulsively want to jump straight into coding. \n\nBut let's pause and reflect, every action we take should have a purpose. \n\nFirst Let's Discuss what we usually do:\n\n- **Loading the data**:\n  Before we can do anything, we need to see what information we have, what will we perform our operations on? we need the data right? hence we need to load the Data.\n- **Exploring the data**:\n  We need to understand the data, what is it?, what does it look like?, what columns it has?, the values in respective column, and what can we gather by looking at the data?, that is the entire context behind the numbers. Eg. Who were the passengers? Are there patterns in who survived?\n- **Cleaning the data**:\n  Real-world data is messy. There are missing ages, strange ticket numbers, and more. We need to tidy this up so that our models don’t get confused. Btw, this is also a reason why we explore the data.\n- **Feature engineering**:\n  Sometimes, the raw data isn’t enough. We can create new features-like extracting a person’s title from their name (Mr, Mrs, etc.)-to give our models more clues.\n- **Building models**:\n  We try different algorithms to see which one best captures the patterns in the data. These models are basically mathematical models (analogy, think of them like functions or relations or mappings or algorithms etc), we implement them or use a library or module (basically a .py file with code already written in it for us) to implement them.\n- **Evaluating and comparing**:\n  No model is perfect. We compare their results to see which is most reliable. If we knew the exact steps to take to get 100% accurate output, we would have used that algorithm instead, and not bother about ML or DL, but in real world we do not have the entire data including all possibilities that could 100% tell the correct output for any case as no case would have been outside of that data (as it is the most complete data), Obviously, we cannot do that, so we resort to approximation or finding hidden logics or patterns in limited data, for this we train a model that can give correct or approximately correct output most of the time. Here, let's talk about titanic dataset, lets assume we have a complete data of all the people in the world that if they had boarded the ship, had they survived or not, then we could accurately say that this person X would 100% survive or Y would 100% not survive. But we only have data of people who boarded that ship (that is, a very small subset of complete data), and even among that we have some discrepancy, like someones age is missing etc. So we try to train a model that will closely(more accurately) predict whether an unknown person will survive or not.\n- **Submission**:\n  Finally, we take our best model, make predictions on the test set, and submit them to Kaggle to see how we did.","metadata":{}},{"cell_type":"markdown","source":"### **What Will We Learn?**\n\nBy working through this notebook, we’ll learn:\n\n- How to load and explore data using Python and pandas.\n- How to visualize patterns and spot missing data.\n- How to clean and preprocess data for machine learning.\n- How to engineer new features to improve model performance.\n- How to train, compare, and evaluate different machine learning models.\n- How to make a Kaggle submission and interpret the results.","metadata":{}},{"cell_type":"markdown","source":"### **Thinking Like a Data Scientist**\n\nThroughout this journey, we’ll constantly ask ourselves:\n\n- *What is the data telling me or what can I infer from this data?*\n- *Why am I making this decision, why should I do this instead of that?*\n- *How can I improve my results?*\n\nThis mindset-of questioning, exploring, and iterating-is at the heart of data science and ML/DL.","metadata":{}},{"cell_type":"markdown","source":"# 2. Data","metadata":{}},{"cell_type":"markdown","source":"In this section, we prepare and explore the Titanic dataset by converting key variables into PyTorch tensors and visualizing their distributions to gain insights into the data structure and potential patterns for modeling.  \n\nFirst, we read the CSV files using pandas and inspect the shape of each DataFrame to confirm the number of samples and features available for training and testing.  \n\nNext, we isolate numeric features-such as PassengerId, Pclass, Age, SibSp, Parch, and Fare-and convert them into float32 PyTorch tensors, which is the required format for downstream neural network inputs and tensor-based operations.  \n\nTo understand the central tendencies and spread of these features, we call pandas’ `describe()` on the training set, giving us quick access to mean, standard deviation, and quartile information.  \n\nFinally, we employ seaborn and matplotlib to visualize:  \n- The distribution of **Age** (to spot skewness or outliers)  \n- The distribution of **Fare** (to observe fare ranges and potential bins)  \n- Overall **survival counts** (to see class imbalance)  \n- **Survival rate by sex** (to identify strong predictors)  \nThese plots reveal patterns that will inform our feature engineering steps.  ","metadata":{}},{"cell_type":"markdown","source":"## 2.1. EDA - Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"Let's Import the required libraries first. *(Ask Yourself Why?)*","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T13:29:02.530113Z","iopub.execute_input":"2025-04-28T13:29:02.530526Z","iopub.status.idle":"2025-04-28T13:29:02.536373Z","shell.execute_reply.started":"2025-04-28T13:29:02.530497Z","shell.execute_reply":"2025-04-28T13:29:02.535316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Dont mind this for now, this is not directly relevant to our aim here.","metadata":{}},{"cell_type":"code","source":"# Set plot style\nsns.set(style=\"whitegrid\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T13:24:01.311588Z","iopub.execute_input":"2025-04-28T13:24:01.311922Z","iopub.status.idle":"2025-04-28T13:24:09.533827Z","shell.execute_reply.started":"2025-04-28T13:24:01.311897Z","shell.execute_reply":"2025-04-28T13:24:09.532684Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Now Let’s load the Titanic data and take a peek.","metadata":{}},{"cell_type":"code","source":"# Load data with pandas\ntrain_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df  = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T13:24:01.311588Z","iopub.execute_input":"2025-04-28T13:24:01.311922Z","iopub.status.idle":"2025-04-28T13:24:09.533827Z","shell.execute_reply.started":"2025-04-28T13:24:01.311897Z","shell.execute_reply":"2025-04-28T13:24:09.532684Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Let's Look at how we approach anything, I will only do this once, later we will do shorter explanations (probably, or i might just continue like this)**\n\nLet's break down these two lines of Python code step-by-step.\n\nImagine we have some information written down, maybe in a notebook or on index cards. To do anything useful with that information using a computer, we first need to get it *into* the computer's memory in a way the computer understands.\n\nThese lines of code are doing exactly that: **loading data from files into the computer's memory**.\n\nLet's look at the first line:\n\n```python\ntrain_df = pd.read_csv('/kaggle/input/titanic/train.csv')\n```\n\n**1. What's the Goal Here?**\n\nThe goal is to read data from a specific file (`train.csv`) and store it somewhere in our program so we can work with it later.\n\n**2. How is Data Often Stored?**\n\nWhen dealing with data, especially structured data (like tables with rows and columns), a very common file format is **CSV**.\n\n*   **What does CSV stand for?** Comma-Separated Values.\n*   **What does that mean?** It's usually a plain text file (you know, like that .txt one) where each line represents a row of data, and the values (the pieces of information) within that row are separated by commas. Let's think of it like a simple spreadsheet saved as text.\n*   **Why use CSV?** It's simple, human-readable (to some extent), and widely supported by different programs (spreadsheets like Excel or Google Sheets, databases, programming languages). Most importantly, its simple.\n\n**3. How Do We Read a CSV File in Python?**\n\nPython itself has built-in ways to read files line by line. We *could* manually open the file, read each line, split the line by commas, and try to organize it. By manually I mean like using string methods or simple for loops, etc., like we split a row of a csv using string_1.split(',') to create it into a list then maybe store these lists into a bigger list making something like a 2d list. Most importantly, these libraries optimize this, that is make it faster.\n\n*   **But wait, is that the best way?** Probably not! Doing it manually can be tedious. What if a value itself contains a comma (like \"Doe, John\")? What if some values are numbers and some are text? We'd have to handle all these details ourselves. It sounds like a lot of work and potential for mistakes.\n\n*   **Is there a specialized tool for this?** Yes! This is where **libraries** come in. Libraries are collections of pre-written code designed to solve common problems. For handling data in Python, especially table-like data, probably the most popular in use and powerful library is **pandas**.\n\n**4. What is `pd`?**\n\n*   You see `pd` at the beginning. Where did that come from? Certainly, it came from *before*, **Remember** there was a line like this:\n    ```python\n    import pandas as pd\n    ```\n*   **What does `import pandas` do?** It tells Python: \"Hey, I want to use the functionality available in the 'pandas' library.\" basically, for simplicity, think of it like manually copying the functions in pandas into the current file (current scope) and then calling those functions (here calling them like pandas.dothis() etc.), this is a better, more systemized way to do it.\n*   **What does `as pd` do?** It's like giving the library a nickname. Instead of typing `pandas` every time we want to use something from it, we can just type the shorter alias `pd`. It's a very common convention among Python data analysts and scientists. Think of it as referring to your friend \"Alexander\" as \"Alex\" for short.\n\n**5. What is `.read_csv()`?**\n\n*   The dot (`.`) after `pd` means we are accessing something *inside* the pandas library (which we're calling `pd`).\n*   `read_csv` is a **function** provided by pandas. A function is a block of code that performs a specific task.\n*   **What task does `read_csv` perform?** Its specific job is to read data from a Comma-Separated Values (CSV) file. It's designed to handle all those tricky details we worried about earlier (commas within values, different data types, missing values, etc.) automatically and efficiently.\n\n**6. What is `('/kaggle/input/titanic/train.csv')`?**\n\n*   This is the **argument** we are giving to the `read_csv` function. An argument is the input that a function needs to do its job.\n*   **What does this specific argument mean?** It's a **string** (a piece of text, denoted by the quotes `' '`) that represents the **file path**.\n*   **What's a file path?** It's like an address that tells the computer exactly where to find the file on the system's storage (like a hard drive).\n    *   `/`: Often signifies the root or top-level directory.\n    *   `kaggle/`: A folder named 'kaggle'.\n    *   `input/`: A sub-folder inside 'kaggle' named 'input'.\n    *   `titanic/`: A sub-folder inside 'input' named 'titanic'.\n    *   `train.csv`: The actual name of the file we want to read.\n*   **Why this specific path?** Well Duh! This path contains our files that we will work on. Well, it's where Kaggle typically makes competition data available to the code environments running on their platform, you know after you create a notebook from kaggle competetion eg titanic here.\n*   **So, what are we telling `pd.read_csv`?** We're telling it: \"Please read the data from the file named `train.csv` which is located inside the `titanic` folder, which is inside the `input` folder, which is inside the `kaggle` folder, starting from the root directory.\"\n\n**7. What does `train_df =` mean?**\n\n*   `=` is the **assignment operator** in Python. It takes the result of whatever is on the right side and stores it in a **variable** named on the left side.\n*   `train_df` is the name we've chosen for our variable.\n*   **What is the result of `pd.read_csv(...)`?** When pandas reads a CSV file, it organizes the data into a special structure called a **DataFrame**.\n*   **What's a DataFrame?** It's the primary data structure in pandas. Think of it as a highly optimized and flexible table in memory, similar to a spreadsheet or a database table. It has rows and columns, and you can easily select data, manipulate it, calculate things, etc.\n*   **Why the name `train_df`?**\n    *   `train`: Because the data comes from `train.csv`, which typically contains the data used to \"train\" a machine learning model (i.e., teach it patterns).\n    *   `df`: This is a very common suffix used by convention to indicate that the variable holds a pandas DataFrame. It makes the code easier to read, as you immediately know the type of data stored in `train_df`.\n\n**So what does the first line say?:**\n\n\"Import the pandas library and call it `pd`. Then, use the `read_csv` function from pandas to read the file located at `/kaggle/input/titanic/train.csv`. Take the resulting table-like structure (a DataFrame) and store it in a variable named `train_df`.\"","metadata":{}},{"cell_type":"markdown","source":"Now, let's look at the second line:\n\n```python\ntest_df  = pd.read_csv('/kaggle/input/titanic/test.csv')\n```\n\n> \"Hmm, this looks almost identical to the first line. What's different?\"\n\n*   **`pd.read_csv(...)`**: We're using the same function from the pandas library (`pd`) to read another CSV file.\n*   **`'/kaggle/input/titanic/test.csv'`**: The file path is slightly different. It's still in the same directory (`/kaggle/input/titanic/`), but this time the filename is `test.csv`.\n    *   **Why `test.csv`?** In many data science tasks, especially competitions (like the Titanic), the data is split into two sets:\n        *   `train.csv`: Contains data *with* the answers (e.g., it includes whether each passenger survived). This is used to build and train a predictive model.\n        *   `test.csv`: Contains data *without* the answers. This is used to test how well the trained model performs on unseen data. The goal is usually to predict the answers for this dataset.\n*   **`test_df =`**: We're assigning the result (the DataFrame created from `test.csv`) to a *new* variable named `test_df`.\n    *   **Why `test_df`?** Following the same logic, `test` indicates it's the testing data, and `df` indicates it's a DataFrame.\n\n**What does the second line say?:**\n\n\"Use the `read_csv` function from pandas (`pd`) again. This time, read the file located at `/kaggle/input/titanic/test.csv`. Store the resulting DataFrame in a new variable named `test_df`.\"","metadata":{}},{"cell_type":"markdown","source":"These two lines are fundamental starting points, you will almost always use these.\n\nThey use the powerful pandas library to:\n\n1.  Load a dataset intended for training (`train.csv`) into a DataFrame called `train_df`.\n2.  Load a separate dataset intended for testing (`test.csv`) into another DataFrame called `test_df`.\n\nOnce these lines are executed, the data from those files is now readily available inside the Python program, stored in these two DataFrame variables (`train_df` and `test_df`), ready for inspection, cleaning, analysis, and modeling and whatever we want to do.","metadata":{}},{"cell_type":"code","source":"# Display basic info\nprint(f\"Train data shape: {train_df.shape}\")\nprint(f\"Test data shape:  {test_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T13:56:38.285918Z","iopub.execute_input":"2025-04-28T13:56:38.286339Z","iopub.status.idle":"2025-04-28T13:56:38.292354Z","shell.execute_reply.started":"2025-04-28T13:56:38.286312Z","shell.execute_reply":"2025-04-28T13:56:38.291196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert numeric columns to PyTorch tensors\nnumeric_cols = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\ntrain_tensor = torch.tensor(train_df[numeric_cols].values, dtype=torch.float32)\nprint(f\"Train tensor shape: {train_tensor.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T13:56:41.544887Z","iopub.execute_input":"2025-04-28T13:56:41.545328Z","iopub.status.idle":"2025-04-28T13:56:41.55576Z","shell.execute_reply.started":"2025-04-28T13:56:41.545299Z","shell.execute_reply":"2025-04-28T13:56:41.554542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Basic statistics\nprint(train_df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T13:56:45.73845Z","iopub.execute_input":"2025-04-28T13:56:45.738867Z","iopub.status.idle":"2025-04-28T13:56:45.771735Z","shell.execute_reply.started":"2025-04-28T13:56:45.738837Z","shell.execute_reply":"2025-04-28T13:56:45.770527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T13:57:25.178208Z","iopub.execute_input":"2025-04-28T13:57:25.178575Z","iopub.status.idle":"2025-04-28T13:57:25.189399Z","shell.execute_reply.started":"2025-04-28T13:57:25.178551Z","shell.execute_reply":"2025-04-28T13:57:25.187821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df.tail())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T13:57:51.683316Z","iopub.execute_input":"2025-04-28T13:57:51.683877Z","iopub.status.idle":"2025-04-28T13:57:51.695628Z","shell.execute_reply.started":"2025-04-28T13:57:51.683839Z","shell.execute_reply":"2025-04-28T13:57:51.694435Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> *What Do you See?*\n\nThe dataset has rows (passengers) and columns (features like `Age`, `Sex`, etc.).\n\n**Here’s what the columns mean:**\n\n- `PassengerId`: Unique ID for each passenger.\n- `Survived`: 1 (survived) or 0 (didn’t survive).\n- `Pclass`: Ticket class (1 = 1st class, 2 = 2nd, 3 = 3rd).\n- `Name`: Passenger’s name.\n- `Sex`: Male or female.\n- `Age`: Passenger’s age.\n- `SibSp`: Number of siblings/spouses aboard.\n- `Parch`: Number of parents/children aboard.\n- `Ticket`: Ticket number.\n- `Fare`: Ticket price.\n- `Cabin`: Cabin number (often missing).\n- `Embarked`: Port they boarded from (C = Cherbourg, Q = Queenstown, S = Southampton).","metadata":{}},{"cell_type":"markdown","source":"#### **Now Let's Now Visualize it**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(train_df['Age'].dropna(), kde=True)\nplt.title('Age Distribution')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T13:58:54.285506Z","iopub.execute_input":"2025-04-28T13:58:54.285838Z","iopub.status.idle":"2025-04-28T13:58:54.678665Z","shell.execute_reply.started":"2025-04-28T13:58:54.285817Z","shell.execute_reply":"2025-04-28T13:58:54.677273Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**What do you see?**\n- Most passengers are around 20–40 years old.\n- The **kde** (curvy line) shows the shape. If it’s skewed (lopsided), we might need to adjust `Age` later.","metadata":{}},{"cell_type":"markdown","source":"**1. What's the Goal here?**\n\nThe goal is to create a **histogram** showing how ages are distributed in the training dataset.\n\n*   **What's a histogram?** Imagine you have a bunch of ages, like 25, 30, 25, 40, 25, 35... A histogram groups these ages into ranges (e.g., 20-25, 25-30, 30-35, etc.) and then shows how many people fall into each range. It's a way to see the shape of the data, like whether most people are young, old, or somewhere in the middle.\n*   **Why visualize data?** Because sometimes seeing a picture is much easier than looking at a table full of numbers! Visualizations help us understand patterns, spot outliers (unusual data points), and get a feel for the data.\n\n**2. What are `plt` and `sns`?**\n\n*   Just like with `pd` (for pandas), `plt` and `sns` are aliases for libraries:\n    *   `plt` is short for `matplotlib.pyplot`.\n        *   **What's `matplotlib`?** It's the most fundamental plotting library in Python. Think of it as the foundation for creating all sorts of graphs and charts.\n        *   **What's `pyplot`?** It's a module within `matplotlib` that provides a convenient way to create plots in a style similar to MATLAB (another popular numerical computing environment).\n    *   `sns` is short for `seaborn`.\n        *   **What's `seaborn`?** It's a library built on top of `matplotlib`. Seaborn makes it easier to create statistically informative and visually appealing plots. It has a higher-level interface, meaning you can often create more complex plots with less code compared to using `matplotlib` directly. It's like having a more advanced toolset for plotting.\n*   **How do we know they're libraries?** Somewhere before this code, we probably had lines like:\n    ```python\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    ```\n    These lines import the libraries and give them the aliases `plt` and `sns`, respectively.\n\n**3. `plt.figure(figsize=(10, 6))`**\n\n*   **What's `plt.figure()`?** This is a function from `matplotlib.pyplot` (`plt`) that creates a new figure. Think of a figure as the overall canvas or window where your plot will be drawn.\n*   **What's `figsize=(10, 6)`?** This is an argument to the `figure()` function that specifies the size of the figure.\n    *   `figsize` is a keyword argument, meaning we're explicitly telling the function what we want to set.\n    *   `(10, 6)` is a **tuple** (an ordered, immutable sequence of values) that represents the width and height of the figure, respectively, in inches.\n    *   **Why set the figure size?** The default size might not be ideal for all plots. We often adjust it to make the plot easier to read and fit better on the screen or in a report.\n*   **So, what does this line do?** It creates a new plotting figure that is 10 inches wide and 6 inches high.\n\n**4. `sns.histplot(train_df['Age'].dropna(), kde=True)`**\n\n*   **What's `sns.histplot()`?** This is the star of the show! It's a function from the `seaborn` library (`sns`) specifically designed to create histograms (and related plots).\n*   **What's `train_df['Age']`?**\n    *   Remember `train_df` is our DataFrame containing the training data.\n    *   `['Age']` is how we select a specific **column** from the DataFrame. In this case, we're selecting the column named \"Age,\" which presumably contains the ages of the passengers.\n    *   **Why select the \"Age\" column?** Because we want to visualize the distribution of ages, so that's the data we need to use for the histogram.\n*   **What's `.dropna()`?**\n    *   This is a method (a function associated with an object) that we're calling on the result of `train_df['Age']`.\n    *   **What could be in `train_df['Age']` besides ages?** It's possible that some age values are missing (e.g., recorded as \"NaN\" - Not a Number).\n    *   **Why remove missing values?** If we include \"NaN\" values in the histogram, it might mess up the plot or even cause an error. `dropna()` creates a new series (a one-dimensional labeled array) containing only the non-missing age values.\n    *   Think of it as cleaning up the data before plotting.\n*   **What's `kde=True`?**\n    *   `kde` is another keyword argument to `histplot()`.\n    *   **What's KDE?** It stands for Kernel Density Estimation. It's a way to estimate the probability density function of a continuous random variable (in this case, age).\n    *   **What does that mean in plain English?** Instead of just showing the bars of the histogram, KDE draws a smooth curve that represents the overall shape of the distribution. It's like a smoothed-out version of the histogram that can make it easier to see the general trend.\n    *   **Why use KDE?** It can help you see the underlying shape of the distribution more clearly, especially if the histogram bars are a bit noisy (uneven).\n*   **So, what does this line do?** It tells seaborn to create a histogram using the \"Age\" column from the `train_df` DataFrame, after removing any missing values. It also adds a Kernel Density Estimation (KDE) curve to the plot, showing a smoothed estimate of the age distribution.\n\n**5. `plt.title('Age Distribution')`**\n\n*   **What's `plt.title()`?** This is a function from `matplotlib.pyplot` (`plt`) that sets the title of the plot.\n*   **What's `'Age Distribution'`?** This is the text we want to use as the title of the plot. It's a string.\n*   **Why add a title?** A title helps people quickly understand what the plot is showing.\n*   **So, what does this line do?** It adds the title \"Age Distribution\" to our plot.\n\n**6. `plt.show()`**\n\n*   **What's `plt.show()`?** This is a function from `matplotlib.pyplot` (`plt`) that displays the plot.\n*   **Why is this needed?** Matplotlib doesn't automatically show the plot as soon as you create it. You need to explicitly call `plt.show()` to make the plot appear on the screen (or save it to a file).\n*   **So, what does this line do?** It tells matplotlib to display the histogram we've created.\n\n**Putting It All Together:**\n\n>\"Okay, I want to see how the ages are distributed in my training data.\n\n>1.  First, I'll create a blank canvas (a figure) that's a good size for my plot.\n>2.  Then, I'll use seaborn to draw a histogram of the \"Age\" column from my `train_df`. But first, I'll remove any missing age values so they don't mess things up. And I'll also add a smooth curve (KDE) to show the overall shape of the distribution.\n>3.  I'll give my plot a title so people know what it's about.\n>4.  Finally, I'll tell matplotlib to actually show me the plot. \"\n\nAnd that's how this code creates a visual representation of the age distribution!","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(train_df['Fare'], kde=True)\nplt.title('Fare Distribution')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T13:58:57.298826Z","iopub.execute_input":"2025-04-28T13:58:57.299222Z","iopub.status.idle":"2025-04-28T13:58:57.724993Z","shell.execute_reply.started":"2025-04-28T13:58:57.299196Z","shell.execute_reply":"2025-04-28T13:58:57.724027Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Similar explanation for this, so i will not explain again.\n\n**Now Let's Look at What’s this?**\n- `Fare` is super skewed—most are cheap tickets, but a few are crazy expensive.\n- This suggests we might bin `Fare` (group into ranges) later.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nsns.countplot(x='Survived', data=train_df)\nplt.title('Survival Counts')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T13:59:01.050315Z","iopub.execute_input":"2025-04-28T13:59:01.050739Z","iopub.status.idle":"2025-04-28T13:59:01.205771Z","shell.execute_reply.started":"2025-04-28T13:59:01.05071Z","shell.execute_reply":"2025-04-28T13:59:01.20474Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**What’s the deal?**\n- More people didn’t survive (0) than survived (1).\n- This **class imbalance** means our model needs to handle fewer survivors carefully.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nsns.barplot(x='Sex', y='Survived', data=train_df)\nplt.title('Survival Rate by Sex')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T13:59:09.133918Z","iopub.execute_input":"2025-04-28T13:59:09.13428Z","iopub.status.idle":"2025-04-28T13:59:09.345733Z","shell.execute_reply.started":"2025-04-28T13:59:09.134257Z","shell.execute_reply":"2025-04-28T13:59:09.34468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**What do we observe here?**\n- Women (`female`) survived way more than men (`male`).\n- `Sex` is likely a super important feature for our model.","metadata":{}},{"cell_type":"markdown","source":"#### These plots tell us:\n- `Age` and `Fare` need careful handling (skewness, outliers).\n- `Sex` is a big deal for survival.\n- We have more non-survivors, so our model shouldn’t just guess “didn’t survive” all the time.\n\nAlright! Now let's move forward.","metadata":{}},{"cell_type":"markdown","source":"## 2.2. Preprocessing the Data","metadata":{}},{"cell_type":"markdown","source":">You can't bake a delicious cake with rotten eggs or unmixed ingredients. Similarly, a machine learning model can't learn well from messy, inconsistent, or incomplete data.\n\nThis section is all about cleaning, transforming, and preparing the Titanic data so that our model can learn effectively.","metadata":{}},{"cell_type":"markdown","source":"The goal of this entire section is to transform the raw data from the `train_df` (and `test_df`) DataFrames into a format that's suitable for a machine learning model.","metadata":{}},{"cell_type":"markdown","source":"**Subsections Breakdown:**\n\nThis \"Section 2.2\" is divided into several subsections, each tackling a specific aspect of data preparation:\n\n1.  **Handling Missing Values:** Dealing with empty or incomplete data entries.\n2.  **Encoding Categorical Variables:** Converting text-based categories into numerical representations. Because a model can only read numbers not text.\n3.  **Feature Engineering:** Creating new, potentially more informative features from existing ones.\n4.  **Dropping Unneeded Columns:** Removing irrelevant or redundant information.\n5.  **Scaling Numeric Features:** Bringing numerical features onto a comparable scale.","metadata":{}},{"cell_type":"markdown","source":"### 2.2.1 Handling Missing Values\n\n*   **What's the Problem?** Missing values (often represented as \"NaN\" in pandas) can cause problems for machine learning models. Most algorithms can't handle them directly, and they might also indicate something important about the data itself (e.g., maybe passengers with missing ages had something in common).\n*   **Our Strategy:** The approach here is to fill in some missing values with reasonable estimates and to drop one column that has *too* many missing values.","metadata":{}},{"cell_type":"markdown","source":"#### Filling `Embarked`","metadata":{}},{"cell_type":"code","source":"train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T14:25:17.831201Z","iopub.execute_input":"2025-04-28T14:25:17.831582Z","iopub.status.idle":"2025-04-28T14:25:17.839763Z","shell.execute_reply.started":"2025-04-28T14:25:17.831557Z","shell.execute_reply":"2025-04-28T14:25:17.838626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*   **What's `train_df['Embarked']`?** This selects the \"Embarked\" column from the `train_df` DataFrame.\n    *   **What is \"Embarked\"?** This column indicates the port where the passenger boarded the Titanic (e.g., Cherbourg, Queenstown, Southampton).\n*   **What's `.fillna()`?** This is a pandas method used to fill missing values (NaNs) in a Series (like a single column of a DataFrame).\n*   **What's `train_df['Embarked'].mode()`?**\n    *   `mode()` is a pandas method that calculates the **mode** of a series.\n    *   **What's the mode?** It's the most frequent value in a dataset. In this case, it's the most common port of embarkation.\n    *   So, `train_df['Embarked'].mode()` returns a Series containing the most frequent embarkation port(s). There could be more than one mode if multiple ports have the same highest frequency.\n*   **What's `[0]`?** We're using indexing to select the *first* mode from the Series returned by `mode()`. This assumes that we only need one value to fill the missing data.\n*   **What's `inplace=True`?** This modifies the `train_df` DataFrame directly, so the changes are permanent. Without `inplace=True`, the `fillna` operation would return a *new* DataFrame with the filled values, but wouldn't change `train_df` itself.\n*   **Why use the mode to fill `Embarked`?** Since \"Embarked\" is a categorical variable (a limited set of values), filling missing values with the most frequent category is a common and reasonable strategy. We're essentially saying,\n>\"If we don't know where they embarked, let's guess the most likely port.\"\n*   **Why are we only filling missing values in the `train_df` set?** There are no missing values in the `test_df` set for the \"Embarked\" column. Don't Believe me?","metadata":{}},{"cell_type":"code","source":"# Check for missing values in the 'Embarked' column\nmissing_embarked_values = test_df['Embarked'].isnull().sum()\n\n# Print the number of missing values\nprint(f\"Number of missing values in the 'Embarked' column of test.csv: {missing_embarked_values}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T14:33:41.520513Z","iopub.execute_input":"2025-04-28T14:33:41.52142Z","iopub.status.idle":"2025-04-28T14:33:41.528138Z","shell.execute_reply.started":"2025-04-28T14:33:41.521387Z","shell.execute_reply":"2025-04-28T14:33:41.526685Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"See? \n\nI was right.","metadata":{}},{"cell_type":"markdown","source":">\"For any missing values in the 'Embarked' column of the training data, fill them with the most frequent port of embarkation. And make these changes directly in the `train` DataFrame.\"","metadata":{}},{"cell_type":"markdown","source":"#### Dropping `Cabin`","metadata":{}},{"cell_type":"code","source":"train_df.drop('Cabin', axis=1, inplace=True)\ntest_df.drop('Cabin', axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T14:27:14.453489Z","iopub.execute_input":"2025-04-28T14:27:14.453871Z","iopub.status.idle":"2025-04-28T14:27:14.46238Z","shell.execute_reply.started":"2025-04-28T14:27:14.453848Z","shell.execute_reply":"2025-04-28T14:27:14.461295Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*   **What's `train_df.drop()` and `test_df.drop()`?** This is a pandas method used to remove rows or columns from a DataFrame.\n*   **What's `'Cabin'`?** This is the name of the column we want to drop.\n    *   **What is \"Cabin\"?** This column represents the cabin number of the passenger.\n*   **What's `axis=1`?** This specifies that we want to drop a *column* (axis 1) rather than a row (axis 0).\n*   **Why `inplace=True` again?** To modify the DataFrames directly.\n*   **Why are we dropping `Cabin`?** The \"Cabin\" is \"mostly missing.\" This means that a large percentage of the values in this column are NaN. Filling in a lot of missing data can introduce bias or noise. Dropping the column is a simpler approach when a feature has too many missing values.\n*   **We're dropping it from *both* `train` and `test`?** Yes, it's crucial to apply the same data preprocessing steps to both the training and testing data. If we remove a feature from the training data, we need to remove it from the testing data as well, otherwise, our model won't be able to make predictions on the test set.\n>\"Since the 'Cabin' column has a lot of missing values, let's just remove it entirely from both the training and testing DataFrames.\"","metadata":{}},{"cell_type":"markdown","source":"### 2.2.2 Encoding Categorical Variables\n\n*   **What's the Problem?** Machine learning models work best with numerical data. Columns like \"Sex\" (male/female) and \"Embarked\" (C/Q/S) are categorical, meaning they represent distinct categories rather than continuous numerical values. We need to convert these categories into numbers.\n*   **Our Strategy:** We'll use two common techniques:\n    *   **Mapping:** For binary categories (like \"Sex\"), we can directly map each category to a number.\n    *   **One-Hot Encoding:** For categories with multiple values (like \"Embarked\"), we'll create new binary columns representing each category.","metadata":{}},{"cell_type":"markdown","source":"#### Encoding `Sex`","metadata":{}},{"cell_type":"code","source":"train_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\ntest_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T14:37:33.764782Z","iopub.execute_input":"2025-04-28T14:37:33.765143Z","iopub.status.idle":"2025-04-28T14:37:33.773829Z","shell.execute_reply.started":"2025-04-28T14:37:33.765122Z","shell.execute_reply":"2025-04-28T14:37:33.772731Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*   **What's `train_df['Sex'].map(...)`?**\n    *   `.map()` is a pandas method that allows you to substitute values in a Series based on a mapping (a dictionary in this case).\n    *   `{'male': 0, 'female': 1}` is a Python dictionary that defines the mapping: \"male\" will be replaced with 0, and \"female\" will be replaced with 1.\n*   **Why use mapping for \"Sex\"?** Because it's a simple binary (two-category) variable. We can represent it with a single numerical column where 0 and 1 represent the two categories.\n*   **Why are we doing this for *both* `train` and `test`?** Consistency is key. We need to encode the \"Sex\" column in the same way in both datasets.\n>\"In both the training and testing DataFrames, replace 'male' in the 'Sex' column with 0 and 'female' with 1.\"\n\nNote that we could also one hot encode 'Sex', that is for male we would have [1, 0] and for females [0, 1], why did we not do that? Think...","metadata":{}},{"cell_type":"markdown","source":"#### Encoding `Embarked`","metadata":{}},{"cell_type":"code","source":"train_df = pd.get_dummies(train_df, columns=['Embarked'], drop_first=True)\ntest_df = pd.get_dummies(test_df, columns=['Embarked'], drop_first=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T14:48:41.457929Z","iopub.execute_input":"2025-04-28T14:48:41.458336Z","iopub.status.idle":"2025-04-28T14:48:41.474487Z","shell.execute_reply.started":"2025-04-28T14:48:41.458309Z","shell.execute_reply":"2025-04-28T14:48:41.473486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*   **What's `pd.get_dummies()`?** This is a pandas function that performs **one-hot encoding**.\n    *   **What's one-hot encoding?** It's a technique for representing categorical variables as binary (0 or 1) columns. For each category, it creates a new column. If the original value was that category, the corresponding column gets a 1; otherwise, it gets a 0.\n        *   **Example:** If \"Embarked\" could be 'C', 'Q', or 'S', one-hot encoding would create three new columns: \"Embarked\\_C\", \"Embarked\\_Q\", and \"Embarked\\_S\".\n*   **What's `columns=['Embarked']`?** This tells `get_dummies()` which columns to encode. We're only encoding the \"Embarked\" column here.\n*   **What's `drop_first=True`?** This is a crucial argument.\n    *   **Why `drop_first`?** It helps to avoid **multicollinearity**.\n        *   **What's multicollinearity?** It's when one feature can be predicted from another feature, which adds redundancy. For Embarked with 3 ports, we only need two columns, because if both columns are 0, it indicates the third port.\n        *   **How does `drop_first=True` prevent it?** If we have *n* categories, one-hot encoding creates *n* columns. But, we only need *n* - 1 columns to represent the information because the last category is implied if all other columns are 0. `drop_first=True` drops the first category.\n    *   **Example:** If \"Embarked\" has values 'C', 'Q', and 'S', `get_dummies` would normally create three columns: \"Embarked\\_C\", \"Embarked\\_Q\", and \"Embarked\\_S\". With `drop_first=True`, the \"Embarked\\_C\" column is dropped. If a passenger's \"Embarked\\_Q\" is 0 and \"Embarked\\_S\" is 0, we know they embarked at \"C\".\n*   **Why are we doing this for *both* `train_df` and `test_df`?** Again, consistency!\n\n>\"For both the training and testing DataFrames, use one-hot encoding to convert the 'Embarked' column into numerical columns. But drop the first encoded column to avoid redundancy.\"","metadata":{}},{"cell_type":"markdown","source":"Let's get back to our question.\n\nwhy we used a simple 0/1 mapping for 'Sex' instead of one-hot encoding.\n\n**Why not one-hot encode the 'Sex' column?**\n\nThe reason we typically *don't* one-hot encode a binary variable like 'Sex' and instead use a single column with 0s and 1s is to avoid the issue of **multicollinearity**.\n\nHere's why:\n\n1.  **Redundancy:** If you have a 'Sex_male' column (1 for male, 0 for female) and a 'Sex_female' column (1 for female, 0 for male), these two columns are perfectly correlated. If you know the value in one column, you automatically know the value in the other (e.g., if 'Sex_male' is 1, 'Sex_female' must be 0).\n2.  **Multicollinearity:** Multicollinearity occurs when independent variables in a regression model are highly correlated. While many machine learning models are robust to multicollinearity, it can cause problems in some models (like linear regression) by making it difficult to determine the individual effect of each correlated variable on the target variable. It can also lead to unstable model coefficients.\n\n**A single binary column (0/1) is sufficient:**\n\nBy mapping 'male' to 0 and 'female' to 1 (or vice-versa), you create a single numerical feature that completely represents the two categories. This single column provides all the necessary information to distinguish between the two sexes without introducing the redundancy and potential multicollinearity issues of two one-hot encoded columns.\n\nMost machine learning algorithms can directly use this single binary numerical feature effectively.\n\n>**In short:** For a categorical variable with only two unique values, a simple binary encoding (0 and 1) is preferred over one-hot encoding to avoid multicollinearity, while still capturing the categorical information.","metadata":{}},{"cell_type":"markdown","source":"Now, why did we not map the embarked column to values 0, 1, 2 for C, Q, S ? Again, Think...\n\nI mean its not like anyone could board a ship from multiple places, also the dataset only has C, Q, or S one of these values in the `Embarked` column.","metadata":{}},{"cell_type":"markdown","source":"### 2.2.3 Feature Engineering\n\n*   **What's Feature Engineering?** This is the art of creating new features (columns) from existing ones. It's like adding special ingredients to a recipe to enhance the flavor. Good features can significantly improve the performance of your machine learning model.\n*   **Our Strategy:** We'll create features related to family size and titles extracted from passenger names.","metadata":{}},{"cell_type":"markdown","source":"#### Family Size","metadata":{}},{"cell_type":"code","source":"train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\ntest_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T14:58:46.552048Z","iopub.execute_input":"2025-04-28T14:58:46.552381Z","iopub.status.idle":"2025-04-28T14:58:46.559662Z","shell.execute_reply.started":"2025-04-28T14:58:46.552359Z","shell.execute_reply":"2025-04-28T14:58:46.558607Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*   **What are `SibSp` and `Parch`?**\n    *   `SibSp`: Number of siblings/spouses aboard the Titanic.\n    *   `Parch`: Number of parents/children aboard the Titanic.\n*   **Why add `1`?** Because the passenger themselves is part of the family size!\n*   **Why create \"FamilySize\"?** It's a hypothesis, I think that family size might be related to survival. People traveling with family might have behaved differently (e.g., tried harder to stay together).\n>\"Create a new column called 'FamilySize' in both DataFrames by adding the number of siblings/spouses, the number of parents/children, and 1 (for the passenger themselves).\"","metadata":{}},{"cell_type":"markdown","source":"#### IsAlone","metadata":{}},{"cell_type":"code","source":"train_df['IsAlone'] = (train_df['FamilySize'] == 1).astype(int)\ntest_df['IsAlone'] = (test_df['FamilySize'] == 1).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:00:27.525917Z","iopub.execute_input":"2025-04-28T15:00:27.526308Z","iopub.status.idle":"2025-04-28T15:00:27.535422Z","shell.execute_reply.started":"2025-04-28T15:00:27.526285Z","shell.execute_reply":"2025-04-28T15:00:27.5344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*   **What's `(train_df['FamilySize'] == 1)`?** This creates a boolean Series (a series of True/False values) where `True` indicates that the family size is 1 (the passenger was alone) and `False` indicates they were with family.\n*   **What's `.astype(int)`?** This converts the boolean values to integers: `True` becomes 1, and `False` becomes 0.\n*   **Why create \"IsAlone\"?** It's a simplification of \"FamilySize.\" Being alone versus being with family might be an important factor.\n>\"Create a new column called 'IsAlone' in both DataFrames. Set it to 1 if the family size is 1 (passenger was alone) and 0 otherwise.\"","metadata":{}},{"cell_type":"markdown","source":"#### Title from Name","metadata":{}},{"cell_type":"code","source":"train_df['Title'] = train_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_df['Title'] = test_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# Group rare titles\ntrain_df['Title'] = train_df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest_df['Title'] = test_df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n# Convert to numbers\ntrain_df['Title'] = train_df['Title'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4})\ntest_df['Title'] = test_df['Title'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:12:10.755208Z","iopub.execute_input":"2025-04-28T15:12:10.755555Z","iopub.status.idle":"2025-04-28T15:12:10.776195Z","shell.execute_reply.started":"2025-04-28T15:12:10.755534Z","shell.execute_reply":"2025-04-28T15:12:10.775086Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This section is a bit more complex, let's See it like this:\n\n*   **What's `train_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)`?**\n    *   `train_df['Name']`: Selects the \"Name\" column (which contains passenger names).\n    *   `.str`: Accesses string methods for the Series.\n    *   `.extract(...)`: Uses a regular expression to extract a pattern from the strings.\n        *   **What's a regular expression (regex)?** It's a sequence of characters that defines a search pattern.\n        *   **What's the pattern here? ` ([A-Za-z]+)\\.`**\n            *   ` `: Matches a space character.\n            *   `([A-Za-z]+)`: This is the capturing group (the part we're interested in extracting).\n                *   `[A-Za-z]`: Matches any letter (uppercase or lowercase).\n                *   `+`: Matches one or more occurrences of the preceding character (so, one or more letters).\n            *   `\\.`: Matches a period character (the `\\` is an escape character because `.` has a special meaning in regex).\n        *   **What does the whole pattern mean?** It's looking for a space, followed by a word consisting of letters, followed by a period. This is a common pattern for titles in names (e.g., \" Mr.\", \" Miss.\").\n    *   `expand=False`: This argument tells pandas to return a Series instead of a DataFrame when there's only one capturing group.\n*   **Why extract the title?** Titles like \"Mr.\", \"Mrs.\", \"Miss.\", \"Master.\" might provide information about age, marital status, social status, etc., which could be related to survival.\n*   **What's the \"Group rare titles\" section doing?**\n    ```python\n    train_df['Title'] = train['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    test_df['Title'] = test_df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    ```\n    *   `.replace(...)`: This is a pandas method used to replace values in a Series.\n    *   **Why group rare titles?** Some titles appear very infrequently in the dataset. If we treat each title as a separate category, our model might not have enough data to learn patterns for the rare ones. Grouping them into a single \"Rare\" category helps to avoid overfitting (where the model learns the training data too well and doesn't generalize well to new data).\n*   **What's the \"Convert to numbers\" section doing?**\n    ```python\n    train_df['Title'] = train_df['Title'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4})\n    test_df['Title'] = test_df['Title'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4})\n    ```\n    *   This is the same mapping technique we used for \"Sex.\" We're converting the titles into numerical values.\n    *   **Why map them to these specific numbers (0, 1, 2, 3, 4)?** The specific numbers don't matter too much, as long as they're distinct. What's important is that we're representing the categories numerically.\n\n>1.  \"Extract the title (like 'Mr.', 'Miss.') from the passenger's name using a regular expression.\"\n>2.  \"Group rare titles into a single 'Rare' category to avoid overfitting.\"\n>3.  \"Convert the titles into numbers so the model can use them.\"","metadata":{}},{"cell_type":"markdown","source":"### 2.2.4 Dropping Unneeded Columns","metadata":{}},{"cell_type":"code","source":"train_df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\ntest_ids = test_df['PassengerId']  # Save for submission\ntest_df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:16:50.240751Z","iopub.execute_input":"2025-04-28T15:16:50.241186Z","iopub.status.idle":"2025-04-28T15:16:50.251968Z","shell.execute_reply.started":"2025-04-28T15:16:50.241154Z","shell.execute_reply":"2025-04-28T15:16:50.250587Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*   **What's the `drop()` function doing (again)?** Removing columns.\n*   **Why drop 'PassengerId'?** It's just a unique identifier for each passenger and doesn't carry any information that would help predict survival.\n*   **Why drop 'Name'?** We've already extracted the title from the name, so we don't need the full name anymore. And I can't think of any direct relation of name with survival.\n*   **Why drop 'Ticket'?** Ticket numbers are often complex and don't have a clear pattern. They're difficult to use as features without a lot of additional processing (which might not be worth the effort).\n*   **What's `test_ids = test_df['PassengerId']`?**\n    *   We're saving the \"PassengerId\" column from the test_df set *before* dropping it.\n    *   **Why save it?** The competition requires us to submit predictions with the \"PassengerId\" so that they can match our predictions to the correct passengers.\n>\"Remove the 'PassengerId', 'Name', and 'Ticket' columns from both DataFrames because they're not useful for our model. But, first, save the 'PassengerId' from the test_df set so we can use it for our submission later.\"","metadata":{}},{"cell_type":"markdown","source":"### 2.2.5 Scaling Numeric Features","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain_df[['Age', 'Fare']] = scaler.fit_transform(train_df[['Age', 'Fare']])\ntest_df[['Age', 'Fare']] = scaler.transform(test_df[['Age', 'Fare']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T15:19:45.14272Z","iopub.execute_input":"2025-04-28T15:19:45.143686Z","iopub.status.idle":"2025-04-28T15:19:45.158066Z","shell.execute_reply.started":"2025-04-28T15:19:45.143654Z","shell.execute_reply":"2025-04-28T15:19:45.156978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*   **Why scale numeric features?**\n    *   Many machine learning algorithms (especially those based on distances, like support vector machines or k-nearest neighbors) are sensitive to the scale of features. If one feature has a much larger range of values than another, it can dominate the calculations.\n    *   Scaling brings the features onto a similar scale, preventing this dominance and helping the algorithm to learn more effectively.\n*   **What's standardization?** It's a specific type of scaling that centers the data around 0 and scales it to have a standard deviation of 1.","metadata":{}},{"cell_type":"markdown","source":"*   **What's `from sklearn.preprocessing import StandardScaler`?**\n    *   We're importing the `StandardScaler` class from the `sklearn.preprocessing` module.\n    *   **What's `sklearn`?** It's scikit-learn, a very popular Python library for machine learning.\n    *   **What's `StandardScaler`?** It's a class that performs standardization, a type of feature scaling.\n*   **What's `scaler = StandardScaler()`?** We're creating an instance of the `StandardScaler` class, calling it `scaler`.\n*   **What's `train_df[['Age', 'Fare']] = scaler.fit_transform(train_df[['Age', 'Fare']])`?** This is the core of the scaling process.\n    *   `train_df[['Age', 'Fare']]`: Selects the \"Age\" and \"Fare\" columns from the `train_df` DataFrame.\n    *   `scaler.fit_transform(...)`:\n        *   `fit()`: This method calculates the mean and standard deviation of the features (in this case, \"Age\" and \"Fare\") in the training data. It \"learns\" the scaling parameters from the training set.\n        *   `transform()`: This method applies the scaling to the data. It subtracts the mean and divides by the standard deviation, so the scaled features have a mean of 0 and a standard deviation of 1.\n        *   `fit_transform()`: This is a convenient combination of `fit()` and `transform()`. We use it on the *training* data because we need to both learn the scaling parameters and apply them.\n*   **What's `test_df[['Age', 'Fare']] = scaler.transform(test_df[['Age', 'Fare']])`?**\n    *   We're scaling the \"Age\" and \"Fare\" columns in the *test_df* set.\n    *   **Why only `transform()` here, and not `fit_transform()`?** This is very important! We're using the scaling parameters (mean and standard deviation) that we *learned* from the *training* data. We don't want to calculate new scaling parameters from the test data because that would introduce information leakage. We want to treat the test data as unseen data.\n> \"For the 'Age' and 'Fare' columns:\n    > 1.  Learn how to scale the features from the *training* data (calculate the mean and standard deviation).\n    > 2.  Apply that same scaling to both the training and testing data so that the features have a mean of 0 and a standard deviation of 1.\"","metadata":{}},{"cell_type":"markdown","source":"**Summary of Section 2.2: Preprocessing the Data**\n\nWe've taken the raw Titanic dataset and transformed it into a format that's much better suited for machine learning:\n\n*   We handled missing values by filling in some (`Embarked`) and dropping one column with too many (`Cabin`).\n*   We converted categorical features into numerical representations using mapping (`Sex`) and one-hot encoding (`Embarked`).\n*   We engineered new features (`FamilySize`, `IsAlone`, `Title`) that might be more informative than the original ones.\n*   We dropped columns that were unlikely to be helpful for prediction. (`PassengerId`, `Name`, `Ticket`)\n*   We scaled the numerical features (`Age`, `Fare`) to a common range.\n\nThis section is like carefully preparing all your ingredients before starting to cook. \n\nNow, the data is ready to be fed into a machine learning model!","metadata":{}},{"cell_type":"markdown","source":"# 3. Modeling","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Comparison ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"# Changelog\n\n28 April 2025 IST (UTC+5.30)\n\n* Initial Public Release\n* Initial Completion of Section 1. Introduction\n* Initial Completion of Section 2. Data\n","metadata":{}},{"cell_type":"markdown","source":"# *References:*\n\n* [DataCamp Tutorial: Machine Learning from the Titanic](https://www.datacamp.com/tutorial/tutorial-kaggle-competition-tutorial-machine-learning-from-titanic)\n* [Introduction to Kaggle and Scoring Top 7 in the Titanic Competition](https://towardsdatascience.com/introduction-to-kaggle-and-scoring-top-7-in-the-titanic-competition-7a29ce9c24ae/)\n* [agconti/kaggle-titanic GitHub Repository](https://github.com/agconti/kaggle-titanic/blob/master/ReadMe.md)\n* [A Comprehensive Guide to Titanic Machine Learning - Kaggle](https://www.kaggle.com/code/eraaz1/a-comprehensive-guide-to-titanic-machine-learning)\n* [What I Learned Analyzing the Famous Titanic Dataset - LinkedIn Pulse](https://www.linkedin.com/pulse/what-i-learned-analyzing-famous-titanic-dateset-murilo-gustineli)\n* [Titanic - Machine Learning from Disaster | Kaggle Competition](https://www.kaggle.com/competitions/titanic)\n* [Titanic Tutorial - Kaggle](https://www.kaggle.com/code/alexisbcook/titanic-tutorial)\n* [Titanic: A step by step intro to Machine Learning - Kaggle](https://www.kaggle.com/code/ydalat/titanic-a-step-by-step-intro-to-machine-learning)\n* [A Beginner's Analysis of the Titanic Dataset - Kaggle](https://www.kaggle.com/code/idawoodjee/a-beginner-s-analysis-of-the-titanic-dataset)\n* [CS470 Homework 6 - Yale University](https://zoo.cs.yale.edu/classes/cs470/materials/hws/hw6/hw6.html)\n* [Titanic: A Complete Beginner's Guide - Kaggle](https://www.kaggle.com/code/reighns/titanic-a-complete-beginner-s-guide)\n* [Kaggle Titanic Data Exploration - Observable](https://observablehq.com/@jjimenez/kaggle-titanic-data-exploration)\n* [Data Science with Spark - SlideShare](https://www.slideshare.net/ksankar/data-science-with-spark)\n* [YouTube video 1](https://www.youtube.com/watch?v=I3FBJdiExcg)\n* [FDS Lecture Notes - University of Edinburgh](https://opencourse.inf.ed.ac.uk/sites/default/files/2024-01/FDS-lecture-notes-2024-01-28.pdf)\n* [YouTube video 2](https://www.youtube.com/watch?v=2fExV5KHU9s)","metadata":{"execution":{"iopub.status.busy":"2025-04-28T13:08:52.570312Z","iopub.execute_input":"2025-04-28T13:08:52.570694Z","iopub.status.idle":"2025-04-28T13:08:52.581593Z","shell.execute_reply.started":"2025-04-28T13:08:52.570667Z","shell.execute_reply":"2025-04-28T13:08:52.580223Z"}}}]}
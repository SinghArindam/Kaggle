{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-06T14:08:48.16093Z","iopub.execute_input":"2025-02-06T14:08:48.161337Z","iopub.status.idle":"2025-02-06T14:08:48.170699Z","shell.execute_reply.started":"2025-02-06T14:08:48.161305Z","shell.execute_reply":"2025-02-06T14:08:48.169538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier, Pool\n\nimport optuna\n\n# ---------------------------\n# 1. Feature Engineering\n# ---------------------------\ndef feature_engineering(df, is_train=True):\n    # Extract Title from Name\n    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    df['Title'] = df['Title'].replace(\n        ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'],\n        'Rare')\n    df['Title'] = df['Title'].replace({'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs'})\n    \n    # Impute missing Age based on Title\n    df['Age'] = df.groupby('Title')['Age'].transform(lambda x: x.fillna(x.median()))\n    \n    # Fill missing Fare (only in test)\n    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n    \n    # Fill missing Embarked with mode\n    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n    \n    # Create FamilySize and IsAlone features\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    df['IsAlone'] = 1\n    df.loc[df['FamilySize'] > 1, 'IsAlone'] = 0\n    \n    # Interaction feature: Age * Class\n    df['Age_Class'] = df['Age'] * df['Pclass']\n    \n    # Extract Cabin letter; fill missing cabins with 'U'\n    df['Cabin'].fillna('U', inplace=True)\n    df['Cabin'] = df['Cabin'].apply(lambda x: x[0])\n    \n    # Optionally drop columns that are less informative\n    if is_train:\n        df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n    else:\n        # For test set, keep PassengerId for submission\n        df.drop(['Name', 'Ticket'], axis=1, inplace=True)\n    \n    return df\n\ndef encode_features(train, test):\n    # Combine datasets to encode categorical features consistently\n    combined = pd.concat([train, test], sort=False)\n    for col in combined.select_dtypes(include=['object']).columns:\n        le = LabelEncoder()\n        combined[col] = le.fit_transform(combined[col])\n    return combined.iloc[:train.shape[0]].copy(), combined.iloc[train.shape[0]:].copy()\n\n# ---------------------------\n# 2. Load Data & Prepare\n# ---------------------------\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\ntest_passenger_ids = test['PassengerId'].copy()\n\ntrain = feature_engineering(train, is_train=True)\ntest = feature_engineering(test, is_train=False)\n\n# Separate target variable and features\ny = train['Survived']\nX = train.drop('Survived', axis=1)\nX_test = test.drop('PassengerId', axis=1)  # keep PassengerId for submission\n\n# Encode categorical features\nX, X_test = encode_features(X, X_test)\n\n# Set up cross validation\nNFOLDS = 5\ncv = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n# ---------------------------\n# 3. Hyperparameter Optimization\n# ---------------------------\n# LightGBM Objective\ndef objective_lgb(trial, X, y, cv):\n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0)\n    }\n    cv_scores = []\n    for train_idx, val_idx in cv.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        train_set = lgb.Dataset(X_train, y_train)\n        val_set = lgb.Dataset(X_val, y_val, reference=train_set)\n        model = lgb.train(params, train_set, num_boost_round=1000,\n                          valid_sets=[val_set],\n                          early_stopping_rounds=50,\n                          verbose_eval=False)\n        preds = model.predict(X_val, num_iteration=model.best_iteration)\n        cv_scores.append(log_loss(y_val, preds))\n    return np.mean(cv_scores)\n\nstudy_lgb = optuna.create_study(direction='minimize')\nstudy_lgb.optimize(lambda trial: objective_lgb(trial, X, y, cv), n_trials=30)\nbest_params_lgb = study_lgb.best_trial.params\nprint(\"Best LGB params:\", best_params_lgb)\n\n# XGBoost Objective\ndef objective_xgb(trial, X, y, cv):\n    params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'booster': 'gbtree',\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0)\n    }\n    cv_scores = []\n    for train_idx, val_idx in cv.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dval = xgb.DMatrix(X_val, label=y_val)\n        evals = [(dval, 'eval')]\n        model = xgb.train(params, dtrain, num_boost_round=1000,\n                          evals=evals,\n                          early_stopping_rounds=50,\n                          verbose_eval=False)\n        preds = model.predict(dval, ntree_limit=model.best_ntree_limit)\n        cv_scores.append(log_loss(y_val, preds))\n    return np.mean(cv_scores)\n\nstudy_xgb = optuna.create_study(direction='minimize')\nstudy_xgb.optimize(lambda trial: objective_xgb(trial, X, y, cv), n_trials=30)\nbest_params_xgb = study_xgb.best_trial.params\nprint(\"Best XGB params:\", best_params_xgb)\n\n# CatBoost Objective\ndef objective_cat(trial, X, y, cv):\n    params = {\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n        'depth': trial.suggest_int('depth', 3, 10),\n        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-2, 10.0),\n        'loss_function': 'Logloss',\n        'verbose': False,\n        'iterations': 1000\n    }\n    cv_scores = []\n    for train_idx, val_idx in cv.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        train_pool = Pool(X_train, y_train)\n        val_pool = Pool(X_val, y_val)\n        model = CatBoostClassifier(**params)\n        model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50, verbose=False)\n        preds = model.predict_proba(X_val)[:, 1]\n        cv_scores.append(log_loss(y_val, preds))\n    return np.mean(cv_scores)\n\nstudy_cat = optuna.create_study(direction='minimize')\nstudy_cat.optimize(lambda trial: objective_cat(trial, X, y, cv), n_trials=30)\nbest_params_cat = study_cat.best_trial.params\nprint(\"Best CatBoost params:\", best_params_cat)\n\n# ---------------------------\n# 4. Out-Of-Fold Predictions for Stacking\n# ---------------------------\ndef get_oof_predictions(models, X, y, cv):\n    oof_preds = np.zeros((X.shape[0], len(models)))\n    for i, name in enumerate(models.keys()):\n        fold_preds = np.zeros(X.shape[0])\n        for train_idx, val_idx in cv.split(X, y):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n            \n            if name == 'lgb':\n                train_set = lgb.Dataset(X_train, y_train)\n                model = lgb.train(best_params_lgb, train_set, num_boost_round=1000,\n                                  valid_sets=[lgb.Dataset(X_val, y_val)],\n                                  early_stopping_rounds=50,\n                                  verbose_eval=False)\n                fold_preds[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n            \n            elif name == 'xgb':\n                dtrain = xgb.DMatrix(X_train, label=y_train)\n                dval = xgb.DMatrix(X_val, label=y_val)\n                model = xgb.train(best_params_xgb, dtrain, num_boost_round=1000,\n                                  evals=[(dval, 'eval')],\n                                  early_stopping_rounds=50,\n                                  verbose_eval=False)\n                fold_preds[val_idx] = model.predict(xgb.DMatrix(X_val), ntree_limit=model.best_ntree_limit)\n            \n            elif name == 'cat':\n                train_pool = Pool(X_train, y_train)\n                val_pool = Pool(X_val, y_val)\n                model = CatBoostClassifier(**best_params_cat, iterations=1000, loss_function='Logloss', verbose=False)\n                model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50, verbose=False)\n                fold_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n        oof_preds[:, i] = fold_preds\n    return oof_preds\n\n# List of base models for stacking\nbase_model_names = ['lgb', 'xgb', 'cat']\noof_train = get_oof_predictions(dict.fromkeys(base_model_names), X, y, cv)\nprint(\"OOF predictions shape:\", oof_train.shape)\n\n# Train meta-model on OOF predictions\nmeta_model = LogisticRegression()\nmeta_model.fit(oof_train, y)\n\n# ---------------------------\n# 5. Train Final Models on Full Data\n# ---------------------------\n# Final LightGBM model\nfinal_lgb = lgb.train(best_params_lgb, lgb.Dataset(X, y), num_boost_round=1000)\n\n# Final XGBoost model\nfinal_xgb = xgb.train(best_params_xgb, xgb.DMatrix(X, label=y), num_boost_round=1000)\n\n# Final CatBoost model\nfinal_cat = CatBoostClassifier(**best_params_cat, iterations=1000, loss_function='Logloss', verbose=False)\nfinal_cat.fit(X, y)\n\n# Prepare dictionary of trained models\nfinal_models = {'lgb': final_lgb, 'xgb': final_xgb, 'cat': final_cat}\n\n# ---------------------------\n# 6. Predict on Test Set & Blend\n# ---------------------------\ndef get_test_predictions(models, X_test):\n    preds = np.zeros((X_test.shape[0], len(models)))\n    for i, name in enumerate(models.keys()):\n        if name == 'lgb':\n            preds[:, i] = models[name].predict(X_test, num_iteration=models[name].best_iteration)\n        elif name == 'xgb':\n            dtest = xgb.DMatrix(X_test)\n            preds[:, i] = models[name].predict(dtest, ntree_limit=models[name].best_ntree_limit)\n        elif name == 'cat':\n            preds[:, i] = models[name].predict_proba(X_test)[:, 1]\n    return preds\n\ntest_preds = get_test_predictions(final_models, X_test)\n# Stack the base predictions using the meta-model\nstacked_test = meta_model.predict_proba(test_preds)[:, 1]\n\n# You can use 0.5 as a threshold or optimize your own threshold\nfinal_test_pred = (stacked_test > 0.5).astype(int)\n\n# ---------------------------\n# 7. Create Submission\n# ---------------------------\nsubmission = pd.DataFrame({\n    \"PassengerId\": test_passenger_ids,\n    \"Survived\": final_test_pred\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T14:08:48.36854Z","iopub.execute_input":"2025-02-06T14:08:48.368863Z","iopub.status.idle":"2025-02-06T14:08:48.514556Z","shell.execute_reply.started":"2025-02-06T14:08:48.368839Z","shell.execute_reply":"2025-02-06T14:08:48.512986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier, Pool\n\nimport optuna\n\n# ---------------------------\n# 1. Feature Engineering\n# ---------------------------\ndef feature_engineering(df, is_train=True):\n    # Extract Title from Name\n    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    df['Title'] = df['Title'].replace(\n        ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'],\n        'Rare')\n    df['Title'] = df['Title'].replace({'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs'})\n    \n    # Impute missing Age based on Title\n    df['Age'] = df.groupby('Title')['Age'].transform(lambda x: x.fillna(x.median()))\n    \n    # Fill missing Fare (only in test)\n    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n    \n    # Fill missing Embarked with mode\n    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n    \n    # Create FamilySize and IsAlone features\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    df['IsAlone'] = 1\n    df.loc[df['FamilySize'] > 1, 'IsAlone'] = 0\n    \n    # Interaction feature: Age * Class\n    df['Age_Class'] = df['Age'] * df['Pclass']\n    \n    # Extract Cabin letter; fill missing cabins with 'U'\n    df['Cabin'].fillna('U', inplace=True)\n    df['Cabin'] = df['Cabin'].apply(lambda x: x[0])\n    \n    # Optionally drop columns that are less informative\n    if is_train:\n        df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n    else:\n        # For test set, keep PassengerId for submission\n        df.drop(['Name', 'Ticket'], axis=1, inplace=True)\n    \n    return df\n\ndef encode_features(train, test):\n    # Combine datasets to encode categorical features consistently\n    combined = pd.concat([train, test], sort=False)\n    for col in combined.select_dtypes(include=['object']).columns:\n        le = LabelEncoder()\n        combined[col] = le.fit_transform(combined[col])\n    return combined.iloc[:train.shape[0]].copy(), combined.iloc[train.shape[0]:].copy()\n\n# ---------------------------\n# 2. Load Data & Prepare\n# ---------------------------\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\ntest_passenger_ids = test['PassengerId'].copy()\n\ntrain = feature_engineering(train, is_train=True)\ntest = feature_engineering(test, is_train=False)\n\n# Separate target variable and features\ny = train['Survived']\nX = train.drop('Survived', axis=1)\nX_test = test.drop('PassengerId', axis=1)  # keep PassengerId for submission\n\n# Encode categorical features\nX, X_test = encode_features(X, X_test)\n\n# Set up cross validation\nNFOLDS = 5\ncv = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n# ---------------------------\n# 3. Hyperparameter Optimization\n# ---------------------------\n# LightGBM Objective\ndef objective_lgb(trial, X, y, cv):\n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0)\n    }\n    cv_scores = []\n    for train_idx, val_idx in cv.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        train_set = lgb.Dataset(X_train, y_train)\n        val_set = lgb.Dataset(X_val, y_val, reference=train_set)\n        model = lgb.train(params, train_set, num_boost_round=1000,\n                          valid_sets=[val_set],\n                          callbacks=[lgb.early_stopping(stopping_rounds=50)],\n                          verbose_eval=False)\n        preds = model.predict(X_val, num_iteration=model.best_iteration)\n        cv_scores.append(log_loss(y_val, preds))\n    return np.mean(cv_scores)\n\nstudy_lgb = optuna.create_study(direction='minimize')\nstudy_lgb.optimize(lambda trial: objective_lgb(trial, X, y, cv), n_trials=30)\nbest_params_lgb = study_lgb.best_trial.params\nprint(\"Best LGB params:\", best_params_lgb)\n\n# XGBoost Objective\ndef objective_xgb(trial, X, y, cv):\n    params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'booster': 'gbtree',\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0)\n    }\n    cv_scores = []\n    for train_idx, val_idx in cv.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dval = xgb.DMatrix(X_val, label=y_val)\n        evals = [(dval, 'eval')]\n        model = xgb.train(params, dtrain, num_boost_round=1000,\n                          evals=evals,\n                          early_stopping_rounds=50,\n                          verbose_eval=False)\n        preds = model.predict(dval, ntree_limit=model.best_ntree_limit)\n        cv_scores.append(log_loss(y_val, preds))\n    return np.mean(cv_scores)\n\nstudy_xgb = optuna.create_study(direction='minimize')\nstudy_xgb.optimize(lambda trial: objective_xgb(trial, X, y, cv), n_trials=30)\nbest_params_xgb = study_xgb.best_trial.params\nprint(\"Best XGB params:\", best_params_xgb)\n\n# CatBoost Objective\ndef objective_cat(trial, X, y, cv):\n    params = {\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n        'depth': trial.suggest_int('depth', 3, 10),\n        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-2, 10.0),\n        'loss_function': 'Logloss',\n        'verbose': False,\n        'iterations': 1000\n    }\n    cv_scores = []\n    for train_idx, val_idx in cv.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        train_pool = Pool(X_train, y_train)\n        val_pool = Pool(X_val, y_val)\n        model = CatBoostClassifier(**params)\n        model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50, verbose=False)\n        preds = model.predict_proba(X_val)[:, 1]\n        cv_scores.append(log_loss(y_val, preds))\n    return np.mean(cv_scores)\n\nstudy_cat = optuna.create_study(direction='minimize')\nstudy_cat.optimize(lambda trial: objective_cat(trial, X, y, cv), n_trials=30)\nbest_params_cat = study_cat.best_trial.params\nprint(\"Best CatBoost params:\", best_params_cat)\n\n# ---------------------------\n# 4. Out-Of-Fold Predictions for Stacking\n# ---------------------------\ndef get_oof_predictions(models, X, y, cv):\n    oof_preds = np.zeros((X.shape[0], len(models)))\n    for i, name in enumerate(models.keys()):\n        fold_preds = np.zeros(X.shape[0])\n        for train_idx, val_idx in cv.split(X, y):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n            \n            if name == 'lgb':\n                train_set = lgb.Dataset(X_train, y_train)\n                model = lgb.train(best_params_lgb, train_set, num_boost_round=1000,\n                                  valid_sets=[lgb.Dataset(X_val, y_val)],\n                                  callbacks=[lgb.early_stopping(stopping_rounds=50)],\n                                  verbose_eval=False)\n                fold_preds[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n            \n            elif name == 'xgb':\n                dtrain = xgb.DMatrix(X_train, label=y_train)\n                dval = xgb.DMatrix(X_val, label=y_val)\n                model = xgb.train(best_params_xgb, dtrain, num_boost_round=1000,\n                                  evals=[(dval, 'eval')],\n                                  early_stopping_rounds=50,\n                                  verbose_eval=False)\n                fold_preds[val_idx] = model.predict(xgb.DMatrix(X_val), ntree_limit=model.best_ntree_limit)\n            \n            elif name == 'cat':\n                train_pool = Pool(X_train, y_train)\n                val_pool = Pool(X_val, y_val)\n                model = CatBoostClassifier(**best_params_cat, iterations=1000, loss_function='Logloss', verbose=False)\n                model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50, verbose=False)\n                fold_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n        oof_preds[:, i] = fold_preds\n    return oof_preds\n\n# List of base models for stacking\nbase_model_names = ['lgb', 'xgb', 'cat']\noof_train = get_oof_predictions(dict.fromkeys(base_model_names), X, y, cv)\nprint(\"OOF predictions shape:\", oof_train.shape)\n\n# Train meta-model on OOF predictions\nmeta_model = LogisticRegression()\nmeta_model.fit(oof_train, y)\n\n# ---------------------------\n# 5. Train Final Models on Full Data\n# ---------------------------\n# Final LightGBM model (no early stopping used here)\nfinal_lgb = lgb.train(best_params_lgb, lgb.Dataset(X, y), num_boost_round=1000)\n\n# Final XGBoost model\nfinal_xgb = xgb.train(best_params_xgb, xgb.DMatrix(X, label=y), num_boost_round=1000)\n\n# Final CatBoost model\nfinal_cat = CatBoostClassifier(**best_params_cat, iterations=1000, loss_function='Logloss', verbose=False)\nfinal_cat.fit(X, y)\n\n# Prepare dictionary of trained models\nfinal_models = {'lgb': final_lgb, 'xgb': final_xgb, 'cat': final_cat}\n\n# ---------------------------\n# 6. Predict on Test Set & Blend\n# ---------------------------\ndef get_test_predictions(models, X_test):\n    preds = np.zeros((X_test.shape[0], len(models)))\n    for i, name in enumerate(models.keys()):\n        if name == 'lgb':\n            preds[:, i] = models[name].predict(X_test, num_iteration=models[name].best_iteration)\n        elif name == 'xgb':\n            dtest = xgb.DMatrix(X_test)\n            preds[:, i] = models[name].predict(dtest, ntree_limit=models[name].best_ntree_limit)\n        elif name == 'cat':\n            preds[:, i] = models[name].predict_proba(X_test)[:, 1]\n    return preds\n\ntest_preds = get_test_predictions(final_models, X_test)\n# Stack the base predictions using the meta-model\nstacked_test = meta_model.predict_proba(test_preds)[:, 1]\n\n# You can use 0.5 as a threshold or optimize your own threshold\nfinal_test_pred = (stacked_test > 0.5).astype(int)\n\n# ---------------------------\n# 7. Create Submission\n# ---------------------------\nsubmission = pd.DataFrame({\n    \"PassengerId\": test_passenger_ids,\n    \"Survived\": final_test_pred\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T14:10:36.326917Z","iopub.execute_input":"2025-02-06T14:10:36.327269Z","iopub.status.idle":"2025-02-06T14:10:36.432862Z","shell.execute_reply.started":"2025-02-06T14:10:36.327244Z","shell.execute_reply":"2025-02-06T14:10:36.431609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier, Pool\n\nimport optuna\n\n# ---------------------------\n# 1. Feature Engineering\n# ---------------------------\ndef feature_engineering(df, is_train=True):\n    # Extract Title from Name\n    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    df['Title'] = df['Title'].replace(\n        ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'],\n        'Rare')\n    df['Title'] = df['Title'].replace({'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs'})\n    \n    # Impute missing Age based on Title\n    df['Age'] = df.groupby('Title')['Age'].transform(lambda x: x.fillna(x.median()))\n    \n    # Fill missing Fare (only in test)\n    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n    \n    # Fill missing Embarked with mode\n    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n    \n    # Create FamilySize and IsAlone features\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    df['IsAlone'] = 1\n    df.loc[df['FamilySize'] > 1, 'IsAlone'] = 0\n    \n    # Interaction feature: Age * Class\n    df['Age_Class'] = df['Age'] * df['Pclass']\n    \n    # Extract Cabin letter; fill missing cabins with 'U'\n    df['Cabin'].fillna('U', inplace=True)\n    df['Cabin'] = df['Cabin'].apply(lambda x: x[0])\n    \n    # Optionally drop columns that are less informative\n    if is_train:\n        df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n    else:\n        # For test set, keep PassengerId for submission\n        df.drop(['Name', 'Ticket'], axis=1, inplace=True)\n    \n    return df\n\ndef encode_features(train, test):\n    # Combine datasets to encode categorical features consistently\n    combined = pd.concat([train, test], sort=False)\n    for col in combined.select_dtypes(include=['object']).columns:\n        le = LabelEncoder()\n        combined[col] = le.fit_transform(combined[col])\n    return combined.iloc[:train.shape[0]].copy(), combined.iloc[train.shape[0]:].copy()\n\n# ---------------------------\n# 2. Load Data & Prepare\n# ---------------------------\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\ntest_passenger_ids = test['PassengerId'].copy()\n\ntrain = feature_engineering(train, is_train=True)\ntest = feature_engineering(test, is_train=False)\n\n# Separate target variable and features\ny = train['Survived']\nX = train.drop('Survived', axis=1)\nX_test = test.drop('PassengerId', axis=1)  # keep PassengerId for submission\n\n# Encode categorical features\nX, X_test = encode_features(X, X_test)\n\n# Set up cross validation\nNFOLDS = 5\ncv = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n# ---------------------------\n# 3. Hyperparameter Optimization\n# ---------------------------\n# LightGBM Objective\ndef objective_lgb(trial, X, y, cv):\n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0)\n    }\n    cv_scores = []\n    for train_idx, val_idx in cv.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        train_set = lgb.Dataset(X_train, y_train)\n        val_set = lgb.Dataset(X_val, y_val, reference=train_set)\n        model = lgb.train(\n            params, train_set, num_boost_round=1000,\n            valid_sets=[val_set],\n            callbacks=[lgb.early_stopping(stopping_rounds=50)]\n        )\n        preds = model.predict(X_val, num_iteration=model.best_iteration)\n        cv_scores.append(log_loss(y_val, preds))\n    return np.mean(cv_scores)\n\nstudy_lgb = optuna.create_study(direction='minimize')\nstudy_lgb.optimize(lambda trial: objective_lgb(trial, X, y, cv), n_trials=30)\nbest_params_lgb = study_lgb.best_trial.params\nprint(\"Best LGB params:\", best_params_lgb)\n\n# XGBoost Objective\n# XGBoost Objective\ndef objective_xgb(trial, X, y, cv):\n    params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'booster': 'gbtree',\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0)\n    }\n    cv_scores = []\n    for train_idx, val_idx in cv.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dval = xgb.DMatrix(X_val, label=y_val)\n        evals = [(dval, 'eval')]\n        model = xgb.train(params, dtrain, num_boost_round=1000,\n                          evals=evals,\n                          early_stopping_rounds=50,\n                          verbose_eval=False)\n        # Use iteration_range instead of ntree_limit\n        preds = model.predict(dval, iteration_range=(0, model.best_iteration))\n        cv_scores.append(log_loss(y_val, preds))\n    return np.mean(cv_scores)\n\n\nstudy_xgb = optuna.create_study(direction='minimize')\nstudy_xgb.optimize(lambda trial: objective_xgb(trial, X, y, cv), n_trials=30)\nbest_params_xgb = study_xgb.best_trial.params\nprint(\"Best XGB params:\", best_params_xgb)\n\n# CatBoost Objective\ndef objective_cat(trial, X, y, cv):\n    params = {\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n        'depth': trial.suggest_int('depth', 3, 10),\n        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-2, 10.0),\n        'loss_function': 'Logloss',\n        'verbose': False,\n        'iterations': 1000\n    }\n    cv_scores = []\n    for train_idx, val_idx in cv.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        train_pool = Pool(X_train, y_train)\n        val_pool = Pool(X_val, y_val)\n        model = CatBoostClassifier(**params)\n        model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50, verbose=False)\n        preds = model.predict_proba(X_val)[:, 1]\n        cv_scores.append(log_loss(y_val, preds))\n    return np.mean(cv_scores)\n\nstudy_cat = optuna.create_study(direction='minimize')\nstudy_cat.optimize(lambda trial: objective_cat(trial, X, y, cv), n_trials=30)\nbest_params_cat = study_cat.best_trial.params\nprint(\"Best CatBoost params:\", best_params_cat)\n\n# ---------------------------\n# 4. Out-Of-Fold Predictions for Stacking\n# ---------------------------\n# In the Out-Of-Fold Predictions function:\n\n# In the Out-Of-Fold Predictions function:\ndef get_oof_predictions(models, X, y, cv):\n    oof_preds = np.zeros((X.shape[0], len(models)))\n    for i, name in enumerate(models.keys()):\n        fold_preds = np.zeros(X.shape[0])\n        for train_idx, val_idx in cv.split(X, y):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n            \n            if name == 'xgb':\n                dtrain = xgb.DMatrix(X_train, label=y_train)\n                dval = xgb.DMatrix(X_val, label=y_val)\n                model = xgb.train(best_params_xgb, dtrain, num_boost_round=1000,\n                                  evals=[(dval, 'eval')],\n                                  early_stopping_rounds=50,\n                                  verbose_eval=False)\n                fold_preds[val_idx] = model.predict(xgb.DMatrix(X_val), iteration_range=(0, model.best_iteration))\n            # Other model branches (lgb, cat) omitted for brevity\n        oof_preds[:, i] = fold_preds\n    return oof_preds\n\n\n# List of base models for stacking\nbase_model_names = ['lgb', 'xgb', 'cat']\noof_train = get_oof_predictions(dict.fromkeys(base_model_names), X, y, cv)\nprint(\"OOF predictions shape:\", oof_train.shape)\n\n# Train meta-model on OOF predictions\nmeta_model = LogisticRegression()\nmeta_model.fit(oof_train, y)\n\n# ---------------------------\n# 5. Train Final Models on Full Data\n# ---------------------------\n# Final LightGBM model (no early stopping used here)\nfinal_lgb = lgb.train(best_params_lgb, lgb.Dataset(X, y), num_boost_round=1000)\n\n# Final XGBoost model\nfinal_xgb = xgb.train(best_params_xgb, xgb.DMatrix(X, label=y), num_boost_round=1000)\n\n# Final CatBoost model\nfinal_cat = CatBoostClassifier(**best_params_cat, iterations=1000, loss_function='Logloss', verbose=False)\nfinal_cat.fit(X, y)\n\n# Prepare dictionary of trained models\nfinal_models = {'lgb': final_lgb, 'xgb': final_xgb, 'cat': final_cat}\n\n# ---------------------------\n# 6. Predict on Test Set & Blend\n# ---------------------------\ndef get_test_predictions(models, X_test):\n    preds = np.zeros((X_test.shape[0], len(models)))\n    for i, name in enumerate(models.keys()):\n        if name == 'lgb':\n            preds[:, i] = models[name].predict(X_test, num_iteration=models[name].best_iteration)\n        elif name == 'xgb':\n            dtest = xgb.DMatrix(X_test)\n            # Use iteration_range instead of ntree_limit\n            preds[:, i] = models[name].predict(dtest, iteration_range=(0, models[name].best_iteration))\n        elif name == 'cat':\n            preds[:, i] = models[name].predict_proba(X_test)[:, 1]\n    return preds\n\n\ntest_preds = get_test_predictions(final_models, X_test)\n# Stack the base predictions using the meta-model\nstacked_test = meta_model.predict_proba(test_preds)[:, 1]\n\n# You can use 0.5 as a threshold or optimize your own threshold\nfinal_test_pred = (stacked_test > 0.5).astype(int)\n\n# ---------------------------\n# 7. Create Submission\n# ---------------------------\nsubmission = pd.DataFrame({\n    \"PassengerId\": test_passenger_ids,\n    \"Survived\": final_test_pred\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T14:17:35.5671Z","iopub.execute_input":"2025-02-06T14:17:35.567512Z","iopub.status.idle":"2025-02-06T14:19:47.656473Z","shell.execute_reply.started":"2025-02-06T14:17:35.567483Z","shell.execute_reply":"2025-02-06T14:19:47.655102Z"},"scrolled":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom collections import Counter\nimport re\nfrom sklearn.model_selection import train_test_split\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Custom IMDB Dataset class\nclass IMDBDataset(Dataset):\n    def __init__(self, reviews, labels, word2idx, max_len=500):\n        self.reviews = reviews\n        self.labels = labels\n        self.word2idx = word2idx\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.reviews)\n\n    def __getitem__(self, idx):\n        review = self.reviews[idx]\n        label = self.labels[idx]\n        # Convert review to indices\n        indices = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in review.split()][:self.max_len]\n        # Pad or truncate\n        if len(indices) < self.max_len:\n            indices += [self.word2idx['<PAD>']] * (self.max_len - len(indices))\n        else:\n            indices = indices[:self.max_len]\n        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n\n# Text preprocessing function\ndef preprocess_text(text):\n    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = text.lower().strip()\n    return text\n\n# Build vocabulary\ndef build_vocab(reviews, min_freq=5):\n    word_counts = Counter()\n    for review in reviews:\n        words = review.split()\n        word_counts.update(words)\n    # Include special tokens\n    vocab = {'<PAD>': 0, '<UNK>': 1}\n    idx = len(vocab)\n    for word, count in word_counts.items():\n        if count >= min_freq:\n            vocab[word] = idx\n            idx += 1\n    return vocab\n\n# Load and preprocess IMDB dataset\ndef load_imdb_data(data_dir='/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'):\n    df = pd.read_csv(data_dir)\n    reviews = df['review'].apply(preprocess_text).tolist()\n    labels = df['sentiment'].map({'positive': 1, 'negative': 0}).tolist()\n    return reviews, labels\n\n# # GRU Model\n# class GRUModel(nn.Module):\n#     def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=1, dropout=0.3):\n#         super(GRUModel, self).__init__()\n#         self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n#         self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n#         self.fc = nn.Linear(hidden_size, output_size)\n#         self.sigmoid = nn.Sigmoid()\n\n#     def.Concurrent(self, x):\n#         embedded = self.embedding(x)  # [batch_size, seq_len, embed_size]\n#         gru_out, _ = self.gru(embedded)  # [batch_size, seq_len, hidden_size]\n#         last_out = gru_out[:, -1, :]  # Take the last time step\n#         out = self.fc(last_out)\n#         out = self.sigmoid(out)\n#         return out\n\n# GRU Model\nclass GRUModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=1, dropout=0.3):\n        super(GRUModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        embedded = self.embedding(x)  # [batch_size, seq_len, embed_size]\n        gru_out, _ = self.gru(embedded)  # [batch_size, seq_len, hidden_size]\n        last_out = gru_out[:, -1, :]  # Take the last time step\n        out = self.fc(last_out)\n        out = self.sigmoid(out)\n        return out\n\n# Training function\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=30):\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n\n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        train_loss = 0\n        train_correct = 0\n        train_total = 0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs).squeeze()\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            predicted = (outputs >= 0.5).float()\n            train_total += labels.size(0)\n            train_correct += (predicted == labels).sum().item()\n        \n        train_loss /= len(train_loader)\n        train_acc = train_correct / train_total\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs).squeeze()\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                predicted = (outputs >= 0.5).float()\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n        \n        val_loss /= len(val_loader)\n        val_acc = val_correct / val_total\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n    return train_losses, val_losses, train_accs, val_accs\n\n# Plotting function\ndef plot_metrics(train_losses, val_losses, train_accs, val_accs):\n    epochs = range(1, len(train_losses) + 1)\n    \n    # Plot loss\n    plt.figure(figsize=(10, 5))\n    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.savefig('loss_plot.png')\n    plt.show()\n    plt.close()\n\n    # Plot accuracy\n    plt.figure(figsize=(10, 5))\n    plt.plot(epochs, train_accs, 'b-', label='Training Accuracy')\n    plt.plot(epochs, val_accs, 'r-', label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    plt.savefig('accuracy_plot.png')\n    plt.show()\n    plt.close()\n\n# Main execution\ndef main():\n    # Load data\n    reviews, labels = load_imdb_data()\n    \n    # Build vocabulary\n    word2idx = build_vocab(reviews)\n    print(f\"Vocabulary size: {len(word2idx)}\")\n    \n    # Split data\n    train_reviews, test_reviews, train_labels, test_labels = train_test_split(\n        reviews, labels, test_size=0.5, random_state=42\n    )\n    train_reviews, val_reviews, train_labels, val_labels = train_test_split(\n        train_reviews, train_labels, test_size=0.2, random_state=42\n    )\n    \n    # Create datasets\n    train_dataset = IMDBDataset(train_reviews, train_labels, word2idx)\n    val_dataset = IMDBDataset(val_reviews, val_labels, word2idx)\n    test_dataset = IMDBDataset(test_reviews, test_labels, word2idx)\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=64)\n    test_loader = DataLoader(test_dataset, batch_size=64)\n    \n    # Model parameters\n    vocab_size = len(word2idx)\n    embed_size = 128\n    hidden_size = 256\n    output_size = 1\n    num_layers = 2\n    \n    # Initialize model, criterion, optimizer\n    model = GRUModel(vocab_size, embed_size, hidden_size, output_size, num_layers).to(device)\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Train model\n    train_losses, val_losses, train_accs, val_accs = train_model(\n        model, train_loader, val_loader, criterion, optimizer\n    )\n    \n    # Plot metrics\n    plot_metrics(train_losses, val_losses, train_accs, val_accs)\n    \n    # Evaluate on test set\n    model.eval()\n    test_correct = 0\n    test_total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs).squeeze()\n            predicted = (outputs >= 0.5).float()\n            test_total += labels.size(0)\n            test_correct += (predicted == labels).sum().item()\n    \n    test_acc = test_correct / test_total\n    print(f\"Test Accuracy: {test_acc:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:08:43.731465Z","iopub.execute_input":"2025-04-20T13:08:43.731966Z","iopub.status.idle":"2025-04-20T13:14:18.669067Z","shell.execute_reply.started":"2025-04-20T13:08:43.731944Z","shell.execute_reply":"2025-04-20T13:14:18.668286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python --version\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:14:28.626896Z","iopub.execute_input":"2025-04-20T13:14:28.627163Z","iopub.status.idle":"2025-04-20T13:14:28.780347Z","shell.execute_reply.started":"2025-04-20T13:14:28.627145Z","shell.execute_reply":"2025-04-20T13:14:28.779673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}